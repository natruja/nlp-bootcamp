{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6: Building a Text Classifier\n",
    "**The AI Engineer Course 2026 - Section 26**\n",
    "\n",
    "**Student:** Natruja\n",
    "\n",
    "**Date:** Tuesday, February 17, 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "1. Build a complete text classification pipeline\n",
    "2. Learn Naive Bayes for text classification\n",
    "3. Use sklearn Pipeline for streamlined workflows\n",
    "4. Evaluate classifiers with metrics\n",
    "5. Handle multi-class classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install scikit-learn\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn\", \"nltk\", \"-q\"])\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "print(\"✓ Libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Workflow\n",
    "\n",
    "Building a text classifier involves these steps:\n",
    "\n",
    "### 1. Data Preparation\n",
    "- Collect labeled examples\n",
    "- Split into train/test sets\n",
    "- Prepare features\n",
    "\n",
    "### 2. Feature Extraction\n",
    "- Convert text to vectors (TF-IDF, BoW)\n",
    "- Select important features\n",
    "\n",
    "### 3. Model Training\n",
    "- Choose algorithm (Naive Bayes, SVM, etc.)\n",
    "- Train on training data\n",
    "\n",
    "### 4. Evaluation\n",
    "- Test on test set\n",
    "- Calculate metrics (accuracy, precision, recall)\n",
    "\n",
    "### 5. Optimization\n",
    "- Fine-tune parameters\n",
    "- Improve performance\n",
    "\n",
    "### 6. Deployment\n",
    "- Use on new, unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes for Text Classification\n",
    "\n",
    "**Multinomial Naive Bayes** is the most popular algorithm for text classification.\n",
    "\n",
    "### Why It Works Well:\n",
    "- Fast and efficient\n",
    "- Works well with high-dimensional sparse data (text)\n",
    "- Requires relatively little training data\n",
    "- Probabilistic: gives confidence scores\n",
    "\n",
    "### How It Works:\n",
    "- Uses Bayes' theorem: P(Class|Text) = P(Text|Class) × P(Class) / P(Text)\n",
    "- Assumes words are independent (naive assumption)\n",
    "- Calculates probability of each class\n",
    "- Picks class with highest probability\n",
    "\n",
    "### Applications:\n",
    "- Spam detection\n",
    "- Sentiment analysis\n",
    "- Topic classification\n",
    "- Language detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Simple Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training data (texts and labels)\n",
    "texts = [\n",
    "    \"This movie is great and entertaining\",\n",
    "    \"Loved this film, highly recommend\",\n",
    "    \"Amazing performances and cinematography\",\n",
    "    \"Best movie I have seen all year\",\n",
    "    \"Terrible movie, waste of time\",\n",
    "    \"Horrible acting and boring plot\",\n",
    "    \"Did not enjoy this film at all\",\n",
    "    \"Worst movie ever made\"\n",
    "]\n",
    "\n",
    "labels = [1, 1, 1, 1, 0, 0, 0, 0]  # 1=positive, 0=negative\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"\\nTraining examples:\")\n",
    "for text, label in zip(X_train, y_train):\n",
    "    sentiment = \"Positive\" if label == 1 else \"Negative\"\n",
    "    print(f\"  [{sentiment}] {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Training a Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorizer\n",
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "# Transform training data\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "print(f\"Training data shape: {X_train_vec.shape}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "# Create and train classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "print(\"\\n✓ Classifier trained successfully!\")\n",
    "print(f\"Classes: {classifier.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test data\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test_vec)\n",
    "y_pred_proba = classifier.predict_proba(X_test_vec)\n",
    "\n",
    "print(\"Predictions on Test Set:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Text':<40} | {'Actual':<8} | {'Predicted':<10} | {'Confidence'}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for text, actual, pred, proba in zip(X_test, y_test, y_pred, y_pred_proba):\n",
    "    actual_label = \"Positive\" if actual == 1 else \"Negative\"\n",
    "    pred_label = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    confidence = max(proba)\n",
    "    \n",
    "    display_text = text[:37] + \"...\" if len(text) > 40 else text\n",
    "    print(f\"{display_text:<40} | {actual_label:<8} | {pred_label:<10} | {confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Using Pipeline for Streamlined Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline combining vectorizer and classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(lowercase=True, stop_words='english')),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train on original texts (no need to vectorize separately!)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions directly on texts\n",
    "y_pred_pipeline = pipeline.predict(X_test)\n",
    "\n",
    "# Test on new texts\n",
    "new_texts = [\n",
    "    \"This is a fantastic movie!\",\n",
    "    \"Absolutely terrible, don't watch it\"\n",
    "]\n",
    "\n",
    "print(\"Pipeline Predictions on New Texts:\")\n",
    "print(\"=\"*50)\n",
    "for text in new_texts:\n",
    "    pred = pipeline.predict([text])[0]\n",
    "    proba = pipeline.predict_proba([text])[0]\n",
    "    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Prediction: {sentiment} (Confidence: {max(proba):.2f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Metrics\n",
    "\n",
    "### Accuracy\n",
    "- Percentage of correct predictions\n",
    "- Formula: (TP + TN) / (TP + TN + FP + FN)\n",
    "- Good for balanced datasets\n",
    "\n",
    "### Precision\n",
    "- Of predicted positives, how many are actually positive?\n",
    "- Formula: TP / (TP + FP)\n",
    "- Important when false positives are costly\n",
    "\n",
    "### Recall\n",
    "- Of actual positives, how many did we find?\n",
    "- Formula: TP / (TP + FN)\n",
    "- Important when false negatives are costly\n",
    "\n",
    "### F1-Score\n",
    "- Harmonic mean of precision and recall\n",
    "- Formula: 2 × (Precision × Recall) / (Precision + Recall)\n",
    "- Good for imbalanced datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model Evaluation Results:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  True Negatives: {cm[0][0]}\")\n",
    "print(f\"  False Positives: {cm[0][1]}\")\n",
    "print(f\"  False Negatives: {cm[1][0]}\")\n",
    "print(f\"  True Positives: {cm[1][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EXERCISES: Text Classification\n",
    "\n",
    "Complete all 15 exercises below to master text classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 1 - Split Data into Train/Test Sets\n",
    "\n",
    "**Goal:** Learn how to split data using `train_test_split` with proper parameters.\n",
    "\n",
    "**Concepts:** train_test_split, test_size, random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentiment data\n",
    "sample_texts = [\n",
    "    \"I love this product\",\n",
    "    \"This is amazing\",\n",
    "    \"I hate this\",\n",
    "    \"Terrible quality\",\n",
    "    \"Excellent service\",\n",
    "    \"Very disappointed\"\n",
    "]\n",
    "sample_labels = [1, 1, 0, 0, 1, 0]  # 1=positive, 0=negative\n",
    "\n",
    "# TODO: Use train_test_split to split the data\n",
    "# - Use test_size=0.3 (30% test, 70% train)\n",
    "# - Use random_state=42 (for reproducibility)\n",
    "X_train, X_test, y_train, y_test = ___\n",
    "\n",
    "# TODO: Print the sizes of train and test sets\n",
    "print(f\"Training set size: ___\")\n",
    "print(f\"Test set size: ___\")\n",
    "\n",
    "# TODO: Print one example from training set\n",
    "print(f\"\\nExample from training set: {___}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 2 - Train a MultinomialNB Model\n",
    "\n",
    "**Goal:** Train a basic Naive Bayes classifier on vectorized text data.\n",
    "\n",
    "**Concepts:** TfidfVectorizer, fit_transform, MultinomialNB, fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Python is awesome\", \"I love Python\", \"Python rocks\",\n",
    "    \"Java is boring\", \"I hate Java\", \"Java is slow\"\n",
    "]\n",
    "labels = [1, 1, 1, 0, 0, 0]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "# TODO: Create a TfidfVectorizer with stop_words='english'\n",
    "vectorizer = ___\n",
    "\n",
    "# TODO: Fit and transform the training data\n",
    "X_train_vec = ___\n",
    "\n",
    "# TODO: Create a MultinomialNB classifier\n",
    "classifier = ___\n",
    "\n",
    "# TODO: Train the classifier on vectorized training data\n",
    "___\n",
    "\n",
    "print(\"✓ Model trained successfully!\")\n",
    "print(f\"Classifier classes: {classifier.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 3 - Make Predictions on Test Data\n",
    "\n",
    "**Goal:** Use the trained model to predict labels for test data.\n",
    "\n",
    "**Concepts:** transform(), predict(), predict_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Transform test data using the trained vectorizer\n",
    "X_test_vec = ___\n",
    "\n",
    "# TODO: Make predictions on test data\n",
    "y_pred = ___\n",
    "\n",
    "# TODO: Get probability scores for predictions\n",
    "y_pred_proba = ___\n",
    "\n",
    "# Print results\n",
    "print(\"Predictions:\")\n",
    "for text, true_label, pred_label, proba in zip(X_test, y_test, y_pred, y_pred_proba):\n",
    "    true_sentiment = \"Positive\" if true_label == 1 else \"Negative\"\n",
    "    pred_sentiment = \"Positive\" if pred_label == 1 else \"Negative\"\n",
    "    confidence = max(proba)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"  True: {true_sentiment}, Predicted: {pred_sentiment}, Confidence: {confidence:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 4 - Calculate Accuracy Score\n",
    "\n",
    "**Goal:** Evaluate model performance using accuracy metric.\n",
    "\n",
    "**Concepts:** accuracy_score, score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the predictions from Exercise 3\n",
    "# (y_pred and y_test should still be in memory)\n",
    "\n",
    "# TODO: Calculate accuracy using accuracy_score\n",
    "acc = ___\n",
    "\n",
    "# TODO: Also calculate accuracy using the model's score() method\n",
    "acc_score = ___\n",
    "\n",
    "print(f\"Accuracy (using accuracy_score): {acc:.4f}\")\n",
    "print(f\"Accuracy (using model.score()): {acc_score:.4f}\")\n",
    "print(f\"\\nAccuracy percentage: {acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 5 - Print Confusion Matrix\n",
    "\n",
    "**Goal:** Understand model errors using the confusion matrix.\n",
    "\n",
    "**Concepts:** confusion_matrix, True Positives, False Positives, True Negatives, False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate confusion matrix\n",
    "cm = ___\n",
    "\n",
    "# Print the matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# TODO: Extract values from confusion matrix\n",
    "# cm[i][j] where i is actual class, j is predicted class\n",
    "tn = ___  # True Negatives: predicted 0, actually 0\n",
    "fp = ___  # False Positives: predicted 1, actually 0\n",
    "fn = ___  # False Negatives: predicted 0, actually 1\n",
    "tp = ___  # True Positives: predicted 1, actually 1\n",
    "\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  True Negatives: {tn}\")\n",
    "print(f\"  False Positives: {fp}\")\n",
    "print(f\"  False Negatives: {fn}\")\n",
    "print(f\"  True Positives: {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 6 - Build a Pipeline with TfidfVectorizer + MultinomialNB\n",
    "\n",
    "**Goal:** Create a complete pipeline combining vectorization and classification.\n",
    "\n",
    "**Concepts:** Pipeline, combining multiple steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Great movie\", \"Love it\", \"Amazing\",\n",
    "    \"Bad film\", \"Hate it\", \"Terrible\"\n",
    "]\n",
    "labels = [1, 1, 1, 0, 0, 0]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "# TODO: Create a Pipeline with two steps:\n",
    "# 1. ('tfidf', TfidfVectorizer(stop_words='english'))\n",
    "# 2. ('classifier', MultinomialNB())\n",
    "pipe = Pipeline([\n",
    "    ___,\n",
    "    ___\n",
    "])\n",
    "\n",
    "# TODO: Train the pipeline on X_train and y_train\n",
    "___\n",
    "\n",
    "# TODO: Get accuracy on test set using pipe.score()\n",
    "accuracy = ___\n",
    "\n",
    "print(f\"Pipeline Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 7 - Use Pipeline to Predict New Text\n",
    "\n",
    "**Goal:** Use the trained pipeline to classify new, unseen text examples.\n",
    "\n",
    "**Concepts:** Pipeline prediction, predict_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pipeline from Exercise 6\n",
    "# (pipe should still be trained in memory)\n",
    "\n",
    "new_texts = [\n",
    "    \"This is fantastic!\",\n",
    "    \"I absolutely hate it\",\n",
    "    \"Not bad, pretty good\"\n",
    "]\n",
    "\n",
    "# TODO: Make predictions using the pipeline\n",
    "predictions = ___\n",
    "\n",
    "# TODO: Get probability scores\n",
    "probabilities = ___\n",
    "\n",
    "# Print results\n",
    "print(\"Predictions on New Texts:\")\n",
    "print(\"=\"*60)\n",
    "for text, pred, proba in zip(new_texts, predictions, probabilities):\n",
    "    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    confidence = max(proba)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"  Prediction: {sentiment} | Confidence: {confidence:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 8 - Print Classification Report and Explain Metrics\n",
    "\n",
    "**Goal:** Understand precision, recall, and F1-score from the classification report.\n",
    "\n",
    "**Concepts:** classification_report, precision, recall, F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from the pipeline (Exercise 6)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# TODO: Print classification report with target_names=['Negative', 'Positive']\n",
    "print(\"Classification Report:\")\n",
    "print(___)\n",
    "\n",
    "# TODO: Calculate and print these metrics manually:\n",
    "# Precision = TP / (TP + FP)\n",
    "# Recall = TP / (TP + FN)\n",
    "# F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "\n",
    "precision = ___\n",
    "recall = ___\n",
    "f1 = ___\n",
    "\n",
    "print(f\"\\nManual Calculations:\")\n",
    "print(f\"Precision: {precision:.3f} (of predicted positives, how many are actually positive?)\")\n",
    "print(f\"Recall: {recall:.3f} (of actual positives, how many did we find?)\")\n",
    "print(f\"F1-Score: {f1:.3f} (harmonic mean of precision and recall)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 9 - Compare Train Accuracy vs Test Accuracy\n",
    "\n",
    "**Goal:** Detect overfitting by comparing training and test accuracy.\n",
    "\n",
    "**Concepts:** overfitting, generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Love Python\", \"Python is great\", \"I enjoy Python\", \"Python rocks\",\n",
    "    \"Hate Java\", \"Java is bad\", \"I dislike Java\", \"Java is slow\",\n",
    "    \"Love C++\", \"C++ is fast\", \"I like C++\", \"C++ is good\",\n",
    "    \"Hate JavaScript\", \"JavaScript is confusing\", \"JS is bad\", \"JS is frustrating\"\n",
    "]\n",
    "labels = [1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Create and train pipeline\n",
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Calculate accuracy on training set\n",
    "train_accuracy = ___\n",
    "\n",
    "# TODO: Calculate accuracy on test set\n",
    "test_accuracy = ___\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.3f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
    "print(f\"Difference: {(train_accuracy - test_accuracy):.3f}\")\n",
    "\n",
    "if train_accuracy > test_accuracy:\n",
    "    print(\"\\n⚠️  Model shows signs of overfitting (higher train accuracy)\")\n",
    "else:\n",
    "    print(\"\\n✓ Model generalizes well (similar train and test accuracy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 10 - Try LogisticRegression Instead of NB\n",
    "\n",
    "**Goal:** Compare different algorithms by replacing MultinomialNB with LogisticRegression.\n",
    "\n",
    "**Concepts:** LogisticRegression, algorithm comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same data from Exercise 9\n",
    "# X_train, X_test, y_train, y_test should be in memory\n",
    "\n",
    "# Create pipeline with LogisticRegression\n",
    "# TODO: Replace MultinomialNB() with LogisticRegression(max_iter=200)\n",
    "pipe_lr = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier', ___)\n",
    "])\n",
    "\n",
    "# Train the new pipeline\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Calculate accuracies for LogisticRegression\n",
    "lr_train_acc = ___\n",
    "lr_test_acc = ___\n",
    "\n",
    "# TODO: Calculate accuracies for MultinomialNB (from previous exercise)\n",
    "nb_train_acc = ___\n",
    "nb_test_acc = ___\n",
    "\n",
    "# Compare results\n",
    "print(\"Algorithm Comparison:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Algorithm':<20} {'Train':<12} {'Test':<12}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{'MultinomialNB':<20} {nb_train_acc:<12.3f} {nb_test_acc:<12.3f}\")\n",
    "print(f\"{'LogisticRegression':<20} {lr_train_acc:<12.3f} {lr_test_acc:<12.3f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if lr_test_acc > nb_test_acc:\n",
    "    print(f\"\\n✓ LogisticRegression performs better (+{(lr_test_acc - nb_test_acc):.3f})\")\n",
    "else:\n",
    "    print(f\"\\n✓ MultinomialNB performs better (+{(nb_test_acc - lr_test_acc):.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 11 - Build Complete Spam Classifier Pipeline from Scratch\n",
    "\n",
    "**Goal:** Build an end-to-end spam classifier with data preprocessing, training, and evaluation.\n",
    "\n",
    "**Concepts:** Complete pipeline workflow, data splitting, training, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spam classifier data\n",
    "emails = [\n",
    "    # Spam (1)\n",
    "    \"Click here to win free money now!\",\n",
    "    \"You have won the lottery! Claim your prize\",\n",
    "    \"Limited time offer: 99% off everything\",\n",
    "    \"Congratulations! You are selected for a prize\",\n",
    "    \"Act now! Special deal expires today\",\n",
    "    \"Get rich quick with our secret method\",\n",
    "    # Ham (0)\n",
    "    \"Meeting scheduled for tomorrow at 3 PM\",\n",
    "    \"Can you review the attached document?\",\n",
    "    \"Project update: Phase 1 is complete\",\n",
    "    \"Please confirm your attendance for the event\",\n",
    "    \"Here are the quarterly results\",\n",
    "    \"Thanks for your email, I will respond soon\"\n",
    "]\n",
    "\n",
    "labels = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]  # 1=spam, 0=ham\n",
    "\n",
    "# TODO: Step 1 - Split data (80-20 split, random_state=42)\n",
    "X_train, X_test, y_train, y_test = ___\n",
    "\n",
    "# TODO: Step 2 - Create pipeline with TfidfVectorizer and MultinomialNB\n",
    "spam_pipe = Pipeline([\n",
    "    ___,\n",
    "    ___\n",
    "])\n",
    "\n",
    "# TODO: Step 3 - Train the pipeline\n",
    "___\n",
    "\n",
    "# TODO: Step 4 - Make predictions\n",
    "y_pred = ___\n",
    "\n",
    "# TODO: Step 5 - Calculate accuracy\n",
    "accuracy = ___\n",
    "\n",
    "# TODO: Step 6 - Print confusion matrix and classification report\n",
    "print(f\"Spam Classifier Accuracy: {accuracy:.2f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(___)\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(___)\n",
    "\n",
    "# Test on new emails\n",
    "new_emails = [\n",
    "    \"You have been selected to receive $1000\",\n",
    "    \"Can you help me with the project?\"\n",
    "]\n",
    "\n",
    "# TODO: Predict on new emails\n",
    "new_preds = ___\n",
    "\n",
    "print(f\"\\nNew Email Classification:\")\n",
    "for email, pred in zip(new_emails, new_preds):\n",
    "    label = \"Spam\" if pred == 1 else \"Ham\"\n",
    "    print(f\"  [{label}] {email}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 12 - Use cross_val_score for Model Evaluation\n",
    "\n",
    "**Goal:** Use cross-validation to get more reliable performance estimates.\n",
    "\n",
    "**Concepts:** cross_val_score, k-fold cross-validation, model reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Excellent product\", \"Very satisfied\", \"Love it\", \"Highly recommend\", \"Amazing quality\",\n",
    "    \"Terrible\", \"Disappointed\", \"Waste of money\", \"Very bad\", \"Horrible\"\n",
    "]\n",
    "labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "# Create pipeline\n",
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# TODO: Use cross_val_score with cv=5 folds\n",
    "# This will train 5 different models and return 5 accuracy scores\n",
    "cv_scores = ___\n",
    "\n",
    "print(f\"Cross-Validation Results (5-fold):\")\n",
    "print(f\"Fold scores: {cv_scores}\")\n",
    "print(f\"Mean accuracy: {cv_scores.mean():.3f}\")\n",
    "print(f\"Std deviation: {cv_scores.std():.3f}\")\n",
    "\n",
    "# Interpretation:\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - Mean shows average performance: {cv_scores.mean():.1%}\")\n",
    "print(f\"  - Std shows consistency (lower is better): {cv_scores.std():.3f}\")\n",
    "if cv_scores.std() < 0.1:\n",
    "    print(f\"  ✓ Model is consistent across all folds\")\n",
    "else:\n",
    "    print(f\"  ⚠️  Model performance varies across folds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 13 - Compare MultinomialNB vs LogisticRegression Performance\n",
    "\n",
    "**Goal:** Use cross-validation to fairly compare two different algorithms.\n",
    "\n",
    "**Concepts:** algorithm comparison, cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same data from Exercise 12\n",
    "# texts and labels should be in memory\n",
    "\n",
    "# Create pipeline with MultinomialNB\n",
    "nb_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Create pipeline with LogisticRegression\n",
    "lr_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier', LogisticRegression(max_iter=200))\n",
    "])\n",
    "\n",
    "# TODO: Get cross-validation scores for both pipelines (cv=5)\n",
    "nb_scores = ___\n",
    "lr_scores = ___\n",
    "\n",
    "# Print comparison\n",
    "print(\"Algorithm Comparison (5-fold Cross-Validation):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Algorithm':<25} {'Mean CV':<15} {'Std':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'MultinomialNB':<25} {nb_scores.mean():<15.3f} {nb_scores.std():<15.3f}\")\n",
    "print(f\"{'LogisticRegression':<25} {lr_scores.mean():<15.3f} {lr_scores.std():<15.3f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Determine winner\n",
    "if nb_scores.mean() > lr_scores.mean():\n",
    "    winner = \"MultinomialNB\"\n",
    "    diff = nb_scores.mean() - lr_scores.mean()\n",
    "else:\n",
    "    winner = \"LogisticRegression\"\n",
    "    diff = lr_scores.mean() - nb_scores.mean()\n",
    "\n",
    "print(f\"\\nWinner: {winner} (+{diff:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 14 - Tune TfidfVectorizer Parameters\n",
    "\n",
    "**Goal:** Optimize model performance by tuning vectorizer parameters.\n",
    "\n",
    "**Concepts:** max_features, ngram_range, hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"This is an amazing product\", \"Love the quality and service\", \"Excellent choice\",\n",
    "    \"Terrible experience\", \"Very disappointed\", \"Do not recommend\",\n",
    "    \"Great value for money\", \"Outstanding performance\", \"Best purchase ever\",\n",
    "    \"Worst decision ever\", \"Complete waste of time\", \"Very poor quality\"\n",
    "]\n",
    "labels = [1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "# Test different configurations\n",
    "configs = [\n",
    "    {'max_features': 50, 'ngram_range': (1, 1)},\n",
    "    {'max_features': 100, 'ngram_range': (1, 1)},\n",
    "    {'max_features': 50, 'ngram_range': (1, 2)},\n",
    "    {'max_features': 100, 'ngram_range': (1, 2)}\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for config in configs:\n",
    "    # TODO: Create pipeline with the given config parameters\n",
    "    pipe = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            max_features=___,\n",
    "            ngram_range=___\n",
    "        )),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "    \n",
    "    # Train and evaluate\n",
    "    pipe.fit(X_train, y_train)\n",
    "    # TODO: Calculate test accuracy\n",
    "    accuracy = ___\n",
    "    \n",
    "    results.append({\n",
    "        'max_features': config['max_features'],\n",
    "        'ngram_range': config['ngram_range'],\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "\n",
    "# Print results\n",
    "print(\"Hyperparameter Tuning Results:\")\n",
    "print(\"=\"*60)\n",
    "for res in results:\n",
    "    print(f\"max_features={res['max_features']:3d}, ngram={res['ngram_range']} → Accuracy: {res['accuracy']:.3f}\")\n",
    "\n",
    "# Find best configuration\n",
    "best = max(results, key=lambda x: x['accuracy'])\n",
    "print(f\"\\nBest configuration: max_features={best['max_features']}, ngram={best['ngram_range']}\")\n",
    "print(f\"Best accuracy: {best['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 15 - Build Prediction Function with Confidence\n",
    "\n",
    "**Goal:** Create a reusable function that predicts labels and returns confidence scores.\n",
    "\n",
    "**Concepts:** Functions, predict_proba(), confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data\n",
    "texts = [\n",
    "    \"I love this\", \"Amazing product\", \"Excellent service\", \"Very satisfied\",\n",
    "    \"I hate this\", \"Terrible product\", \"Bad experience\", \"Very unhappy\"\n",
    "]\n",
    "labels = [1, 1, 1, 1, 0, 0, 0, 0]\n",
    "\n",
    "# Create and train pipeline\n",
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "pipe.fit(texts, labels)\n",
    "\n",
    "# TODO: Create a function called 'predict_with_confidence' that:\n",
    "# 1. Takes a text string as input\n",
    "# 2. Returns a dictionary with:\n",
    "#    - 'label': predicted class (0 or 1)\n",
    "#    - 'sentiment': 'Positive' or 'Negative' (mapped from label)\n",
    "#    - 'confidence': probability of the predicted class (0.0 to 1.0)\n",
    "# 3. Handles confidence values properly\n",
    "\n",
    "def predict_with_confidence(text):\n",
    "    \"\"\"\n",
    "    Predict sentiment with confidence score.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to classify\n",
    "        \n",
    "    Returns:\n",
    "        dict: {'label': int, 'sentiment': str, 'confidence': float}\n",
    "    \"\"\"\n",
    "    # TODO: Get prediction\n",
    "    pred = ___\n",
    "    \n",
    "    # TODO: Get probability scores\n",
    "    proba = ___\n",
    "    \n",
    "    # TODO: Extract confidence (maximum probability)\n",
    "    confidence = ___\n",
    "    \n",
    "    # TODO: Map label to sentiment\n",
    "    sentiment = ___  # 'Positive' if pred == 1 else 'Negative'\n",
    "    \n",
    "    return {\n",
    "        'label': pred,\n",
    "        'sentiment': sentiment,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "\n",
    "# Test the function\n",
    "test_texts = [\n",
    "    \"This product is amazing!\",\n",
    "    \"I really hate this\",\n",
    "    \"It's okay, not great\"\n",
    "]\n",
    "\n",
    "print(\"Predictions with Confidence:\")\n",
    "print(\"=\"*70)\n",
    "for text in test_texts:\n",
    "    result = predict_with_confidence(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"  Prediction: {result['sentiment']}\")\n",
    "    print(f\"  Confidence: {result['confidence']:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Text Classification** converts documents to categories using machine learning\n",
    "- **Naive Bayes** is fast and effective for text classification tasks\n",
    "- **Pipelines** combine feature extraction and classification into a single workflow\n",
    "- **Evaluation metrics** (accuracy, precision, recall, F1) measure classifier performance\n",
    "- **Train/test split** prevents overfitting and provides reliable performance estimates\n",
    "- **Cross-validation** gives more reliable performance estimates than single splits\n",
    "- **Hyperparameter tuning** can improve model performance significantly\n",
    "- **Confidence scores** help assess model certainty in predictions\n",
    "\n",
    "### Common Use Cases:\n",
    "- Spam detection\n",
    "- Sentiment analysis\n",
    "- Topic categorization\n",
    "- Intent classification\n",
    "- Document routing\n",
    "- Fake news detection\n",
    "\n",
    "### What's Next:\n",
    "Tomorrow we'll apply these skills to **Fake News Detection** using real-world data!\n",
    "\n",
    "---\n",
    "\n",
    "*Created for Natruja's NLP study plan*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
