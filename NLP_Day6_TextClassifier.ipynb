{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6: Building a Text Classifier\n",
    "**The AI Engineer Course 2026 - Section 26**\n",
    "\n",
    "**Student:** Natruja\n",
    "\n",
    "**Date:** Tuesday, February 17, 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "1. Build a complete text classification pipeline\n",
    "2. Learn Naive Bayes for text classification\n",
    "3. Use sklearn Pipeline for streamlined workflows\n",
    "4. Evaluate classifiers with metrics\n",
    "5. Handle multi-class classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install scikit-learn\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn\", \"nltk\", \"-q\"])\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "print(\"✓ Libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Workflow\n",
    "\n",
    "Building a text classifier involves these steps:\n",
    "\n",
    "### 1. Data Preparation\n",
    "- Collect labeled examples\n",
    "- Split into train/test sets\n",
    "- Prepare features\n",
    "\n",
    "### 2. Feature Extraction\n",
    "- Convert text to vectors (TF-IDF, BoW)\n",
    "- Select important features\n",
    "\n",
    "### 3. Model Training\n",
    "- Choose algorithm (Naive Bayes, SVM, etc.)\n",
    "- Train on training data\n",
    "\n",
    "### 4. Evaluation\n",
    "- Test on test set\n",
    "- Calculate metrics (accuracy, precision, recall)\n",
    "\n",
    "### 5. Optimization\n",
    "- Fine-tune parameters\n",
    "- Improve performance\n",
    "\n",
    "### 6. Deployment\n",
    "- Use on new, unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes for Text Classification\n",
    "\n",
    "**Multinomial Naive Bayes** is the most popular algorithm for text classification.\n",
    "\n",
    "### Why It Works Well:\n",
    "- Fast and efficient\n",
    "- Works well with high-dimensional sparse data (text)\n",
    "- Requires relatively little training data\n",
    "- Probabilistic: gives confidence scores\n",
    "\n",
    "### How It Works:\n",
    "- Uses Bayes' theorem: P(Class|Text) = P(Text|Class) × P(Class) / P(Text)\n",
    "- Assumes words are independent (naive assumption)\n",
    "- Calculates probability of each class\n",
    "- Picks class with highest probability\n",
    "\n",
    "### Applications:\n",
    "- Spam detection\n",
    "- Sentiment analysis\n",
    "- Topic classification\n",
    "- Language detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Simple Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training data (texts and labels)\n",
    "texts = [\n",
    "    \"This movie is great and entertaining\",\n",
    "    \"Loved this film, highly recommend\",\n",
    "    \"Amazing performances and cinematography\",\n",
    "    \"Best movie I have seen all year\",\n",
    "    \"Terrible movie, waste of time\",\n",
    "    \"Horrible acting and boring plot\",\n",
    "    \"Did not enjoy this film at all\",\n",
    "    \"Worst movie ever made\"\n",
    "]\n",
    "\n",
    "labels = [1, 1, 1, 1, 0, 0, 0, 0]  # 1=positive, 0=negative\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"\\nTraining examples:\")\n",
    "for text, label in zip(X_train, y_train):\n",
    "    sentiment = \"Positive\" if label == 1 else \"Negative\"\n",
    "    print(f\"  [{sentiment}] {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Training a Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorizer\n",
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "# Transform training data\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "print(f\"Training data shape: {X_train_vec.shape}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "# Create and train classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "print(\"\\n✓ Classifier trained successfully!\")\n",
    "print(f\"Classes: {classifier.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test data\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test_vec)\n",
    "y_pred_proba = classifier.predict_proba(X_test_vec)\n",
    "\n",
    "print(\"Predictions on Test Set:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Text':<40} | {'Actual':<8} | {'Predicted':<10} | {'Confidence'}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for text, actual, pred, proba in zip(X_test, y_test, y_pred, y_pred_proba):\n",
    "    actual_label = \"Positive\" if actual == 1 else \"Negative\"\n",
    "    pred_label = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    confidence = max(proba)\n",
    "    \n",
    "    display_text = text[:37] + \"...\" if len(text) > 40 else text\n",
    "    print(f\"{display_text:<40} | {actual_label:<8} | {pred_label:<10} | {confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Using Pipeline for Streamlined Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline combining vectorizer and classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(lowercase=True, stop_words='english')),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train on original texts (no need to vectorize separately!)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions directly on texts\n",
    "y_pred_pipeline = pipeline.predict(X_test)\n",
    "\n",
    "# Test on new texts\n",
    "new_texts = [\n",
    "    \"This is a fantastic movie!\",\n",
    "    \"Absolutely terrible, don't watch it\"\n",
    "]\n",
    "\n",
    "print(\"Pipeline Predictions on New Texts:\")\n",
    "print(\"=\"*50)\n",
    "for text in new_texts:\n",
    "    pred = pipeline.predict([text])[0]\n",
    "    proba = pipeline.predict_proba([text])[0]\n",
    "    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Prediction: {sentiment} (Confidence: {max(proba):.2f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Metrics\n",
    "\n",
    "### Accuracy\n",
    "- Percentage of correct predictions\n",
    "- Formula: (TP + TN) / (TP + TN + FP + FN)\n",
    "- Good for balanced datasets\n",
    "\n",
    "### Precision\n",
    "- Of predicted positives, how many are actually positive?\n",
    "- Formula: TP / (TP + FP)\n",
    "- Important when false positives are costly\n",
    "\n",
    "### Recall\n",
    "- Of actual positives, how many did we find?\n",
    "- Formula: TP / (TP + FN)\n",
    "- Important when false negatives are costly\n",
    "\n",
    "### F1-Score\n",
    "- Harmonic mean of precision and recall\n",
    "- Formula: 2 × (Precision × Recall) / (Precision + Recall)\n",
    "- Good for imbalanced datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model Evaluation Results:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  True Negatives: {cm[0][0]}\")\n",
    "print(f\"  False Positives: {cm[0][1]}\")\n",
    "print(f\"  False Negatives: {cm[1][0]}\")\n",
    "print(f\"  True Positives: {cm[1][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EXERCISES: Text Classification\n",
    "\n",
    "Complete all 15 exercises below to master text classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 1 - Split Data into Train/Test Sets\n",
    "\n",
    "**Goal:** Learn how to split data using `train_test_split` with proper parameters.\n",
    "\n",
    "**Concepts:** train_test_split, test_size, random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sample sentiment data\nsample_texts = [\n    \"I love this product\",\n    \"This is amazing\",\n    \"I hate this\",\n    \"Terrible quality\",\n    \"Excellent service\",\n    \"Very disappointed\"\n]\nsample_labels = [1, 1, 0, 0, 1, 0]  # 1=positive, 0=negative\n\n# TODO: Use train_test_split to split the data\n# - Use test_size=0.3 (30% test, 70% train)\n# - Use random_state=42 (for reproducibility)\nX_train, X_test, y_train, y_test = ___\n\n# TODO: Print the sizes of train and test sets\nprint(f\"Training set size: ___\")\nprint(f\"Test set size: ___\")\n\n# TODO: Print one example from training set\nprint(f\"\\nExample from training set: {___}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 2 - Train a MultinomialNB Model\n",
    "\n",
    "**Goal:** Train a basic Naive Bayes classifier on vectorized text data.\n",
    "\n",
    "**Concepts:** TfidfVectorizer, fit_transform, MultinomialNB, fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "texts = [\n    \"Python is awesome\", \"I love Python\", \"Python rocks\",\n    \"Java is boring\", \"I hate Java\", \"Java is slow\"\n]\nlabels = [1, 1, 1, 0, 0, 0]\n\n# Split data\nX_train, X_test, y_train, y_test = ___\n\n# TODO: Create a TfidfVectorizer with stop_words='english'\nvectorizer = ___\n\n# TODO: Fit and transform the training data\nX_train_vec = ___\n\n# TODO: Create a MultinomialNB classifier\nclassifier = ___\n\n# TODO: Train the classifier on vectorized training data\n___\n\nprint(\"✓ Model trained successfully!\")\nprint(f\"Classifier classes: {classifier.classes_}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 3 - Make Predictions on Test Data\n",
    "\n",
    "**Goal:** Use the trained model to predict labels for test data.\n",
    "\n",
    "**Concepts:** transform(), predict(), predict_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Transform test data using the trained vectorizer\nX_test_vec = ___\n\n# TODO: Make predictions on test data\ny_pred = ___\n\n# TODO: Get probability scores for predictions\ny_pred_proba = ___\n\n# Print results\nprint(\"Predictions:\")\nfor text, true_label, pred_label, proba in zip(X_test, y_test, y_pred, y_pred_proba):\n    true_sentiment = \"Positive\" if true_label == 1 else \"Negative\"\n    pred_sentiment = \"Positive\" if pred_label == 1 else \"Negative\"\n    confidence = max(proba)\n    print(f\"Text: {text}\")\n    print(f\"  True: {true_sentiment}, Predicted: {pred_sentiment}, Confidence: {confidence:.2f}\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 4 - Calculate Accuracy Score\n",
    "\n",
    "**Goal:** Evaluate model performance using accuracy metric.\n",
    "\n",
    "**Concepts:** accuracy_score, score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use the predictions from Exercise 3\n# (y_pred and y_test should still be in memory)\n\n# TODO: Calculate accuracy using accuracy_score\nacc = ___\n\n# TODO: Also calculate accuracy using the model's score() method\nacc_score = ___\n\nprint(f\"Accuracy (using accuracy_score): {acc:.4f}\")\nprint(f\"Accuracy (using model.score()): {acc_score:.4f}\")\nprint(f\"\\nAccuracy percentage: {acc * 100:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 5 - Print Confusion Matrix\n",
    "\n",
    "**Goal:** Understand model errors using the confusion matrix.\n",
    "\n",
    "**Concepts:** confusion_matrix, True Positives, False Positives, True Negatives, False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Calculate confusion matrix\ncm = ___\n\n# Print the matrix\nprint(\"Confusion Matrix:\")\nprint(cm)\n\n# TODO: Extract values from confusion matrix\n# cm[i][j] where i is actual class, j is predicted class\ntn = ___  # True Negatives: predicted 0, actually 0\nfp = ___  # False Positives: predicted 1, actually 0\nfn = ___  # False Negatives: predicted 0, actually 1\ntp = ___  # True Positives: predicted 1, actually 1\n\nprint(f\"\\nBreakdown:\")\nprint(f\"  True Negatives: {tn}\")\nprint(f\"  False Positives: {fp}\")\nprint(f\"  False Negatives: {fn}\")\nprint(f\"  True Positives: {tp}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 6 - Build a Pipeline with TfidfVectorizer + MultinomialNB\n",
    "\n",
    "**Goal:** Create a complete pipeline combining vectorization and classification.\n",
    "\n",
    "**Concepts:** Pipeline, combining multiple steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "texts = [\n    \"Great movie\", \"Love it\", \"Amazing\",\n    \"Bad film\", \"Hate it\", \"Terrible\"\n]\nlabels = [1, 1, 1, 0, 0, 0]\n\n# Split data\nX_train, X_test, y_train, y_test = ___\n\n# TODO: Create a Pipeline with two steps:\n# 1. ('tfidf', TfidfVectorizer(stop_words='english'))\n# 2. ('classifier', MultinomialNB())\npipe = Pipeline([\n    ___,\n    ___\n])\n\n# TODO: Train the pipeline on X_train and y_train\n___\n\n# TODO: Get accuracy on test set using pipe.score()\naccuracy = ___\n\nprint(f\"Pipeline Accuracy: {accuracy:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 7 - Use Pipeline to Predict New Text\n",
    "\n",
    "**Goal:** Use the trained pipeline to classify new, unseen text examples.\n",
    "\n",
    "**Concepts:** Pipeline prediction, predict_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use the pipeline from Exercise 6\n# (pipe should still be trained in memory)\n\nnew_texts = [\n    \"This is fantastic!\",\n    \"I absolutely hate it\",\n    \"Not bad, pretty good\"\n]\n\n# TODO: Make predictions using the pipeline\npredictions = ___\n\n# TODO: Get probability scores\nprobabilities = ___\n\n# Print results\nprint(\"Predictions on New Texts:\")\nprint(\"=\"*60)\nfor text, pred, proba in zip(new_texts, predictions, probabilities):\n    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n    confidence = max(proba)\n    print(f\"Text: {text}\")\n    print(f\"  Prediction: {sentiment} | Confidence: {confidence:.3f}\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 8 - Print Classification Report and Explain Metrics\n",
    "\n",
    "**Goal:** Understand precision, recall, and F1-score from the classification report.\n",
    "\n",
    "**Concepts:** classification_report, precision, recall, F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get predictions from the pipeline (Exercise 6)\ny_pred = ___\n\n# TODO: Print classification report with target_names=['Negative', 'Positive']\nprint(\"Classification Report:\")\nprint___\n\n# TODO: Calculate and print these metrics manually:\n# Precision = TP / (TP + FP)\n# Recall = TP / (TP + FN)\n# F1 = 2 * (Precision * Recall) / (Precision + Recall)\n\ncm = ___\ntn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n\nprecision = ___\nrecall = ___\nf1 = ___\n\nprint(f\"\\nManual Calculations:\")\nprint(f\"Precision: {precision:.3f} (of predicted positives, how many are actually positive?)\")\nprint(f\"Recall: {recall:.3f} (of actual positives, how many did we find?)\")\nprint(f\"F1-Score: {f1:.3f} (harmonic mean of precision and recall)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 9 - Compare Train Accuracy vs Test Accuracy\n",
    "\n",
    "**Goal:** Detect overfitting by comparing training and test accuracy.\n",
    "\n",
    "**Concepts:** overfitting, generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "texts = [\n    \"Love Python\", \"Python is great\", \"I enjoy Python\", \"Python rocks\",\n    \"Hate Java\", \"Java is bad\", \"I dislike Java\", \"Java is slow\",\n    \"Love C++\", \"C++ is fast\", \"I like C++\", \"C++ is good\",\n    \"Hate JavaScript\", \"JavaScript is confusing\", \"JS is bad\", \"JS is frustrating\"\n]\nlabels = [1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]\n\n# Split data\nX_train, X_test, y_train, y_test = ___\n\n# Create and train pipeline\npipe = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words='english')),\n    ('classifier', MultinomialNB())\n])\npipe.fit(X_train, y_train)\n\n# TODO: Calculate accuracy on training set\ntrain_accuracy = ___\n\n# TODO: Calculate accuracy on test set\ntest_accuracy = ___\n\nprint(f\"Training Accuracy: {train_accuracy:.3f}\")\nprint(f\"Test Accuracy: {test_accuracy:.3f}\")\nprint(f\"Difference: {(train_accuracy - test_accuracy):.3f}\")\n\nif train_accuracy > test_accuracy:\n    print(\"\\n⚠️  Model shows signs of overfitting (higher train accuracy)\")\nelse:\n    print(\"\\n✓ Model generalizes well (similar train and test accuracy)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 10 - Try LogisticRegression Instead of NB\n",
    "\n",
    "**Goal:** Compare different algorithms by replacing MultinomialNB with LogisticRegression.\n",
    "\n",
    "**Concepts:** LogisticRegression, algorithm comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use the same data from Exercise 9\n# X_train, X_test, y_train, y_test should be in memory\n\n# Create pipeline with LogisticRegression\n# TODO: Replace MultinomialNB() with LogisticRegression(max_iter=200)\npipe_lr = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words='english')),\n    ('classifier', ___\n])\n\n# Train the new pipeline\npipe_lr.fit(X_train, y_train)\n\n# TODO: Calculate accuracies for LogisticRegression\nlr_train_acc = ___\nlr_test_acc = ___\n\n# TODO: Calculate accuracies for MultinomialNB (from previous exercise)\nnb_train_acc = ___\nnb_test_acc = ___\n\n# Compare results\nprint(\"Algorithm Comparison:\")\nprint(\"=\"*50)\nprint(f\"{'Algorithm':<20} {'Train':<12} {'Test':<12}\")\nprint(\"-\"*50)\nprint(f\"{'MultinomialNB':<20} {nb_train_acc:<12.3f} {nb_test_acc:<12.3f}\")\nprint(f\"{'LogisticRegression':<20} {lr_train_acc:<12.3f} {lr_test_acc:<12.3f}\")\nprint(\"=\"*50)\n\nif lr_test_acc > nb_test_acc:\n    print(f\"\\n✓ LogisticRegression performs better (+{(lr_test_acc - nb_test_acc):.3f})\")\nelse:\n    print(f\"\\n✓ MultinomialNB performs better (+{(nb_test_acc - lr_test_acc):.3f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 11 - Build Complete Spam Classifier Pipeline from Scratch\n",
    "\n",
    "**Goal:** Build an end-to-end spam classifier with data preprocessing, training, and evaluation.\n",
    "\n",
    "**Concepts:** Complete pipeline workflow, data splitting, training, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Spam classifier data\nemails = [\n    # Spam (1)\n    \"Click here to win free money now!\",\n    \"You have won the lottery! Claim your prize\",\n    \"Limited time offer: 99% off everything\",\n    \"Congratulations! You are selected for a prize\",\n    \"Act now! Special deal expires today\",\n    \"Get rich quick with our secret method\",\n    # Ham (0)\n    \"Meeting scheduled for tomorrow at 3 PM\",\n    \"Can you review the attached document?\",\n    \"Project update: Phase 1 is complete\",\n    \"Please confirm your attendance for the event\",\n    \"Here are the quarterly results\",\n    \"Thanks for your email, I will respond soon\"\n]\n\nlabels = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]  # 1=spam, 0=ham\n\n# TODO: Step 1 - Split data (80-20 split, random_state=42)\nX_train, X_test, y_train, y_test = ___\n\n# TODO: Step 2 - Create pipeline with TfidfVectorizer and MultinomialNB\nspam_pipe = Pipeline([\n    ___,\n    ___\n])\n\n# TODO: Step 3 - Train the pipeline\n___\n\n# TODO: Step 4 - Make predictions\ny_pred = ___\n\n# TODO: Step 5 - Calculate accuracy\naccuracy = ___\n\n# TODO: Step 6 - Print confusion matrix and classification report\nprint(f\"Spam Classifier Accuracy: {accuracy:.2f}\")\nprint(f\"\\nConfusion Matrix:\")\nprint___\nprint(f\"\\nClassification Report:\")\nprint___\n\n# Test on new emails\nnew_emails = [\n    \"You have been selected to receive $1000\",\n    \"Can you help me with the project?\"\n]\n\n# TODO: Predict on new emails\nnew_preds = ___\n\nprint(f\"\\nNew Email Classification:\")\nfor email, pred in zip(new_emails, new_preds):\n    label = \"Spam\" if pred == 1 else \"Ham\"\n    print(f\"  [{label}] {email}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 12 - Use cross_val_score for Model Evaluation\n",
    "\n",
    "**Goal:** Use cross-validation to get more reliable performance estimates.\n",
    "\n",
    "**Concepts:** cross_val_score, k-fold cross-validation, model reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "texts = [\n    \"Excellent product\", \"Very satisfied\", \"Love it\", \"Highly recommend\", \"Amazing quality\",\n    \"Terrible\", \"Disappointed\", \"Waste of money\", \"Very bad\", \"Horrible\"\n]\nlabels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n\n# Create pipeline\npipe = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words='english')),\n    ('classifier', MultinomialNB())\n])\n\n# TODO: Use cross_val_score with cv=5 folds\n# This will train 5 different models and return 5 accuracy scores\ncv_scores = ___\n\nprint(f\"Cross-Validation Results (5-fold):\")\nprint(f\"Fold scores: {cv_scores}\")\nprint(f\"Mean accuracy: {cv_scores.mean():.3f}\")\nprint(f\"Std deviation: {cv_scores.std():.3f}\")\n\n# Interpretation:\nprint(f\"\\nInterpretation:\")\nprint(f\"  - Mean shows average performance: {cv_scores.mean():.1%}\")\nprint(f\"  - Std shows consistency (lower is better): {cv_scores.std():.3f}\")\nif cv_scores.std() < 0.1:\n    print(f\"  ✓ Model is consistent across all folds\")\nelse:\n    print(f\"  ⚠️  Model performance varies across folds\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 13 - Compare MultinomialNB vs LogisticRegression Performance\n",
    "\n",
    "**Goal:** Use cross-validation to fairly compare two different algorithms.\n",
    "\n",
    "**Concepts:** algorithm comparison, cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use the same data from Exercise 12\n# texts and labels should be in memory\n\n# Create pipeline with MultinomialNB\nnb_pipe = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words='english')),\n    ('classifier', MultinomialNB())\n])\n\n# Create pipeline with LogisticRegression\nlr_pipe = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words='english')),\n    ('classifier', LogisticRegression(max_iter=200))\n])\n\n# TODO: Get cross-validation scores for both pipelines (cv=5)\nnb_scores = ___\nlr_scores = ___\n\n# Print comparison\nprint(\"Algorithm Comparison (5-fold Cross-Validation):\")\nprint(\"=\"*60)\nprint(f\"{'Algorithm':<25} {'Mean CV':<15} {'Std':<15}\")\nprint(\"-\"*60)\nprint(f\"{'MultinomialNB':<25} {nb_scores.mean():<15.3f} {nb_scores.std():<15.3f}\")\nprint(f\"{'LogisticRegression':<25} {lr_scores.mean():<15.3f} {lr_scores.std():<15.3f}\")\nprint(\"=\"*60)\n\n# Determine winner\nif nb_scores.mean() > lr_scores.mean():\n    winner = \"MultinomialNB\"\n    diff = nb_scores.mean() - lr_scores.mean()\nelse:\n    winner = \"LogisticRegression\"\n    diff = lr_scores.mean() - nb_scores.mean()\n\nprint(f\"\\nWinner: {winner} (+{diff:.3f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 14 - Tune TfidfVectorizer Parameters\n",
    "\n",
    "**Goal:** Optimize model performance by tuning vectorizer parameters.\n",
    "\n",
    "**Concepts:** max_features, ngram_range, hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "texts = [\n    \"This is an amazing product\", \"Love the quality and service\", \"Excellent choice\",\n    \"Terrible experience\", \"Very disappointed\", \"Do not recommend\",\n    \"Great value for money\", \"Outstanding performance\", \"Best purchase ever\",\n    \"Worst decision ever\", \"Complete waste of time\", \"Very poor quality\"\n]\nlabels = [1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0]\n\n# Split data\nX_train, X_test, y_train, y_test = ___\n\n# Test different configurations\nconfigs = [\n    {'max_features': 50, 'ngram_range': (1, 1)},\n    {'max_features': 100, 'ngram_range': (1, 1)},\n    {'max_features': 50, 'ngram_range': (1, 2)},\n    {'max_features': 100, 'ngram_range': (1, 2)}\n]\n\nresults = []\n\nfor config in configs:\n    # TODO: Create pipeline with the given config parameters\n    pipe = Pipeline([\n        ('tfidf', TfidfVectorizer(\n            stop_words='english',\n            max_features=___,\n            ngram_range=___\n        )),\n        ('classifier', MultinomialNB())\n    ])\n    \n    # Train and evaluate\n    pipe.fit(X_train, y_train)\n    # TODO: Calculate test accuracy\n    accuracy = ___\n    \n    results.append({\n        'max_features': config['max_features'],\n        'ngram_range': config['ngram_range'],\n        'accuracy': accuracy\n    })\n\n# Print results\nprint(\"Hyperparameter Tuning Results:\")\nprint(\"=\"*60)\nfor res in results:\n    print(f\"max_features={res['max_features']:3d}, ngram={res['ngram_range']} → Accuracy: {res['accuracy']:.3f}\")\n\n# Find best configuration\nbest = ___\nprint(f\"\\nBest configuration: max_features={best['max_features']}, ngram={best['ngram_range']}\")\nprint(f\"Best accuracy: {best['accuracy']:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 15 - Build Prediction Function with Confidence\n",
    "\n",
    "**Goal:** Create a reusable function that predicts labels and returns confidence scores.\n",
    "\n",
    "**Concepts:** Functions, predict_proba(), confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create training data\ntexts = [\n    \"I love this\", \"Amazing product\", \"Excellent service\", \"Very satisfied\",\n    \"I hate this\", \"Terrible product\", \"Bad experience\", \"Very unhappy\"\n]\nlabels = [1, 1, 1, 1, 0, 0, 0, 0]\n\n# Create and train pipeline\npipe = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words='english')),\n    ('classifier', MultinomialNB())\n])\npipe.fit(texts, labels)\n\n# TODO: Create a function called 'predict_with_confidence' that:\n# 1. Takes a text string as input\n# 2. Returns a dictionary with:\n#    - 'label': predicted class (0 or 1)\n#    - 'sentiment': 'Positive' or 'Negative' (mapped from label)\n#    - 'confidence': probability of the predicted class (0.0 to 1.0)\n# 3. Handles confidence values properly\n\ndef predict_with_confidence(text):\n    \"\"\"\n    Predict sentiment with confidence score.\n    \n    Args:\n        text (str): Input text to classify\n        \n    Returns:\n        dict: {'label': int, 'sentiment': str, 'confidence': float}\n    \"\"\"\n    # TODO: Get prediction\n    pred = ___\n    \n    # TODO: Get probability scores\n    proba = ___\n    \n    # TODO: Extract confidence (maximum probability)\n    confidence = ___\n    \n    # TODO: Map label to sentiment\n    sentiment = ___  # 'Positive' if pred == 1 else 'Negative'\n    \n    return {\n        'label': pred,\n        'sentiment': sentiment,\n        'confidence': confidence\n    }\n\n# Test the function\ntest_texts = [\n    \"This product is amazing!\",\n    \"I really hate this\",\n    \"It's okay, not great\"\n]\n\nprint(\"Predictions with Confidence:\")\nprint(\"=\"*70)\nfor text in test_texts:\n    result = predict_with_confidence(text)\n    print(f\"Text: {text}\")\n    print(f\"  Prediction: {result['sentiment']}\")\n    print(f\"  Confidence: {result['confidence']:.3f}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Text Classification** converts documents to categories using machine learning\n",
    "- **Naive Bayes** is fast and effective for text classification tasks\n",
    "- **Pipelines** combine feature extraction and classification into a single workflow\n",
    "- **Evaluation metrics** (accuracy, precision, recall, F1) measure classifier performance\n",
    "- **Train/test split** prevents overfitting and provides reliable performance estimates\n",
    "- **Cross-validation** gives more reliable performance estimates than single splits\n",
    "- **Hyperparameter tuning** can improve model performance significantly\n",
    "- **Confidence scores** help assess model certainty in predictions\n",
    "\n",
    "### Common Use Cases:\n",
    "- Spam detection\n",
    "- Sentiment analysis\n",
    "- Topic categorization\n",
    "- Intent classification\n",
    "- Document routing\n",
    "- Fake news detection\n",
    "\n",
    "### What's Next:\n",
    "Tomorrow we'll apply these skills to **Fake News Detection** using real-world data!\n",
    "\n",
    "---\n",
    "\n",
    "*Created for Natruja's NLP study plan*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}