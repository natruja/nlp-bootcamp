{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Day 7: Fake News Detection\n",
    "**The AI Engineer Course 2026 - Section 27**\n",
    "\n",
    "**Student:** Natruja\n",
    "\n",
    "**Date:** Wednesday, February 18, 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "1. Build a fake news detection system\n",
    "2. Work with real datasets\n",
    "3. Use advanced classifiers (Logistic Regression)\n",
    "4. Handle imbalanced data\n",
    "5. Evaluate with multiple metrics and confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup: Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install required libraries\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn\", \"pandas\", \"nltk\", \"-q\"])\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "print(\"✓ Libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## The Fake News Problem\n",
    "\n",
    "### What is Fake News?\n",
    "- Deliberately false or misleading information\n",
    "- Spread for political, commercial, or social gain\n",
    "- Difficult to distinguish from real news\n",
    "\n",
    "### Real-World Impact:\n",
    "- Influences elections\n",
    "- Spreads health misinformation\n",
    "- Creates public distrust\n",
    "- Can cause real harm\n",
    "\n",
    "### NLP Solution:\n",
    "- Analyze text patterns\n",
    "- Identify suspicious language\n",
    "- Flag unreliable sources\n",
    "- Help fact-checkers prioritize\n",
    "\n",
    "### Challenges:\n",
    "- Fake news is evolving\n",
    "- Context matters\n",
    "- Satire vs. actual misinformation\n",
    "- Language bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Sample Dataset: Create Your Own Fake/Real News Data\n",
    "\n",
    "Since you may not have a real fake news CSV file, we'll create sample data with realistic fake and real headlines.\n",
    "This lets you learn on real examples without needing external files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample fake and real news datasets\n",
    "# These are representative examples of common patterns in fake vs real news\n",
    "\n",
    "fake_texts = [\n",
    "    \"SHOCKING: Celebrities secretly aliens confirmed by leaked documents! You won't believe what we found!\",\n",
    "    \"UNBELIEVABLE: Free money from government! This will CHANGE YOUR LIFE! Click here before it's deleted!\",\n",
    "    \"You won't BELIEVE what this senator is hiding! Mainstream media refuses to cover this BOMBSHELL!\",\n",
    "    \"Doctors HATE this simple trick that cures cancer! Big pharma trying to HIDE the truth!\",\n",
    "    \"BREAKING: This will DESTROY the mainstream narrative! The government DOESN'T want you knowing!\",\n",
    "    \"SHOCKING SCANDAL: Billionaire caught in bizarre conspiracy! This is the BIGGEST story of the year!\",\n",
    "    \"This SHOCKING evidence proves the election was RIGGED! The mainstream media is LYING to you!\",\n",
    "    \"UNBELIEVABLE: This celebrity just did something SHOCKING that left everyone speechless!\",\n",
    "    \"INCREDIBLE: Scientists discover FOUNTAIN OF YOUTH! Big pharma trying to hide this discovery!\",\n",
    "    \"You won't BELIEVE what happened next! This viral story will leave you in SHOCK!\",\n",
    "    \"EXPOSED: Government conspiracy revealed! They don't want you seeing this video!\",\n",
    "    \"BREAKING NEWS: This will CHANGE EVERYTHING! World leaders meeting in SECRET revealed!\"\n",
    "]\n",
    "\n",
    "real_texts = [\n",
    "    \"Reuters: President signs new economic policy bill. Officials explain implementation timeline starting next month.\",\n",
    "    \"AP: Researchers publish study in Nature journal. Peer review confirms findings on climate change impacts.\",\n",
    "    \"In a groundbreaking study, scientists found a promising new treatment. Clinical trials showed positive results.\",\n",
    "    \"Government health agency announces vaccine safety data. Comprehensive review of millions of doses administered.\",\n",
    "    \"Leading researchers at Stanford University published findings in peer-reviewed journal this week.\",\n",
    "    \"Official statement from Ministry of Health regarding new policy. Implementation begins in Q2 2026.\",\n",
    "    \"Scientists at MIT conducted research on renewable energy. Results published in quarterly scientific report.\",\n",
    "    \"International health organization releases guidance based on evidence. Multiple countries adopt recommendations.\",\n",
    "    \"University research team announces discovery. Methodology and data now available for peer review.\",\n",
    "    \"Press release from government agency. Officials confirmed findings after months of investigation.\",\n",
    "    \"Academic journal publishes peer-reviewed research. Authors describe their methodology in detail.\",\n",
    "    \"Health authorities provide guidance based on clinical evidence. Public consultation period begins Friday.\"\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "data = {\n",
    "    'text': fake_texts + real_texts,\n",
    "    'label': [1] * len(fake_texts) + [0] * len(real_texts)  # 1=fake, 0=real\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Shuffle the data\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Sample Fake News Detection Dataset Created!\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Example: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze features that distinguish fake from real news\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['exclamation_marks'] = df['text'].str.count('!')\n",
    "df['caps_ratio'] = df['text'].str.count(r'[A-Z]') / df['text'].str.len()\n",
    "df['all_caps_words'] = df['text'].str.count(r'\\b[A-Z]+\\b')\n",
    "\n",
    "print(\"Feature Analysis: Real News vs Fake News\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "real_news = df[df['label'] == 0]\n",
    "fake_news = df[df['label'] == 1]\n",
    "\n",
    "print(f\"\\nText Length (characters):\")\n",
    "print(f\"  Real: {real_news['text_length'].mean():.1f} avg\")\n",
    "print(f\"  Fake: {fake_news['text_length'].mean():.1f} avg\")\n",
    "\n",
    "print(f\"\\nExclamation Marks:\")\n",
    "print(f\"  Real: {real_news['exclamation_marks'].mean():.2f} avg\")\n",
    "print(f\"  Fake: {fake_news['exclamation_marks'].mean():.2f} avg\")\n",
    "\n",
    "print(f\"\\nAll-CAPS Words:\")\n",
    "print(f\"  Real: {real_news['all_caps_words'].mean():.2f} avg\")\n",
    "print(f\"  Fake: {fake_news['all_caps_words'].mean():.2f} avg\")\n",
    "\n",
    "print(f\"\\nCapital Letters Ratio:\")\n",
    "print(f\"  Real: {real_news['caps_ratio'].mean():.3f} avg\")\n",
    "print(f\"  Fake: {fake_news['caps_ratio'].mean():.3f} avg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Example: Building a Fake News Detector with Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "texts = df['text'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "# Split data (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    label_name = \"Real\" if u == 0 else \"Fake\"\n",
    "    print(f\"  {label_name}: {c} ({100*c/len(y_train):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Example: Logistic Regression with TfidfVectorizer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=1000, stop_words='english', max_df=0.7)),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Train\n",
    "print(\"Training classifier...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"✓ Training complete!\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred_proba = pipeline.predict_proba(X_test)\n",
    "\n",
    "print(f\"\\nSample predictions on test set:\")\n",
    "for i, (text, pred, proba) in enumerate(zip(X_test[:3], y_pred[:3], y_pred_proba[:3])):\n",
    "    label = \"REAL\" if pred == 0 else \"FAKE\"\n",
    "    confidence = proba[pred]\n",
    "    print(f\"\\nSample {i+1}: {label} (Confidence: {confidence:.2f})\")\n",
    "    print(f\"  Text: {text[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Example: Comprehensive Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "print(\"FAKE NEWS DETECTOR - EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  Accuracy:  {accuracy:.2f}\")\n",
    "print(f\"  Precision: {precision:.2f}\")\n",
    "print(f\"  Recall:    {recall:.2f}\")\n",
    "print(f\"  F1-Score:  {f1:.2f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc:.2f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"  True Negatives (Correct Real):  {cm[0][0]}\")\n",
    "print(f\"  False Positives (Real→Fake):    {cm[0][1]}\")\n",
    "print(f\"  False Negatives (Fake→Real):    {cm[1][0]}\")\n",
    "print(f\"  True Positives (Correct Fake):  {cm[1][1]}\")\n",
    "\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Real', 'Fake']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "# EXERCISES: 15 Progressive Challenges\n",
    "\n",
    "Complete all 5 exercises in each difficulty level. Each exercise has TODO comments and blanks (___) for you to fill in.\n",
    "\n",
    "**Key Concepts to Use:**\n",
    "- pd.read_csv(), df.shape, df['label'].value_counts()\n",
    "- LogisticRegression, TfidfVectorizer(max_df=0.7)\n",
    "- Pipeline, train_test_split, model.score()\n",
    "- confusion_matrix, classification_report, predict_proba\n",
    "- feature importance (coef_), random_state for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 1 - Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Load the dataframe we created earlier (it's already in memory as 'df')\n# TODO: Print the shape using df.shape\n# TODO: Print total number of samples\n# TODO: Print class distribution using df['label'].value_counts()\n\nprint(f\"Dataset shape: {___}\")\nprint(f\"Total samples: {___}\")\nprint(f\"\\nClass distribution:\")\nprint___"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10b",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 2 - Check Column Names and Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11b",
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Print column names\n# TODO: Print data types using df.dtypes\n# TODO: Print first 3 rows using df.head(3)\n\nprint(\"Column names:\")\nprint___\nprint(f\"\\nData types:\")\nprint___\nprint(f\"\\nFirst 3 rows:\")\nprint___"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 3 - Basic Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Split df['text'] and df['label'] into 80-20 train/test\n# TODO: Use train_test_split with test_size=0.2 and random_state=42\n# TODO: Assign to X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = ______\n\nprint(f\"Training samples: {len(X_train)}\")\nprint(f\"Test samples: {len(X_test)}\")\nprint(f\"Total: {len(X_train) + len(X_test)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 4 - Train Simple Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Create a Pipeline with:\n#   1. TfidfVectorizer(max_features=1000, stop_words='english')\n#   2. LogisticRegression(max_iter=1000, random_state=42)\n# TODO: Fit the pipeline on X_train, y_train\n# TODO: Get accuracy score on test set using model.score()\n\nsimple_pipeline = Pipeline([\n    ('tfidf', ___,\n    ('classifier', ___\n])\n\n# Train\n___\n\n# Evaluate\naccuracy = ___\nprint(f\"Model Accuracy: {accuracy:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 5 - Make Predictions on New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Use the pipeline from Exercise 4 to predict on a new article\n# TODO: Make a prediction on this text: \"Scientists publish findings in peer-reviewed journal\"\n# TODO: Use pipeline.predict() to get the class\n# TODO: Print the result (0=real, 1=fake)\n\nnew_text = \"Scientists publish findings in peer-reviewed journal\"\n\nprediction = ___\nlabel = \"REAL\" if prediction[0] == 0 else \"FAKE\"\n\nprint(f\"Text: {new_text}\")\nprint(f\"Prediction: {label}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "## ⭐⭐ MEDIUM: Exercise 6 - Build Pipeline with max_df Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Create a Pipeline with TfidfVectorizer that has max_df=0.7\n# max_df=0.7 removes words that appear in more than 70% of documents\n# TODO: Use LogisticRegression with max_iter=1000 and random_state=42\n# TODO: Train on X_train, y_train\n# TODO: Get accuracy on test set\n\npipeline_with_maxdf = Pipeline([\n    ('tfidf', TfidfVectorizer(max_features=1000, stop_words='english', max_df=___),\n    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n])\n\n___  # Train the pipeline\n\naccuracy_maxdf = ___  # Get accuracy score\nprint(f\"Accuracy with max_df=0.7: {accuracy_maxdf:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 7 - Generate Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Make predictions on X_test using pipeline_with_maxdf\n# TODO: Calculate confusion matrix using confusion_matrix(y_test, y_pred)\n# TODO: Print TN, FP, FN, TP values\n\ny_pred_medium = ___  # Get predictions\n\ncm = ___  # Calculate confusion matrix\n\ntn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n\nprint(\"Confusion Matrix Analysis:\")\nprint(f\"  True Negatives (correctly identified REAL):  {___}\")\nprint(f\"  False Positives (real classified as FAKE):   {___}\")\nprint(f\"  False Negatives (fake classified as REAL):   {___}\")\nprint(f\"  True Positives (correctly identified FAKE):  {___}\")\nprint(f\"\\nInterpretation:\")\nprint(f\"  Correctly classified: {tn + tp} out of {len(y_test)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 8 - Compare Train vs Test Accuracy (Overfitting Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Get accuracy on TRAINING data using model.score(X_train, y_train)\n# TODO: Get accuracy on TEST data using model.score(X_test, y_test)\n# TODO: Calculate the gap (overfitting indicator)\n# TODO: Print both and compare\n\ntrain_acc = ___  # Score on training data\ntest_acc = ___   # Score on test data\n\ngap = train_acc - test_acc\n\nprint(\"Overfitting Analysis:\")\nprint(f\"  Training Accuracy: {train_acc:.4f}\")\nprint(f\"  Test Accuracy:     {test_acc:.4f}\")\nprint(f\"  Gap (overfitting): {gap:.4f}\")\nprint(f\"\\nInterpretation:\")\nif gap < 0.05:\n    print(\"  ✓ Good fit - low overfitting\")\nelif gap < 0.15:\n    print(\"  ~ Moderate fit - some overfitting\")\nelse:\n    print(\"  ✗ Poor fit - significant overfitting\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 9 - Use random_state for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Create two pipelines with random_state=42\n# TODO: Split data TWICE with the same random_state\n# TODO: Train both models and verify they have identical performance\n# TODO: This proves random_state makes results reproducible\n\n# First split\nX_train_1, X_test_1, y_train_1, y_test_1 = ___\n\n# Second split (identical conditions)\nX_train_2, X_test_2, y_train_2, y_test_2 = ___\n\n# Train first model\npipe_1 = Pipeline([\n    ('tfidf', TfidfVectorizer(max_features=1000, stop_words='english', random_state=42)),\n    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n])\n___  # Fit pipe_1 on split 1\nacc_1 = ___  # Get score\n\n# Train second model\npipe_2 = Pipeline([\n    ('tfidf', TfidfVectorizer(max_features=1000, stop_words='english', random_state=42)),\n    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n])\n___  # Fit pipe_2 on split 2\nacc_2 = ___  # Get score\n\nprint(\"Reproducibility Test:\")\nprint(f\"  Model 1 Accuracy: {acc_1:.4f}\")\nprint(f\"  Model 2 Accuracy: {acc_2:.4f}\")\nprint(f\"  Difference: {abs(acc_1 - acc_2):.4f}\")\nif abs(acc_1 - acc_2) == 0:\n    print(\"  ✓ Perfect reproducibility with random_state=42\")\nelse:\n    print(\"  Note: Any difference is due to non-deterministic operations\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 10 - Get Probability Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Use pipeline_with_maxdf to get predictions AND probabilities\n# TODO: Use predict_proba() to get confidence scores\n# TODO: Show predictions with confidence levels\n\ntest_examples = X_test[:5]  # First 5 test samples\ntrue_labels = y_test[:5]\n\npredictions = ___  # Use predict()\nprobabilities = ___  # Use predict_proba()\n\nprint(\"Predictions with Confidence Scores:\")\nprint(\"=\"*70)\nfor i, (text, pred, proba, true_label) in enumerate(zip(test_examples, predictions, probabilities, true_labels)):\n    pred_label = \"FAKE\" if pred == 1 else \"REAL\"\n    true_name = \"FAKE\" if true_label == 1 else \"REAL\"\n    confidence = proba[pred]\n    \n    print(f\"\\n{i+1}. Text: {text[:50]}...\")\n    print(f\"   Predicted: {pred_label} (confidence: {confidence:.3f})\")\n    print(f\"   True Label: {true_name}\")\n    print(f\"   Probabilities - Real: {proba[0]:.3f}, Fake: {proba[1]:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "## ⭐⭐⭐ HARD: Exercise 11 - Build Complete Fake News Detector Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Create a function that encapsulates the entire pipeline\n# TODO: It should:\n#   1. Take texts and labels as input\n#   2. Split data (80-20)\n#   3. Create and train pipeline\n#   4. Return the trained model\n\ndef build_fake_news_detector(texts, labels, test_size=0.2, random_state=42):\n    \"\"\"\n    Build a complete fake news detection model.\n    \n    Args:\n        texts: Array of text samples\n        labels: Array of labels (0=real, 1=fake)\n        test_size: Proportion of test set\n        random_state: For reproducibility\n    \n    Returns:\n        model: Trained pipeline\n        X_test: Test texts\n        y_test: Test labels\n    \"\"\"\n    # TODO: Split data\n    X_train, X_test, y_train, y_test = ______\n    \n    # TODO: Create pipeline with TfidfVectorizer and LogisticRegression\n    model = Pipeline([\n        ('tfidf', TfidfVectorizer(max_features=1000, stop_words='english', max_df=0.7)),\n        ('classifier', LogisticRegression(max_iter=1000, random_state=random_state))\n    ])\n    \n    # TODO: Train the model\n    ___\n    \n    return model, X_test, y_test\n\n# Test your function\ndetector_model, X_test_hard, y_test_hard = build_fake_news_detector(df['text'].values, df['label'].values)\nprint(f\"✓ Detector built successfully\")\nprint(f\"  Test set size: {len(X_test_hard)}\")\nprint(f\"  Model accuracy: {detector_model.score(X_test_hard, y_test_hard):.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 12 - Extract Feature Importance (Top Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Extract feature importance from the trained model\n# TODO: Get the top 10 words associated with FAKE news (positive coefficients)\n# TODO: Get the top 10 words associated with REAL news (negative coefficients)\n# TODO: Use model.named_steps to access the components\n\n# Get the Logistic Regression model and TfidfVectorizer\nlr_model = detector_model.named_steps['classifier']\ntfidf = detector_model.named_steps['tfidf']\n\n# Get feature names\nfeature_names = ___  # Use tfidf.get_feature_names_out()\n\n# Get coefficients (importance scores)\ncoefficients = ___  # Use lr_model.coef_[0]\n\n# Get top 10 FAKE news words (highest positive coefficients)\nfake_indices = np.argsort(coefficients)[-10:][::-1]\nfake_words = feature_names[fake_indices]\nfake_scores = coefficients[fake_indices]\n\n# Get top 10 REAL news words (lowest coefficients)\nreal_indices = np.argsort(coefficients)[:10]\nreal_words = feature_names[real_indices]\nreal_scores = coefficients[real_indices]\n\nprint(\"Feature Importance Analysis\")\nprint(\"=\"*60)\nprint(f\"\\nTop 10 words strongly associated with FAKE news:\")\nfor word, score in zip(fake_words, fake_scores):\n    print(f\"  {word:20s} (score: {score:7.3f})\")\n\nprint(f\"\\nTop 10 words strongly associated with REAL news:\")\nfor word, score in zip(real_words, real_scores):\n    print(f\"  {word:20s} (score: {score:7.3f})\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 13 - Confidence Scores with predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Use predict_proba to get confidence scores\n# TODO: Identify predictions with LOW confidence (close to 0.5)\n# TODO: Identify predictions with HIGH confidence (close to 0 or 1)\n# TODO: Show examples of uncertain vs confident predictions\n\ny_pred_hard = detector_model.predict(X_test_hard)\ny_pred_proba_hard = ___  # Use predict_proba()\n\n# Extract confidence for each prediction\nconfidence = np.max(y_pred_proba_hard, axis=1)\nuncertainty = np.abs(y_pred_proba_hard[:, 0] - y_pred_proba_hard[:, 1])\n\n# Find uncertain predictions (confidence close to 0.5)\nuncertain_indices = np.where(uncertainty < 0.2)[0]\nconfident_indices = np.where(uncertainty >= 0.8)[0]\n\nprint(\"Confidence Analysis\")\nprint(\"=\"*70)\nprint(f\"\\nTotal predictions: {len(X_test_hard)}\")\nprint(f\"Uncertain predictions (confidence 0.5-0.6): {len(uncertain_indices)}\")\nprint(f\"Confident predictions (confidence >0.9): {len(confident_indices)}\")\n\nprint(f\"\\n3 Most UNCERTAIN predictions:\")\nfor idx in uncertain_indices[:3]:\n    text = X_test_hard[idx]\n    pred = \"FAKE\" if y_pred_hard[idx] == 1 else \"REAL\"\n    conf_real, conf_fake = y_pred_proba_hard[idx]\n    print(f\"\\n  Text: {text[:50]}...\")\n    print(f\"  Prediction: {pred} | Real: {conf_real:.3f} | Fake: {conf_fake:.3f}\")\n\nprint(f\"\\n3 Most CONFIDENT predictions:\")\nfor idx in confident_indices[:3]:\n    text = X_test_hard[idx]\n    pred = \"FAKE\" if y_pred_hard[idx] == 1 else \"REAL\"\n    conf_real, conf_fake = y_pred_proba_hard[idx]\n    print(f\"\\n  Text: {text[:50]}...\")\n    print(f\"  Prediction: {pred} | Real: {conf_real:.3f} | Fake: {conf_fake:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 14 - Parameter Tuning and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Train multiple models with different max_df values\n# TODO: Compare their accuracy scores\n# TODO: See how max_df affects model performance\n\nmax_df_values = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\nresults = []\n\nfor max_df in max_df_values:\n    # TODO: Create pipeline with this max_df value\n    pipeline_temp = Pipeline([\n        ('tfidf', TfidfVectorizer(max_features=1000, stop_words='english', max_df=___),\n        ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n    ])\n    \n    # TODO: Train on X_train_hard, y_train_hard (from exercise 11)\n    # First, get these from the detector_model training\n    X_train_hard, _, y_train_hard, _ = ___\n    \n    ___  # Fit the pipeline\n    \n    # TODO: Get test accuracy\n    acc = ___\n    results.append({'max_df': max_df, 'accuracy': acc})\n\nprint(\"Parameter Tuning Results (max_df effect):\")\nprint(\"=\"*50)\nprint(f\"{'max_df':<10} {'Accuracy':<10} {'Status'}\")\nprint(\"-\"*50)\n\nbest_accuracy = 0\nbest_max_df = 0\n\nfor result in results:\n    max_df = result['max_df']\n    acc = result['accuracy']\n    status = \"Best\" if acc == max(r['accuracy'] for r in results) else \"\"\n    print(f\"{max_df:<10.1f} {acc:<10.4f} {status}\")\n    \n    if acc > best_accuracy:\n        best_accuracy = acc\n        best_max_df = max_df\n\nprint(\"-\"*50)\nprint(f\"Best max_df: {best_max_df} with accuracy {best_accuracy:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 15 - Full Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Build a complete evaluation function that:\n#   1. Gets predictions and probabilities\n#   2. Calculates all metrics (accuracy, precision, recall, f1, roc_auc)\n#   3. Generates confusion matrix\n#   4. Generates classification report\n#   5. Returns comprehensive dictionary\n\ndef evaluate_fake_news_detector(model, X_test, y_test):\n    \"\"\"\n    Generate complete evaluation report for fake news detector.\n    \n    Args:\n        model: Trained pipeline\n        X_test: Test texts\n        y_test: Test labels\n    \n    Returns:\n        Dictionary with all evaluation metrics\n    \"\"\"\n    # TODO: Get predictions\n    y_pred = ___\n    \n    # TODO: Get probability predictions\n    y_proba = ___\n    \n    # TODO: Calculate metrics\n    metrics = {\n        'accuracy': ___,\n        'precision': ___,\n        'recall': ___,\n        'f1': ___,\n        'roc_auc': ___\n    }\n    \n    # TODO: Get confusion matrix\n    cm = ___\n    \n    # TODO: Get classification report\n    report = ___\n    \n    return {\n        'metrics': metrics,\n        'confusion_matrix': cm,\n        'classification_report': report\n    }\n\n# Test your evaluation function\nfinal_report = ___\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"COMPLETE FAKE NEWS DETECTOR - FINAL EVALUATION REPORT\")\nprint(\"=\"*70)\n\nprint(f\"\\nKey Metrics:\")\nfor metric, value in final_report['metrics'].items():\n    print(f\"  {metric.upper():12s}: {value:.4f}\")\n\nprint(f\"\\nConfusion Matrix:\")\ncm = final_report['confusion_matrix']\nprint(f\"  TN (Correct Real):    {cm[0][0]:3d}\")\nprint(f\"  FP (Real→Fake):       {cm[0][1]:3d}\")\nprint(f\"  FN (Fake→Real):       {cm[1][0]:3d}\")\nprint(f\"  TP (Correct Fake):    {cm[1][1]:3d}\")\n\nprint(f\"\\nClassification Report:\")\nprint(final_report['classification_report'])\n\nprint(\"=\"*70)\nprint(\"✓ Evaluation Complete!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "## Congratulations! You've Completed All 15 Exercises\n",
    "\n",
    "### What You Learned:\n",
    "\n",
    "**EASY (Exercises 1-5):** Foundational skills\n",
    "- Loading and exploring datasets with pandas\n",
    "- Understanding data shape and class distribution\n",
    "- Splitting data into training and testing sets\n",
    "- Training basic Logistic Regression models\n",
    "- Making predictions on new text\n",
    "\n",
    "**MEDIUM (Exercises 6-10):** Intermediate NLP pipelines\n",
    "- Building sophisticated pipelines with TfidfVectorizer\n",
    "- Using max_df parameter to remove common words\n",
    "- Generating and interpreting confusion matrices\n",
    "- Detecting overfitting by comparing train/test accuracy\n",
    "- Ensuring reproducibility with random_state\n",
    "- Understanding confidence scores with predict_proba()\n",
    "\n",
    "**HARD (Exercises 11-15):** Advanced techniques\n",
    "- Encapsulating complex pipelines in reusable functions\n",
    "- Extracting feature importance to understand what the model learned\n",
    "- Analyzing prediction confidence and uncertainty\n",
    "- Hyperparameter tuning and model comparison\n",
    "- Building comprehensive evaluation reports\n",
    "\n",
    "### Key Concepts Mastered:\n",
    "- **pd.read_csv()** - Loading CSV data\n",
    "- **df.shape, df['label'].value_counts()** - Data exploration\n",
    "- **train_test_split()** - Creating train/test splits\n",
    "- **TfidfVectorizer(max_df=0.7)** - Text feature extraction\n",
    "- **Pipeline** - Building reusable ML workflows\n",
    "- **LogisticRegression** - Binary classification\n",
    "- **model.score()** - Quick accuracy assessment\n",
    "- **predict_proba()** - Confidence scores\n",
    "- **confusion_matrix, classification_report** - Comprehensive metrics\n",
    "- **Feature importance (coef_)** - Model interpretability\n",
    "- **random_state** - Reproducibility\n",
    "\n",
    "### Real-World Applications:\n",
    "- Detecting misinformation on social media\n",
    "- Filtering spam and scam messages\n",
    "- Identifying unreliable news sources\n",
    "- Prioritizing content for human fact-checkers\n",
    "- Building content moderation systems\n",
    "\n",
    "### What's Next:\n",
    "Tomorrow is **Day 8: Final Review** - we'll recap all concepts and explore modern NLP with Transformers and LLMs!\n",
    "\n",
    "---\n",
    "*Created for Natruja's NLP study plan*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}