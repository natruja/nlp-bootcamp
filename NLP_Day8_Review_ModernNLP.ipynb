{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 8: Full Review + Transformers & Modern NLP\n",
    "**The AI Engineer Course 2026 - Sections 28 + Advanced Topics**\n",
    "\n",
    "**Student:** Natruja\n",
    "\n",
    "**Date:** Thursday, February 19, 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "1. Review all NLP concepts from Days 1-7\n",
    "2. Practice exercises covering all topics\n",
    "3. Understand modern approaches (Transformers)\n",
    "4. Learn about Large Language Models (LLMs)\n",
    "5. Explore the future of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install libraries\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\", \"scikit-learn\", \"spacy\", \"textblob\", \"-q\"])\n",
    "\n",
    "# Download data\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "# Download spacy model\n",
    "subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\", \"-q\"])\n",
    "\n",
    "print(\"✓ All libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: COMPREHENSIVE NLP REVIEW\n",
    "\n",
    "## Complete NLP Pipeline Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 1-2: Text Preprocessing\n",
    "- **Tokenization**: Breaking text into words/sentences\n",
    "- **Stop Words**: Removing common, low-value words\n",
    "- **Stemming**: Reducing words to root form (fast, approximate)\n",
    "- **Lemmatization**: Converting to dictionary form (accurate)\n",
    "- **Regex**: Pattern matching and text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 3: Information Extraction\n",
    "- **POS Tagging**: Identifying word roles (NOUN, VERB, etc.)\n",
    "- **NER**: Extracting named entities (people, places, organizations)\n",
    "- **Noun Chunks**: Identifying noun phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 4: Sentiment Analysis\n",
    "- **TextBlob**: Simple sentiment scoring\n",
    "- **VADER**: Social media sentiment analysis\n",
    "- **Classification**: Positive, negative, neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 5: Text Representation\n",
    "- **Bag of Words**: Word frequency vectors\n",
    "- **TF-IDF**: Weighted importance scores\n",
    "- **LDA**: Topic modeling (probabilistic)\n",
    "- **NMF**: Topic modeling (matrix decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 6-7: Text Classification\n",
    "- **Naive Bayes**: Fast, effective for text\n",
    "- **Logistic Regression**: Linear classification\n",
    "- **Pipelines**: Streamlined preprocessing + classification\n",
    "- **Evaluation**: Metrics and confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Complete Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_preprocessing(text):\n",
    "    \"\"\"Complete text preprocessing pipeline.\"\"\"\n",
    "    \n",
    "    # Step 1: Lowercase and basic cleaning\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    \n",
    "    # Step 2: Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Step 3: Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    # Step 4: Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Test\n",
    "sample = \"The quick brown foxes are running through the forest!\"\n",
    "processed = complete_preprocessing(sample)\n",
    "print(f\"Original: {sample}\")\n",
    "print(f\"Processed: {processed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: MODERN NLP & THE FUTURE\n",
    "\n",
    "## Beyond Traditional NLP: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Evolution of NLP\n",
    "\n",
    "**Traditional Approaches (Days 1-7):**\n",
    "- Rule-based and statistical methods\n",
    "- Manual feature engineering\n",
    "- Bag of Words, TF-IDF\n",
    "- Naive Bayes, SVM\n",
    "- Limited context understanding\n",
    "\n",
    "**Deep Learning Era (2012+):**\n",
    "- Neural networks learn features automatically\n",
    "- RNNs, LSTMs, GRUs\n",
    "- Word embeddings (Word2Vec, GloVe, FastText)\n",
    "- Sequence-to-sequence models\n",
    "\n",
    "**Transformer Era (2017+):**\n",
    "- Attention mechanism enables parallelization\n",
    "- BERT, GPT, T5\n",
    "- Pre-trained models on massive data\n",
    "- Transfer learning for downstream tasks\n",
    "- Context-aware representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Transformers\n",
    "\n",
    "### Key Innovations:\n",
    "\n",
    "**1. Attention Mechanism**\n",
    "- Words \"attend to\" other relevant words\n",
    "- Weights show which words are important\n",
    "- Example: In \"The bank robber was caught\", \"bank\" attends to \"robber\" (context matters!)\n",
    "\n",
    "**2. Self-Attention**\n",
    "- Each word looks at all other words in sequence\n",
    "- Learns dependencies regardless of distance\n",
    "- Parallelizable (unlike RNNs)\n",
    "\n",
    "**3. Pre-training + Fine-tuning**\n",
    "- Pre-train on massive text (billions of words)\n",
    "- Fine-tune on specific task with small data\n",
    "- Transfer learning in NLP\n",
    "\n",
    "### Advantages:\n",
    "- Captures long-range dependencies\n",
    "- Parallelize training (faster)\n",
    "- State-of-the-art performance\n",
    "- Transfer learning works well\n",
    "- Understands context much better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular Transformer Models\n",
    "\n",
    "### BERT (Bidirectional Encoder Representations from Transformers)\n",
    "- Developed by Google (2018)\n",
    "- Reads text in both directions\n",
    "- Great for understanding tasks:\n",
    "  - Text classification\n",
    "  - Named entity recognition\n",
    "  - Question answering\n",
    "  - Semantic similarity\n",
    "\n",
    "### GPT (Generative Pre-trained Transformer)\n",
    "- Developed by OpenAI\n",
    "- Reads left-to-right (autoregressive)\n",
    "- Great for generation tasks:\n",
    "  - Text generation\n",
    "  - Creative writing\n",
    "  - Code generation\n",
    "  - Dialogue\n",
    "\n",
    "### T5 (Text-to-Text Transfer Transformer)\n",
    "- Developed by Google (2019)\n",
    "- All tasks as text-to-text\n",
    "- Can do:\n",
    "  - Translation\n",
    "  - Summarization\n",
    "  - Classification\n",
    "  - Question answering\n",
    "\n",
    "### Other Notable Models:\n",
    "- **RoBERTa**: Improved BERT\n",
    "- **ALBERT**: Efficient BERT\n",
    "- **DistilBERT**: Smaller, faster BERT\n",
    "- **ELECTRA**: Better pre-training method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Language Models (LLMs)\n",
    "\n",
    "### What are LLMs?\n",
    "- Massive neural networks trained on billions of words\n",
    "- Billions to hundreds of billions of parameters\n",
    "- Predict next word probability\n",
    "- Generate coherent, contextual text\n",
    "\n",
    "### Examples:\n",
    "- **GPT-4** (OpenAI): 1.8 trillion parameters\n",
    "- **Claude** (Anthropic): Focused on safety and reasoning\n",
    "- **Gemini** (Google): Multimodal (text + images)\n",
    "- **LLaMA** (Meta): Open-source foundation model\n",
    "- **Mistral** (European): Efficient open model\n",
    "\n",
    "### Capabilities:\n",
    "- Text generation and completion\n",
    "- Question answering\n",
    "- Code generation\n",
    "- Translation\n",
    "- Summarization\n",
    "- Reasoning and problem-solving\n",
    "- Creative writing\n",
    "- Multi-turn conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Understanding Attention (Conceptual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual attention visualization\n",
    "sentence = \"The bank robber was caught by police\"\n",
    "words = sentence.split()\n",
    "\n",
    "# Simulate attention weights (how much each word \"looks at\" other words)\n",
    "# Real transformers use learned attention, this is just for illustration\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"\\nAttention Visualization:\")\n",
    "print(\"(How much each word attends to 'bank')\\n\")\n",
    "\n",
    "# Attention weights for understanding 'bank'\n",
    "attention_for_bank = {\n",
    "    'the': 0.1,      # article, low relevance\n",
    "    'bank': 1.0,     # itself\n",
    "    'robber': 0.8,   # high relevance (who is robbing the bank?)\n",
    "    'was': 0.2,      # verb, some relevance\n",
    "    'caught': 0.3,   # action, moderate relevance\n",
    "    'by': 0.1,       # preposition\n",
    "    'police': 0.5    # relevant (who caught them?)\n",
    "}\n",
    "\n",
    "for word, weight in attention_for_bank.items():\n",
    "    bar = '█' * int(weight * 20)\n",
    "    print(f\"  {word:10} {bar} {weight:.1f}\")\n",
    "\n",
    "print(\"\\nKey Insight: 'bank' pays high attention to 'robber'\")\n",
    "print(\"This helps the model understand the context!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer vs Traditional Methods Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = {\n",
    "    'Aspect': ['Feature Engineering', 'Context Understanding', 'Training Time', 'Inference Speed', 'Data Requirements', 'Transfer Learning', 'Performance on Complex Tasks'],\n",
    "    'Traditional (Naive Bayes, SVM)': ['Manual', 'Limited', 'Fast', 'Very Fast', 'Small data ok', 'Difficult', 'Lower'],\n",
    "    'Deep Learning (RNN, LSTM)': ['Automatic', 'Moderate', 'Slow', 'Slow', 'Need more data', 'Possible', 'Good'],\n",
    "    'Transformers (BERT, GPT)': ['Automatic', 'Excellent', 'Very Slow', 'Fast', 'Massive data (pre-trained)', 'Excellent', 'State-of-the-art']\n",
    "}\n",
    "df = pd.DataFrame(comparison)\n",
    "print(\"\\nTRADITIONAL vs DEEP LEARNING vs TRANSFORMERS\")\n",
    "print(\"=\"*80)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Future of NLP\n",
    "\n",
    "### Current Trends (2024-2025):\n",
    "\n",
    "**1. Efficiency & Compression**\n",
    "- Smaller models for edge devices\n",
    "- Quantization and distillation\n",
    "- LoRA and parameter-efficient fine-tuning\n",
    "\n",
    "**2. Multimodal AI**\n",
    "- Text + Images (GPT-4V, Gemini)\n",
    "- Text + Audio (Whisper)\n",
    "- Unified representations\n",
    "\n",
    "**3. Open-Source Development**\n",
    "- LLaMA, Mistral becoming accessible\n",
    "- Democratizing AI development\n",
    "- Fine-tuning on consumer hardware\n",
    "\n",
    "**4. Specialized Models**\n",
    "- Domain-specific (medical, legal, financial)\n",
    "- Task-specific (code, multilingual, reasoning)\n",
    "- Mixture of Experts\n",
    "\n",
    "**5. Better Evaluation**\n",
    "- Beyond accuracy metrics\n",
    "- Bias and fairness testing\n",
    "- Robustness to adversarial examples\n",
    "- Human alignment\n",
    "\n",
    "**6. Reasoning & Planning**\n",
    "- Chain-of-thought prompting\n",
    "- Few-shot learning\n",
    "- In-context learning\n",
    "- Multi-step reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 3: COMPREHENSIVE EXERCISES BY DIFFICULTY\n",
    "\n",
    "**Note:** For exercises using the `transformers` library, you'll need to install it first:\n",
    "```bash\n",
    "pip install transformers torch\n",
    "```\n",
    "\n",
    "This may take a few minutes on first install. Torch is a large dependency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EASY (★) - Exercises 1-5\n",
    "\n",
    "These exercises review fundamental concepts from Days 1-7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ★ EASY: Exercise 1 - Tokenize and Remove Stop Words\n",
    "\n",
    "**Objective:** Review basic text preprocessing: tokenization and stop word removal.\n",
    "\n",
    "**Instructions:**\n",
    "- Tokenize the given text using NLTK\n",
    "- Remove stop words\n",
    "- Print the original and processed tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ★ EASY: Exercise 1 - Tokenize and Remove Stop Words\n\ndef exercise1_tokenize_remove_stopwords(text):\n    \"\"\"\n    TODO: Complete this function\n    1. Tokenize the text using word_tokenize()\n    2. Get English stop words from stopwords.words('english')\n    3. Filter out stop words from the tokens\n    4. Return both original and filtered tokens\n    \"\"\"\n    # Step 1: Tokenize\n    tokens = ___\n    \n    # Step 2: Get stop words\n    stop_words = ___\n    \n    # Step 3: Remove stop words\n    filtered_tokens = ___\n    \n    return tokens, filtered_tokens\n\n# Test your solution\ntest_text = \"Machine learning and artificial intelligence are transforming the world.\"\n# original, filtered = exercise1_tokenize_remove_stopwords(test_text)\n# print(f\"Original tokens: {original}\")\n# print(f\"After stop word removal: {filtered}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ★ EASY: Exercise 2 - Stem vs Lemmatize a Word\n",
    "\n",
    "**Objective:** Understand the difference between stemming and lemmatization.\n",
    "\n",
    "**Instructions:**\n",
    "- Apply stemming using PorterStemmer\n",
    "- Apply lemmatization using WordNetLemmatizer\n",
    "- Compare the results for different words\n",
    "- Note: Lemmatization is more accurate but slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ★ EASY: Exercise 2 - Stem vs Lemmatize a Word\n\ndef exercise2_stem_vs_lemmatize(words):\n    \"\"\"\n    TODO: Complete this function\n    For each word, apply:\n    1. stemmer.stem(word) - using the global stemmer object\n    2. lemmatizer.lemmatize(word) - using the global lemmatizer object\n    Return a dictionary with original, stem, and lemma for each word\n    \"\"\"\n    results = {}\n    \n    for word in words:\n        stemmed = ___\n        lemmatized = ___\n        \n        results[word] = {\n            'stem': stemmed,\n            'lemma': lemmatized\n        }\n    \n    return results\n\n# Test your solution\ntest_words = ['running', 'runs', 'ran', 'studies', 'studying', 'better']\n# results = exercise2_stem_vs_lemmatize(test_words)\n# for word, forms in results.items():\n#     print(f\"{word:12} -> stem: {forms['stem']:12} lemma: {forms['lemma']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ★ EASY: Exercise 3 - Match Concepts to Tools\n",
    "\n",
    "**Objective:** Review what each NLP tool does.\n",
    "\n",
    "**Instructions:**\n",
    "- Create a dictionary mapping NLP tasks/concepts to the tools that perform them\n",
    "- Include: tokenization, POS tagging, NER, sentiment analysis, vectorization, classification\n",
    "- For each tool, provide a brief description of what it does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ★ EASY: Exercise 3 - Match Concepts to Tools\n\ndef exercise3_concept_to_tool():\n    \"\"\"\n    TODO: Create a dictionary that maps NLP concepts to the tools/libraries that perform them\n    Structure: {\n        'concept_name': {\n            'tool': 'library/function name',\n            'description': 'what it does'\n        },\n        ...\n    }\n    \n    Include these concepts:\n    1. 'Tokenization' - Breaking text into words/sentences\n    2. 'POS Tagging' - Identifying parts of speech\n    3. 'Named Entity Recognition' - Extracting entities\n    4. 'Sentiment Analysis' - Analyzing emotions/opinions\n    5. 'Text Vectorization' - Converting text to numbers\n    6. 'Text Classification' - Categorizing text\n    \"\"\"\n    \n    concept_map = {\n        'Tokenization': {\n            'tool': ___,\n            'description': ___\n        },\n        'POS Tagging': {\n            'tool': ___,\n            'description': ___\n        },\n        # TODO: Add remaining 4 concepts\n    }\n    \n    return concept_map\n\n# Test your solution\n# concept_map = exercise3_concept_to_tool()\n# for concept, info in concept_map.items():\n#     print(f\"\\n{concept}:\")\n#     print(f\"  Tool: {info['tool']}\")\n#     print(f\"  Description: {info['description']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ★ EASY: Exercise 4 - Use Transformers Pipeline for Sentiment Analysis\n",
    "\n",
    "**Objective:** Learn to use the transformers library for sentiment analysis (Modern NLP approach).\n",
    "\n",
    "**Note:** This requires `pip install transformers torch` (if not already installed)\n",
    "\n",
    "**Instructions:**\n",
    "- Import the pipeline function from transformers\n",
    "- Create a sentiment-analysis pipeline\n",
    "- Test it on sample sentences\n",
    "- Compare with traditional VADER sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ★ EASY: Exercise 4 - Use Transformers Pipeline for Sentiment Analysis\n\ndef exercise4_transformers_sentiment(texts):\n    \"\"\"\n    TODO: Complete this function using the transformers library\n    1. Import pipeline from transformers\n    2. Create a 'sentiment-analysis' pipeline\n    3. Use the pipeline on each text\n    4. Return results with text, label, and score\n    \n    If you haven't installed transformers yet:\n    pip install transformers torch\n    \"\"\"\n    try:\n        from transformers import pipeline\n    except ImportError:\n        print(\"Please install transformers: pip install transformers torch\")\n        return None\n    \n    # TODO: Create sentiment analysis pipeline\n    sentiment_pipeline = ___\n    \n    results = []\n    for text in texts:\n        # TODO: Use pipeline to analyze text\n        output = ___\n        \n        results.append({\n            'text': text,\n            'label': ___ ,  # Extract label from output\n            'score': ___    # Extract score from output\n        })\n    \n    return results\n\n# Test your solution\ntest_texts = [\n    \"I absolutely love this! It's amazing!\",\n    \"This is terrible and disappointing.\",\n    \"The product is okay, nothing special.\"\n]\n# results = exercise4_transformers_sentiment(test_texts)\n# if results:\n#     for r in results:\n#         print(f\"Text: {r['text']}\")\n#         print(f\"  Prediction: {r['label']} (confidence: {r['score']:.2f})\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ★ EASY: Exercise 5 - BERT vs GPT Comparison (Fill-in-the-Blank)\n",
    "\n",
    "**Objective:** Understand the key differences between BERT and GPT architectures.\n",
    "\n",
    "**Instructions:**\n",
    "- Complete the comparison table by filling in the blanks\n",
    "- This is a conceptual exercise (no code required)\n",
    "- Consider: bidirectional vs unidirectional, encoder vs decoder, tasks, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ★ EASY: Exercise 5 - BERT vs GPT (Conceptual Comparison)\n\ndef exercise5_bert_vs_gpt():\n    \"\"\"\n    TODO: Fill in the blanks for the BERT vs GPT comparison\n    \n    For each property, provide the correct answer:\n    \n    BERT:\n    - Direction: Bidirectional (reads left AND right)\n    - Architecture: ___ (Encoder only / Decoder only / Encoder-Decoder)\n    - Pre-training objective: ___ (Masked Language Modeling / Next Sentence Prediction)\n    - Best for: ___ (Generation / Understanding)\n    - Example tasks: ___ (list 2-3 tasks)\n    \n    GPT:\n    - Direction: ___ (reads left to right only)\n    - Architecture: ___ (Encoder only / Decoder only / Encoder-Decoder)\n    - Pre-training objective: ___ (Masked Language Modeling / Causal Language Modeling)\n    - Best for: ___ (Generation / Understanding)\n    - Example tasks: ___ (list 2-3 tasks)\n    \"\"\"\n    \n    comparison = {\n        'BERT': {\n            'direction': 'Bidirectional',\n            'architecture': ___,\n            'pre_training': ___,\n            'best_for': ___,\n            'example_tasks': ___\n        },\n        'GPT': {\n            'direction': ___,\n            'architecture': ___,\n            'pre_training': ___,\n            'best_for': ___,\n            'example_tasks': ___\n        }\n    }\n    \n    return comparison\n\n# Test your understanding\n# comparison = exercise5_bert_vs_gpt()\n# for model, properties in comparison.items():\n#     print(f\"\\n{model}:\")\n#     for prop, value in properties.items():\n#         print(f\"  {prop}: {value}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEDIUM (★★) - Exercises 6-10\n",
    "\n",
    "These exercises combine multiple concepts and require more complex implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ★★ MEDIUM: Exercise 6 - Build Preprocessing Pipeline from Scratch\n",
    "\n",
    "**Objective:** Create a reusable text preprocessing function that combines multiple steps.\n",
    "\n",
    "**Instructions:**\n",
    "- Combine: lowercase, regex cleaning, tokenization, stop word removal, lemmatization\n",
    "- Make it flexible with optional parameters\n",
    "- Test on multiple types of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ★★ MEDIUM: Exercise 6 - Build Preprocessing Pipeline from Scratch\n\ndef exercise6_preprocessing_pipeline(text, remove_stopwords=True, use_lemmatization=True):\n    \"\"\"\n    TODO: Build a complete preprocessing pipeline\n    \n    Steps:\n    1. Convert to lowercase\n    2. Remove non-alphanumeric characters (keep spaces) using regex\n    3. Tokenize using word_tokenize()\n    4. (Optional) Remove stop words if remove_stopwords=True\n    5. (Optional) Lemmatize if use_lemmatization=True, else stem\n    6. Remove any empty tokens\n    \n    Return: list of processed tokens\n    \"\"\"\n    \n    # Step 1: Lowercase\n    text = ___\n    \n    # Step 2: Remove special characters\n    text = ___\n    \n    # Step 3: Tokenize\n    tokens = ___\n    \n    # Step 4: Remove stop words (if enabled)\n    if remove_stopwords:\n        stop_words = set(___)\n        tokens = ___\n    \n    # Step 5: Lemmatization or stemming\n    if use_lemmatization:\n        tokens = ___\n    else:\n        tokens = ___\n    \n    # Step 6: Remove empty tokens\n    tokens = ___\n    \n    return tokens\n\n# Test your solution\ntest_text = \"Hello, World! Machine Learning is awesome!!! Email: test@example.com\"\n# result1 = exercise6_preprocessing_pipeline(test_text)\n# result2 = exercise6_preprocessing_pipeline(test_text, remove_stopwords=False, use_lemmatization=False)\n# print(f\"With all processing: {result1}\")\n# print(f\"Without stopword removal and stemming: {result2}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ★★ MEDIUM: Exercise 7 - Use spaCy for POS + NER\n",
    "\n",
    "**Objective:** Extract linguistic and named entity information from text using spaCy.\n",
    "\n",
    "**Instructions:**\n",
    "- Process text with spaCy\n",
    "- Extract POS tags for each token\n",
    "- Extract named entities with their labels\n",
    "- Return structured results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ★★ MEDIUM: Exercise 7 - Use spaCy for POS + NER\n\ndef exercise7_spacy_pos_ner(text):\n    \"\"\"\n    TODO: Use spaCy to extract POS tags and named entities\n    \n    Steps:\n    1. Process text with nlp(text) - the spaCy model\n    2. Extract POS tags: for each token, get token.pos_ and token.text\n    3. Extract entities: for each ent in doc.ents, get ent.text and ent.label_\n    4. Return dictionary with 'pos_tags' and 'entities'\n    \"\"\"\n    \n    # Step 1: Process with spaCy\n    doc = ___\n    \n    # Step 2: Extract POS tags\n    pos_tags = ___\n    \n    # Step 3: Extract named entities\n    entities = ___\n    \n    return {\n        'pos_tags': pos_tags,\n        'entities': entities,\n        'text': text\n    }\n\n# Test your solution\ntest_text = \"Apple Inc. CEO Tim Cook announced the iPhone 15 in Cupertino, California.\"\n# result = exercise7_spacy_pos_ner(test_text)\n# print(f\"Text: {result['text']}\")\n# print(f\"\\nPOS Tags: {result['pos_tags']}\")\n# print(f\"\\nEntities: {result['entities']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ★★ MEDIUM: Exercise 8 - Zero-Shot Classification Pipeline\n",
    "\n",
    "**Objective:** Use transformers for zero-shot text classification without training data.\n",
    "\n",
    "**Note:** Requires `pip install transformers torch`\n",
    "\n",
    "**Instructions:**\n",
    "- Create a zero-shot classification pipeline\n",
    "- Classify text into custom candidate labels\n",
    "- Compare predictions across different label sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ★★ MEDIUM: Exercise 8 - Zero-Shot Classification Pipeline\n\ndef exercise8_zero_shot_classification(text, candidate_labels):\n    \"\"\"\n    TODO: Implement zero-shot classification\n    \n    Steps:\n    1. Import pipeline from transformers\n    2. Create a 'zero-shot-classification' pipeline\n    3. Use the pipeline with text and candidate_labels\n    4. Return the results (sequence, labels, scores)\n    \n    Zero-shot means: classify without training on these specific labels\n    The model generalizes from pre-training knowledge\n    \"\"\"\n    try:\n        from transformers import pipeline\n    except ImportError:\n        print(\"Please install transformers: pip install transformers torch\")\n        return None\n    \n    # TODO: Create zero-shot-classification pipeline\n    classifier = ___\n    \n    # TODO: Use pipeline to classify the text\n    results = ___\n    \n    return results\n\n# Test your solution\ntest_text = \"The stock market went up 5% today due to positive earnings reports.\"\nlabels = [\"finance\", \"sports\", \"entertainment\", \"technology\"]\n\n# result = exercise8_zero_shot_classification(test_text, labels)\n# if result:\n#     print(f\"Text: {test_text}\")\n#     print(f\"\\nPredictions:\")\n#     for label, score in zip(result['labels'], result['scores']):\n#         print(f\"  {label}: {score:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ★★ MEDIUM: Exercise 9 - Compare Traditional vs Transformer Sentiment\n",
    "\n",
    "**Objective:** Compare VADER (traditional) with transformer-based sentiment analysis.\n",
    "\n",
    "**Instructions:**\n",
    "- Analyze the same texts with both methods\n",
    "- Compare results and discuss differences\n",
    "- Note when each method works better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ★★ MEDIUM: Exercise 9 - Compare Traditional vs Transformer Sentiment\n\ndef exercise9_compare_sentiment_methods(texts):\n    \"\"\"\n    TODO: Compare VADER and transformer sentiment analysis\n    \n    Steps:\n    1. For each text, get VADER sentiment using analyzer.polarity_scores()\n    2. For each text, get transformer sentiment (use exercise4 or pipeline)\n    3. Create a comparison showing both results\n    4. Return formatted results for analysis\n    \"\"\"\n    from transformers import pipeline\n    \n    # Initialize transformer pipeline\n    transformer_pipeline = ___\n    \n    results = []\n    \n    for text in texts:\n        # Get VADER sentiment\n        vader_scores = ___\n        vader_label = \"Positive\" if vader_scores['compound'] > 0.1 else (\"Negative\" if vader_scores['compound'] < -0.1 else \"Neutral\")\n        \n        # Get transformer sentiment\n        transformer_output = ___\n        transformer_label = transformer_output[0][0]['label']\n        transformer_score = transformer_output[0][0]['score']\n        \n        results.append({\n            'text': text,\n            'vader': {\n                'compound': vader_scores['compound'],\n                'label': vader_label\n            },\n            'transformer': {\n                'label': transformer_label,\n                'score': transformer_score\n            }\n        })\n    \n    return results\n\n# Test your solution\ntest_texts = [\n    \"This product is absolutely amazing! I love it so much!\",\n    \"Terrible quality. Very disappointed with this purchase.\",\n    \"It's okay, nothing special but gets the job done.\"\n]\n\n# results = exercise9_compare_sentiment_methods(test_texts)\n# for r in results:\n#     print(f\"\\nText: {r['text']}\")\n#     print(f\"  VADER: {r['vader']['label']} (compound: {r['vader']['compound']:.2f})\")\n#     print(f\"  Transformer: {r['transformer']['label']} (score: {r['transformer']['score']:.2f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ★★ MEDIUM: Exercise 10 - Create TF-IDF Matrix\n",
    "\n",
    "**Objective:** Vectorize text using TF-IDF and understand matrix properties.\n",
    "\n",
    "**Instructions:**\n",
    "- Create a TF-IDF vectorizer\n",
    "- Transform document collection\n",
    "- Print matrix shape and examine values\n",
    "- Identify important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ★★ MEDIUM: Exercise 10 - Create TF-IDF Matrix\n\ndef exercise10_tfidf_matrix(documents):\n    \"\"\"\n    TODO: Create a TF-IDF matrix from documents\n    \n    Steps:\n    1. Create a TfidfVectorizer with English stop words\n    2. Fit and transform the documents\n    3. Get feature names (vocabulary)\n    4. Return the matrix, feature names, and matrix info\n    \"\"\"\n    \n    # Step 1: Create vectorizer\n    vectorizer = ___\n    \n    # Step 2: Fit and transform\n    tfidf_matrix = ___\n    \n    # Step 3: Get feature names\n    feature_names = ___\n    \n    return {\n        'matrix': tfidf_matrix,\n        'feature_names': feature_names,\n        'shape': tfidf_matrix.shape,  # (num_documents, num_features)\n        'vectorizer': vectorizer\n    }\n\n# Test your solution\ntest_docs = [\n    \"machine learning is powerful\",\n    \"deep learning uses neural networks\",\n    \"natural language processing is important\",\n    \"machine learning and deep learning are AI technologies\"\n]\n\n# result = exercise10_tfidf_matrix(test_docs)\n# print(f\"Matrix shape: {result['shape']}\")\n# print(f\"Number of documents: {result['shape'][0]}\")\n# print(f\"Number of unique words: {result['shape'][1]}\")\n# print(f\"\\nVocabulary (first 20 words): {result['feature_names'][:20]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HARD (★★★) - Exercises 11-15\n",
    "\n",
    "These exercises require combining multiple techniques and creative problem-solving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ★★★ HARD: Exercise 11 - Complete NLP Pipeline (Preprocessing to Classification)\n",
    "\n",
    "**Objective:** Build an end-to-end NLP pipeline from raw text to predictions.\n",
    "\n",
    "**Instructions:**\n",
    "- Step 1: Preprocess documents\n",
    "- Step 2: Vectorize with TF-IDF\n",
    "- Step 3: Train a classifier\n",
    "- Step 4: Make predictions and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ★★★ HARD: Exercise 11 - Complete NLP Pipeline\n\ndef exercise11_complete_nlp_pipeline(documents, labels):\n    \"\"\"\n    TODO: Build a complete NLP pipeline\n    \n    Steps:\n    1. Split data using train_test_split (test_size=0.3, random_state=42)\n    2. Create a Pipeline combining:\n       - TfidfVectorizer (with stop_words='english')\n       - LogisticRegression classifier (max_iter=1000)\n    3. Train the pipeline on training data\n    4. Evaluate on test data (get accuracy)\n    5. Get predictions on test set\n    6. Return predictions and accuracy\n    \"\"\"\n    from sklearn.pipeline import Pipeline\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score\n    \n    # Step 1: Split data\n    X_train, X_test, y_train, y_test = ___\n    \n    # Step 2: Create pipeline\n    pipeline = Pipeline([\n        ('tfidf', ___),\n        ('classifier', ___)\n    ])\n    \n    # Step 3: Train\n    ___\n    \n    # Step 4 & 5: Evaluate and predict\n    accuracy = ___\n    predictions = ___\n    \n    return {\n        'pipeline': pipeline,\n        'accuracy': accuracy,\n        'predictions': predictions,\n        'test_data': X_test,\n        'test_labels': y_test\n    }\n\n# Test your solution\ntest_docs = [\n    \"machine learning is great\", \"neural networks are powerful\",\n    \"python programming is fun\", \"data science is important\",\n    \"artificial intelligence transforms industry\", \"deep learning models\",\n    \"web development with javascript\", \"software engineering practices\"\n]\ntest_labels = [0, 0, 1, 0, 0, 0, 1, 1]  # 0=AI/ML, 1=Other\n\n# result = exercise11_complete_nlp_pipeline(test_docs, test_labels)\n# print(f\"Accuracy: {result['accuracy']:.2f}\")\n# print(f\"Predictions: {result['predictions']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ★★★ HARD: Exercise 12 - Multi-Technique Analysis (Entities + Sentiment + Keywords)\n",
    "\n",
    "**Objective:** Combine multiple NLP techniques on a single text.\n",
    "\n",
    "**Instructions:**\n",
    "- Extract named entities\n",
    "- Analyze sentiment\n",
    "- Extract important keywords (TF-IDF)\n",
    "- Return comprehensive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ★★★ HARD: Exercise 12 - Multi-Technique Analysis\n\ndef exercise12_multi_technique_analysis(text):\n    \"\"\"\n    TODO: Analyze text using multiple NLP techniques\n    \n    Perform:\n    1. Named Entity Recognition - extract entities and labels\n    2. Sentiment Analysis - use VADER for sentiment\n    3. Keyword Extraction - identify important words\n    4. Return comprehensive analysis object\n    \n    For keyword extraction, you can:\n    - Use lemmatization + filter by POS (nouns, verbs)\n    - Remove stop words\n    - Return top keywords\n    \"\"\"\n    \n    # 1. Named Entity Recognition with spaCy\n    doc = ___\n    entities = ___\n    \n    # 2. Sentiment Analysis with VADER\n    sentiment_scores = ___\n    sentiment_label = ___  # Positive/Negative/Neutral based on compound score\n    \n    # 3. Keyword Extraction\n    # Process tokens: remove stop words, lemmatize, filter by POS\n    stop_words = set(___)\n    keywords = [\n        token.text for token in doc \n        if token.pos_ in ['NOUN', 'VERB', 'ADJ'] and token.text not in stop_words\n    ]\n    # Remove duplicates and lemmatize\n    keywords = list(set(___))\n    \n    return {\n        'text': text,\n        'entities': entities,\n        'sentiment': {\n            'label': sentiment_label,\n            'compound_score': sentiment_scores['compound']\n        },\n        'keywords': keywords\n    }\n\n# Test your solution\ntest_text = \"Apple Inc. released the iPhone 15! Customers love it in the USA. The product is amazing!\"\n# result = exercise12_multi_technique_analysis(test_text)\n# print(f\"Text: {result['text']}\")\n# print(f\"\\nEntities: {result['entities']}\")\n# print(f\"Sentiment: {result['sentiment']}\")\n# print(f\"Keywords: {result['keywords']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ★★★ HARD: Exercise 13 - Transformer Text Classification with Custom Labels\n",
    "\n",
    "**Objective:** Use transformers for text classification on custom domains.\n",
    "\n",
    "**Note:** Requires `pip install transformers torch`\n",
    "\n",
    "**Instructions:**\n",
    "- Implement zero-shot classification on multiple texts\n",
    "- Use domain-specific labels\n",
    "- Build a classifier function that handles multiple texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ★★★ HARD: Exercise 13 - Transformer Text Classification with Custom Labels\n\ndef exercise13_transformer_custom_classification(texts, candidate_labels):\n    \"\"\"\n    TODO: Build a transformer-based classifier with custom labels\n    \n    Steps:\n    1. Import pipeline from transformers\n    2. Create zero-shot-classification pipeline\n    3. For each text, get predictions across all labels\n    4. Find the top prediction\n    5. Return comprehensive results with confidence scores\n    \"\"\"\n    try:\n        from transformers import pipeline\n    except ImportError:\n        return None\n    \n    # Step 1-2: Create classifier\n    classifier = ___\n    \n    results = []\n    for text in texts:\n        # Step 3: Get predictions\n        output = ___\n        \n        # Step 4-5: Format results\n        result_dict = {\n            'text': text,\n            'predictions': {},\n            'top_prediction': ___ ,\n            'top_score': ___\n        }\n        \n        # Store all label scores\n        for label, score in zip(output['labels'], output['scores']):\n            result_dict['predictions'][label] = score\n        \n        results.append(result_dict)\n    \n    return results\n\n# Test your solution\ntest_texts = [\n    \"I just finished my yoga session and feel so energized!\",\n    \"The stock market crashed due to economic concerns.\",\n    \"Our team won the championship game yesterday!\"\n]\nlabels = [\"fitness\", \"finance\", \"sports\"]\n\n# results = exercise13_transformer_custom_classification(test_texts, labels)\n# if results:\n#     for r in results:\n#         print(f\"\\nText: {r['text']}\")\n#         print(f\"Top prediction: {r['top_prediction']} ({r['top_score']:.3f})\")\n#         print(f\"All scores: {r['predictions']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ★★★ HARD: Exercise 14 - Build Mini NLP Toolkit Class\n",
    "\n",
    "**Objective:** Create a reusable NLP toolkit class with all learned methods.\n",
    "\n",
    "**Instructions:**\n",
    "- Create a class with methods for:\n",
    "  - Text preprocessing\n",
    "  - Named entity extraction\n",
    "  - Sentiment analysis\n",
    "  - Text vectorization\n",
    "  - Classification\n",
    "- Make it easy to use with a consistent interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ★★★ HARD: Exercise 14 - Build Mini NLP Toolkit Class\n\nclass NLPToolkit:\n    \"\"\"\n    TODO: Complete this NLP toolkit class\n    \n    Methods to implement:\n    1. preprocess(text) - clean and tokenize text\n    2. extract_entities(text) - get named entities\n    3. analyze_sentiment(text) - get sentiment\n    4. extract_keywords(text, top_n=5) - get important words\n    5. vectorize(documents) - create TF-IDF matrix\n    6. classify(documents, labels) - train and classify\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize toolkit with necessary components.\"\"\"\n        self.vectorizer = None\n        self.classifier = None\n    \n    def preprocess(self, text):\n        \"\"\"TODO: Implement preprocessing\"\"\"\n        # Lowercase, clean, tokenize, lemmatize\n        text = ___\n        text = ___\n        tokens = ___\n        stop_words = set(___)\n        tokens = ___\n        tokens = ___\n        return tokens\n    \n    def extract_entities(self, text):\n        \"\"\"TODO: Extract named entities\"\"\"\n        doc = ___\n        return ___\n    \n    def analyze_sentiment(self, text):\n        \"\"\"TODO: Analyze sentiment\"\"\"\n        scores = ___\n        label = ___ if scores['compound'] > 0.1 else (___)\n        return {'label': label, 'score': scores['compound']}\n    \n    def extract_keywords(self, text, top_n=5):\n        \"\"\"TODO: Extract top keywords\"\"\"\n        # Use preprocessing + POS filtering\n        doc = ___\n        keywords = [\n            ___.text for token in doc\n            if token.pos_ in ['NOUN', 'VERB', 'ADJ']\n        ]\n        return list(set(keywords))[:top_n]\n    \n    def vectorize(self, documents):\n        \"\"\"TODO: Vectorize documents with TF-IDF\"\"\"\n        self.vectorizer = ___\n        return self.vectorizer.fit_transform(documents)\n    \n    def classify(self, documents, labels):\n        \"\"\"TODO: Train classifier\"\"\"\n        self.classifier = Pipeline([\n            ('tfidf', ___),\n            ('clf', ___)\n        ])\n        self.classifier.fit(documents, labels)\n        return self.classifier.score(documents, labels)\n    \n    def predict(self, text):\n        \"\"\"Predict class of new text.\"\"\"\n        if self.classifier is None:\n            raise ValueError(\"Classifier not trained. Call classify() first.\")\n        return self.classifier.predict([text])[0]\n\n# Test your solution\n# toolkit = NLPToolkit()\n# \n# # Test preprocessing\n# processed = toolkit.preprocess(\"Hello, World! Machine learning is amazing!\")\n# print(f\"Processed: {processed}\")\n# \n# # Test entities\n# entities = toolkit.extract_entities(\"Apple Inc. was founded by Steve Jobs in California.\")\n# print(f\"Entities: {entities}\")\n# \n# # Test sentiment\n# sentiment = toolkit.analyze_sentiment(\"I love this product!\")\n# print(f\"Sentiment: {sentiment}\")\n# \n# # Test keywords\n# keywords = toolkit.extract_keywords(\"Machine learning and artificial intelligence transform technology\")\n# print(f\"Keywords: {keywords}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ★★★ HARD: Exercise 15 - Final Project: Analyze Text Collection\n",
    "\n",
    "**Objective:** Apply all learned techniques to analyze a collection of texts.\n",
    "\n",
    "**Instructions:**\n",
    "- Analyze multiple texts for:\n",
    "  - Common entities across collection\n",
    "  - Overall sentiment distribution\n",
    "  - Important keywords and topics\n",
    "  - Text classification/clustering\n",
    "- Generate a comprehensive report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ★★★ HARD: Exercise 15 - Final Project: Analyze Text Collection\n\ndef exercise15_text_collection_analysis(texts, labels=None):\n    \"\"\"\n    TODO: Perform comprehensive analysis on a collection of texts\n    \n    Analysis components:\n    1. Entity frequency - count all entities across texts\n    2. Sentiment distribution - overall sentiment trend\n    3. Keyword frequency - important words across collection\n    4. Text classification (if labels provided)\n    5. Generate summary report\n    \n    Return analysis results with insights\n    \"\"\"\n    from collections import Counter\n    \n    analysis_results = {\n        'total_texts': len(texts),\n        'entity_frequency': Counter(),\n        'sentiment_distribution': {'positive': 0, 'negative': 0, 'neutral': 0},\n        'keywords': Counter(),\n        'classification_accuracy': None\n    }\n    \n    # 1. Extract entities and count frequencies\n    for text in texts:\n        doc = ___\n        for ent in ___:\n            analysis_results['entity_frequency'][___] += 1\n    \n    # 2. Analyze sentiment distribution\n    for text in texts:\n        scores = ___\n        if scores['compound'] > 0.1:\n            analysis_results['sentiment_distribution']['positive'] += 1\n        elif scores['compound'] < -0.1:\n            analysis_results['sentiment_distribution']['negative'] += 1\n        else:\n            analysis_results['sentiment_distribution']['neutral'] += 1\n    \n    # 3. Extract keywords\n    stop_words = set(___)\n    for text in texts:\n        tokens = ___)\n        keywords = [\n            lemmatizer.lemmatize(t) for t in tokens \n            if t.isalpha() and t not in stop_words and len(t) > 3\n        ]\n        for kw in keywords:\n            analysis_results['keywords'][___] += 1\n    \n    # 4. Classification (if labels provided)\n    if labels is not None:\n        from sklearn.pipeline import Pipeline\n        from sklearn.feature_extraction.text import TfidfVectorizer\n        from sklearn.linear_model import LogisticRegression\n        \n        pipeline = Pipeline([\n            ('tfidf', ___),\n            ('clf', ___)\n        ])\n        \n        pipeline.fit(texts, labels)\n        accuracy = pipeline.score(texts, labels)\n        analysis_results['classification_accuracy'] = accuracy\n    \n    # Get top entities and keywords\n    analysis_results['top_entities'] = analysis_results['entity_frequency'].most_common(5)\n    analysis_results['top_keywords'] = analysis_results['keywords'].most_common(10)\n    \n    return analysis_results\n\n# Test your solution\ntest_texts = [\n    \"Apple Inc. released the iPhone 15 in California. Customers love it!\",\n    \"Microsoft announced Windows 12 in Seattle. The product is amazing!\",\n    \"Google launched Gemini AI in San Francisco. Innovation at its best!\",\n    \"Amazon expanded AI services in Ohio. Growth continues!\"\n]\ntest_labels = [0, 0, 1, 1]  # Tech companies vs Services\n\n# results = exercise15_text_collection_analysis(test_texts, test_labels)\n# print(f\"\\nCollection Analysis Report\")\n# print(f\"Total texts analyzed: {results['total_texts']}\")\n# print(f\"\\nTop Entities: {results['top_entities']}\")\n# print(f\"Top Keywords: {results['top_keywords']}\")\n# print(f\"Sentiment Distribution: {results['sentiment_distribution']}\")\n# if results['classification_accuracy'] is not None:\n#     print(f\"Classification Accuracy: {results['classification_accuracy']:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary of All 15 Exercises\n",
    "\n",
    "### Easy (★) - Foundations Review\n",
    "1. Tokenize and remove stop words\n",
    "2. Stem vs lemmatize comparison\n",
    "3. Match NLP concepts to tools\n",
    "4. Transformers sentiment analysis pipeline\n",
    "5. BERT vs GPT comparison\n",
    "\n",
    "### Medium (★★) - Combined Techniques\n",
    "6. Build preprocessing pipeline from scratch\n",
    "7. Use spaCy for POS + NER extraction\n",
    "8. Zero-shot classification pipeline\n",
    "9. Compare traditional vs transformer sentiment\n",
    "10. Create TF-IDF matrix and analyze\n",
    "\n",
    "### Hard (★★★) - Advanced Projects\n",
    "11. Complete NLP pipeline (preprocess → vectorize → classify)\n",
    "12. Multi-technique analysis (entities + sentiment + keywords)\n",
    "13. Transformer text classification with custom labels\n",
    "14. Build mini NLP toolkit class\n",
    "15. Final project: comprehensive text collection analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the comprehensive NLP course covering:\n",
    "- Days 1-7: Traditional NLP techniques\n",
    "- Day 8: Modern transformers and LLMs\n",
    "\n",
    "### Next Steps:\n",
    "1. Complete all 15 exercises\n",
    "2. Explore real datasets on Kaggle\n",
    "3. Build your own NLP project\n",
    "4. Learn fine-tuning with Hugging Face\n",
    "5. Contribute to open-source NLP projects\n",
    "\n",
    "### Key Resources:\n",
    "- [Hugging Face](https://huggingface.co/)\n",
    "- [spaCy Documentation](https://spacy.io/)\n",
    "- [NLTK Documentation](https://www.nltk.org/)\n",
    "- [Papers with Code](https://paperswithcode.com/)\n",
    "\n",
    "Good luck on your NLP journey!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}