{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Day 3: POS Tagging & Named Entity Recognition (NER)\n",
    "**The AI Engineer Course 2026 - Section 22**\n",
    "\n",
    "**Student:** Natruja\n",
    "\n",
    "**Date:** Saturday, February 14, 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand Part-of-Speech (POS) tagging\n",
    "2. Learn about Named Entity Recognition (NER)\n",
    "3. Use spaCy for advanced NLP tasks\n",
    "4. Extract meaningful information from text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup: Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "‚úì spaCy installed and English model downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install spaCy\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"spacy\", \"-q\"])\n",
    "\n",
    "# Download English model\n",
    "subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\", \"-q\"])\n",
    "\n",
    "print(\"‚úì spaCy installed and English model downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"‚úì All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Part-of-Speech (POS) Tagging: Understanding Word Roles\n",
    "\n",
    "**POS Tagging** identifies the grammatical role of each word in a sentence.\n",
    "\n",
    "### Common POS Tags:\n",
    "- **NOUN** (NN): person, place, thing (e.g., \"dog\", \"city\", \"book\")\n",
    "- **VERB** (VB): action (e.g., \"run\", \"jump\", \"eat\")\n",
    "- **ADJECTIVE** (JJ): describes nouns (e.g., \"beautiful\", \"quick\")\n",
    "- **ADVERB** (RB): describes verbs (e.g., \"quickly\", \"slowly\")\n",
    "- **PRONOUN** (PRP): replaces nouns (e.g., \"he\", \"she\", \"it\")\n",
    "- **PREPOSITION** (IN): shows relationships (e.g., \"in\", \"on\", \"at\")\n",
    "- **CONJUNCTION** (CC): connects words (e.g., \"and\", \"but\")\n",
    "- **DETERMINER** (DT): specifies nouns (e.g., \"the\", \"a\")\n",
    "\n",
    "### Why POS Tagging Matters:\n",
    "- Understand sentence structure\n",
    "- Extract specific information types\n",
    "- Improve other NLP tasks\n",
    "- Help machines understand meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## EXAMPLE: POS Tagging with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "============================================================\n",
      "Word            | POS Tag         | Explanation              \n",
      "------------------------------------------------------------\n",
      "The             | DET             | Determiner               \n",
      "quick           | ADJ             | Adjective                \n",
      "brown           | ADJ             | Adjective                \n",
      "fox             | NOUN            | Noun                     \n",
      "jumps           | VERB            | Verb                     \n",
      "over            | ADP             | Preposition              \n",
      "the             | DET             | Determiner               \n",
      "lazy            | ADJ             | Adjective                \n",
      "dog             | NOUN            | Noun                     \n",
      ".               | PUNCT           | Punctuation              \n"
     ]
    }
   ],
   "source": [
    "# Sample sentence\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Process with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"{'Word':<15} | {'POS Tag':<15} | {'Explanation':<25}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Display POS tags\n",
    "pos_explanations = {\n",
    "    'DET': 'Determiner',\n",
    "    'ADJ': 'Adjective',\n",
    "    'NOUN': 'Noun',\n",
    "    'VERB': 'Verb',\n",
    "    'ADP': 'Preposition',\n",
    "    'PUNCT': 'Punctuation'\n",
    "}\n",
    "\n",
    "for token in doc:\n",
    "    explanation = pos_explanations.get(token.pos_, token.pos_)\n",
    "    print(f\"{token.text:<15} | {token.pos_:<15} | {explanation:<25}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER): Identifying What's What\n",
    "\n",
    "**NER** identifies and classifies named entities (real-world objects like people, places, organizations).\n",
    "\n",
    "### Common Entity Types:\n",
    "- **PERSON**: Names of people (e.g., \"John Smith\", \"Elon Musk\")\n",
    "- **ORG**: Organizations (e.g., \"Google\", \"NASA\", \"Apple\")\n",
    "- **GPE**: Geographic/Political entities (e.g., \"France\", \"New York\", \"USA\")\n",
    "- **PRODUCT**: Products/Objects (e.g., \"iPhone\", \"Tesla Model 3\")\n",
    "- **MONEY**: Monetary values (e.g., \"$100\", \"‚Ç¨50\")\n",
    "- **DATE**: Dates (e.g., \"Monday\", \"February 14\")\n",
    "- **TIME**: Times (e.g., \"2:30 PM\", \"3 hours\")\n",
    "- **EVENT**: Events (e.g., \"World Cup\", \"Olympics\")\n",
    "\n",
    "### Applications:\n",
    "- Information extraction\n",
    "- Resume parsing\n",
    "- Chatbot understanding\n",
    "- Knowledge graph building\n",
    "- Recommendation systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## EXAMPLE: Named Entity Recognition with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Elon Musk founded Tesla in California. On February 14, 2024, he announced a new product worth $500 million.\n",
      "\n",
      "============================================================\n",
      "\n",
      "Named Entities Found:\n",
      "------------------------------------------------------------\n",
      "Text: Elon Musk            | Type: PERSON     | Start: 0 | End: 9\n",
      "Text: Tesla                | Type: ORG        | Start: 18 | End: 23\n",
      "Text: California           | Type: GPE        | Start: 27 | End: 37\n",
      "Text: February 14, 2024    | Type: DATE       | Start: 42 | End: 59\n",
      "Text: $500 million         | Type: MONEY      | Start: 94 | End: 106\n",
      "\n",
      "Total entities found: 5\n"
     ]
    }
   ],
   "source": [
    "# Sample text with multiple entities\n",
    "text = \"Elon Musk founded Tesla in California. On February 14, 2024, he announced a new product worth $500 million.\"\n",
    "\n",
    "# Process with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nNamed Entities Found:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Display entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"Text: {ent.text:<20} | Type: {ent.label_:<10} | Start: {ent.start_char} | End: {ent.end_char}\")\n",
    "\n",
    "print(f\"\\nTotal entities found: {len(doc.ents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90668f71",
   "metadata": {},
   "source": [
    "## EXAMPLE: Filtering Entities by Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Apple CEO Tim Cook announced a new iPhone in California. The price is $999 on September 12, 2024.\n",
      "\n",
      "============================================================\n",
      "\n",
      "Extracted Entities by Type:\n",
      "  People: ['Tim Cook']\n",
      "  Organizations: ['Apple', 'iPhone']\n",
      "  Locations: ['California']\n",
      "  Dates: ['September 12, 2024']\n",
      "  Money: ['999']\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"Apple CEO Tim Cook announced a new iPhone in California. The price is $999 on September 12, 2024.\"\n",
    "\n",
    "# Process with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Filter entities by type\n",
    "people = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "orgs = [ent.text for ent in doc.ents if ent.label_ == 'ORG']\n",
    "locations = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "dates = [ent.text for ent in doc.ents if ent.label_ == 'DATE']\n",
    "money = [ent.text for ent in doc.ents if ent.label_ == 'MONEY']\n",
    "\n",
    "print(\"\\nExtracted Entities by Type:\")\n",
    "print(f\"  People: {people}\")\n",
    "print(f\"  Organizations: {orgs}\")\n",
    "print(f\"  Locations: {locations}\")\n",
    "print(f\"  Dates: {dates}\")\n",
    "print(f\"  Money: {money}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "# EXERCISES: 15 Tasks Organized by Difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## ‚≠ê EASY: Exercise 1 - Process Text with spaCy and Print Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Python is a powerful programming language.\n",
      "Total tokens: Python is a powerful programming language.\n",
      "\n",
      "Tokens:\n",
      "  - Python\n",
      "  - is\n",
      "  - a\n",
      "  - powerful\n",
      "  - programming\n",
      "  - language\n",
      "  - .\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: Process text and display all tokens\n",
    "text = \"Python is a powerful programming language.\"\n",
    "\n",
    "# TODO: Process text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# TODO: Print total tokens\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Total tokens: {doc}\")\n",
    "\n",
    "print(\"\\nTokens:\")\n",
    "for token in doc:\n",
    "    print(f\"  - {token.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## ‚≠ê EASY: Exercise 2 - Get POS Tag for Each Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Dogs love playing fetch.\n",
      "\n",
      "Word -> POS Tag:\n",
      "  Dogs       -> PROPN\n",
      "  love       -> AUX\n",
      "  playing    -> VERB\n",
      "  fetch      -> NOUN\n",
      "  .          -> PUNCT\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: Get POS tag for each word in a sentence\n",
    "text = \"Dogs love playing fetch.\"\n",
    "\n",
    "# TODO: Process text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(\"\\nWord -> POS Tag:\")\n",
    "for token in doc:\n",
    "    print(f\"  {token.text:<10} -> {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## ‚≠ê EASY: Exercise 3 - Find All Entities in a Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Barack Obama was the 44th President of the United States.\n",
      "\n",
      "Entities found: Barack Obama was the 44th President of the United States.\n",
      "  - Barack Obama: PERSON\n",
      "  - 44th: ORDINAL\n",
      "  - the United States: GPE\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: Extract all entities from text\n",
    "text = \"Barack Obama was the 44th President of the United States.\"\n",
    "\n",
    "# TODO: Process text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"\\nEntities found: {doc}\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"  - {ent.text}: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## ‚≠ê EASY: Exercise 4 - Count Tokens in a Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Machine learning is transforming industries worldwide.\n",
      "Total tokens: 7\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: Count total tokens in text\n",
    "text = \"Machine learning is transforming industries worldwide.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# TODO: Count the total number of tokens\n",
    "token_count = len(doc)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Total tokens: {token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## ‚≠ê EASY: Exercise 5 - Print Entity Text and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Google CEO Sundar Pichai lives in California.\n",
      "\n",
      "Entity Details:\n",
      "Entity Text          | Entity Type\n",
      "--------------------------------\n",
      "Google               | ORG       \n",
      "Sundar Pichai        | PERSON    \n",
      "California           | GPE       \n"
     ]
    }
   ],
   "source": [
    "# Exercise 5: Display entity details in formatted table\n",
    "text = \"Google CEO Sundar Pichai lives in California.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(\"\\nEntity Details:\")\n",
    "print(f\"{'Entity Text':<20} | {'Entity Type':<10}\")\n",
    "print(\"-\" * 32)\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<20} | {ent.label_:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê MEDIUM: Exercise 6 - Extract Only Nouns from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The cat and dog played in the park.\n",
      "Nouns extracted: ['cat', 'dog', 'park']\n",
      "Total nouns: 3\n"
     ]
    }
   ],
   "source": [
    "# Exercise 6: Extract all nouns from text\n",
    "text = \"The cat and dog played in the park.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# TODO: Extract only NOUN tokens\n",
    "nouns = [token.text for token in doc if token.pos_ == 'NOUN']\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Nouns extracted: {nouns}\")\n",
    "print(f\"Total nouns: {len(nouns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê MEDIUM: Exercise 7 - Extract Only PERSON Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Steve Jobs founded Apple. Bill Gates created Microsoft. Both revolutionized technology.\n",
      "People mentioned: ['Steve Jobs', 'Bill Gates']\n",
      "Total people: 2\n"
     ]
    }
   ],
   "source": [
    "# Exercise 7: Extract all PERSON entities\n",
    "text = \"Steve Jobs founded Apple. Bill Gates created Microsoft. Both revolutionized technology.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# TODO: Extract all entities with label_ == 'PERSON'\n",
    "people = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"People mentioned: {people}\")\n",
    "print(f\"Total people: {len(people)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê MEDIUM: Exercise 8 - Get All Verbs and Their Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The students study hard. They studied chemistry yesterday.\n",
      "\n",
      "Verbs and their lemmas:\n",
      "  study -> study\n",
      "  studied -> study\n"
     ]
    }
   ],
   "source": [
    "# Exercise 8: Extract verbs and their base forms (lemmas)\n",
    "text = \"The students study hard. They studied chemistry yesterday.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# TODO: Extract VERB tokens with their lemmas\n",
    "verbs = []\n",
    "for token in doc:\n",
    "    if token.pos_ == 'VERB':\n",
    "        verbs.append((token.text, token.lemma_))\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(\"\\nVerbs and their lemmas:\")\n",
    "for word, lemma in verbs:\n",
    "    print(f\"  {word} -> {lemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê MEDIUM: Exercise 9 - Compare Entities in Two Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cell-30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: John works at Microsoft in Seattle.\n",
      "People: ['John']\n",
      "\n",
      "Text 2: Sarah works at Google in California.\n",
      "People: ['Sarah']\n",
      "\n",
      "People in both texts: set()\n"
     ]
    }
   ],
   "source": [
    "# Exercise 9: Extract and compare entities from two different texts\n",
    "text1 = \"John works at Microsoft in Seattle.\"\n",
    "text2 = \"Sarah works at Google in California.\"\n",
    "\n",
    "doc1 = nlp(text1)\n",
    "doc2 = nlp(text2)\n",
    "\n",
    "# TODO: Extract PERSON entities from both docs\n",
    "people_text1 = [ent.text for ent in doc1.ents if ___]\n",
    "people_text2 = [ent.text for ent in doc2.ents if ___]\n",
    "\n",
    "print(\"Text 1:\", text1)\n",
    "print(\"People:\", people_text1)\n",
    "print()\n",
    "print(\"Text 2:\", text2)\n",
    "print(\"People:\", people_text2)\n",
    "print()\n",
    "print(\"People in both texts:\", set(people_text1) & set(people_text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê MEDIUM: Exercise 10 - Count POS Tag Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cell-32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The quick brown fox jumps over the lazy dog and runs away.\n",
      "\n",
      "POS Tag Frequencies:\n",
      "  ADJ: 3\n",
      "  ADP: 1\n",
      "  ADV: 1\n",
      "  CCONJ: 1\n",
      "  DET: 2\n",
      "  NOUN: 2\n",
      "  PUNCT: 1\n",
      "  VERB: 2\n"
     ]
    }
   ],
   "source": [
    "# Exercise 10: Count how many tokens of each POS type exist\n",
    "text = \"The quick brown fox jumps over the lazy dog and runs away.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# TODO: Create a dictionary to count each POS tag\n",
    "pos_counts = {}\n",
    "for token in doc:\n",
    "    pos = ___\n",
    "    if pos in pos_counts:\n",
    "        pos_counts[___] += 1\n",
    "    else:\n",
    "        pos_counts[___] = 1\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(\"\\nPOS Tag Frequencies:\")\n",
    "for pos, count in sorted(pos_counts.items()):\n",
    "    print(f\"  {pos}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê‚≠ê HARD: Exercise 11 - Build Entity Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cell-34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Elon Musk works at Tesla in California. Jeff Bezos founded Amazon in Seattle.\n",
      "\n",
      "Extracted Entities by Type:\n",
      "  PERSON: ['Elon Musk', 'Jeff Bezos']\n",
      "  ORG: ['Tesla', 'Amazon']\n",
      "  GPE: ['California', 'Seattle']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 11: Create a function that returns entities organized by type\n",
    "def extract_entities_by_type(text):\n",
    "    \"\"\"Extract entities and organize them by type in a dictionary.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # TODO: Initialize a dictionary with entity types as keys\n",
    "    entities = {\n",
    "        'PERSON': [],\n",
    "        'ORG': [],\n",
    "        'GPE': []\n",
    "    }\n",
    "    \n",
    "    # TODO: Loop through entities and add them to the appropriate list\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in entities:\n",
    "            entities[___].append(___)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Test the function\n",
    "text = \"Elon Musk works at Tesla in California. Jeff Bezos founded Amazon in Seattle.\"\n",
    "result = extract_entities_by_type(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(\"\\nExtracted Entities by Type:\")\n",
    "for entity_type, entities_list in result.items():\n",
    "    print(f\"  {entity_type}: {entities_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê‚≠ê HARD: Exercise 12 - Analyze Paragraph: Entities, POS, Key Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cell-36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAGRAPH ANALYSIS\n",
      "============================================================\n",
      "Text: Artificial intelligence is transforming the world.\n",
      "Companies like Google, Microsoft, and OpenAI are ...\n",
      "\n",
      "Total entities: 6\n",
      "Unique entities: {'California', 'Microsoft', 'Google', 'OpenAI', 'New York', 'every day'}\n",
      "\n",
      "Noun count: 6\n",
      "Key nouns: ['intelligence', 'discoveries', 'world', 'day', 'Researchers', 'Companies']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 12: Comprehensive analysis of a paragraph\n",
    "paragraph = \"\"\"Artificial intelligence is transforming the world.\n",
    "Companies like Google, Microsoft, and OpenAI are investing heavily.\n",
    "Researchers in California and New York are making breakthrough discoveries every day.\"\"\"\n",
    "\n",
    "doc = nlp(paragraph)\n",
    "\n",
    "# TODO: Extract unique entities\n",
    "unique_entities = ___\n",
    "\n",
    "# TODO: Count NOUN tokens\n",
    "noun_count = ___\n",
    "\n",
    "# TODO: Extract nouns (not just count)\n",
    "key_nouns = ___\n",
    "\n",
    "print(\"PARAGRAPH ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Text: {paragraph[:100]}...\\n\")\n",
    "print(f\"Total entities: {len(doc.ents)}\")\n",
    "print(f\"Unique entities: {unique_entities}\")\n",
    "print(f\"\\nNoun count: {noun_count}\")\n",
    "print(f\"Key nouns: {key_nouns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê‚≠ê HARD: Exercise 13 - Process Multiple Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cell-38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MULTI-DOCUMENT ANALYSIS\n",
      "============================================================\n",
      "1. Apple released the iPhone in California.\n",
      "2. Google headquarters is also in California.\n",
      "3. Microsoft is based in Washington.\n",
      "\n",
      "All organizations found: {'Apple', 'Microsoft', 'iPhone', 'Google'}\n",
      "Organizations appearing multiple times: set()\n"
     ]
    }
   ],
   "source": [
    "# Exercise 13: Extract and find common entities across multiple documents\n",
    "documents = [\n",
    "    \"Apple released the iPhone in California.\",\n",
    "    \"Google headquarters is also in California.\",\n",
    "    \"Microsoft is based in Washington.\"\n",
    "]\n",
    "\n",
    "# TODO: Process each document and collect all ORG entities\n",
    "all_organizations = []\n",
    "for doc_text in documents:\n",
    "    doc = nlp(doc_text)\n",
    "    orgs = ___\n",
    "    all_organizations.extend(orgs)\n",
    "\n",
    "# TODO: Find organizations that appear in multiple documents\n",
    "common_orgs = ___\n",
    "\n",
    "print(\"MULTI-DOCUMENT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "for i, doc_text in enumerate(documents, 1):\n",
    "    print(f\"{i}. {doc_text}\")\n",
    "\n",
    "print(f\"\\nAll organizations found: {set(all_organizations)}\")\n",
    "print(f\"Organizations appearing multiple times: {set(common_orgs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê‚≠ê HARD: Exercise 14 - Build Text Highlighting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cell-40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: John Smith and Mary Johnson work at Apple.\n",
      "Highlighted: [JOHN SMITH] and [MARY JOHNSON] work at Apple.\n"
     ]
    }
   ],
   "source": [
    "# Exercise 14: Create a function that highlights entities in text\n",
    "def highlight_entities(text, entity_type='PERSON'):\n",
    "    \"\"\"Highlight entities of a specific type in text.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # TODO: Build a list of entity positions\n",
    "    entity_spans = []\n",
    "    for ent in doc.ents:\n",
    "        if ___:\n",
    "            entity_spans.append(___)\n",
    "    \n",
    "    # TODO: Sort by start position and create highlighted version\n",
    "    highlighted = \"\"\n",
    "    last_end = 0\n",
    "    \n",
    "    for start, end, entity_text in sorted(entity_spans):\n",
    "        highlighted += text[last_end:start]\n",
    "        highlighted += f\"[{entity_text.upper()}]\"\n",
    "        last_end = end\n",
    "    \n",
    "    highlighted += text[last_end:]\n",
    "    return highlighted\n",
    "\n",
    "# Test the function\n",
    "text = \"John Smith and Mary Johnson work at Apple.\"\n",
    "result = highlight_entities(text, entity_type='PERSON')\n",
    "\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Highlighted: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê‚≠ê HARD: Exercise 15 - Create NER Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cell-42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER SUMMARY REPORT\n",
      "============================================================\n",
      "Total tokens: 66\n",
      "Total entities: 11\n",
      "\n",
      "Entity Breakdown:\n",
      "  PERSON     (1): ['Tim Cook']\n",
      "  ORG        (3): ['Apple', 'Microsoft', 'Google']\n",
      "  GPE        (2): ['Cupertino', 'California']\n",
      "  MONEY      (1): ['185']\n",
      "  DATE       (3): ['January 15, 2024', 'the coming quarter', 'this month']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 15: Generate a comprehensive NER summary report\n",
    "news_article = \"\"\"Tech giant Apple announced record earnings on January 15, 2024.\n",
    "CEO Tim Cook, based in Cupertino, California, presented the results to investors.\n",
    "The company stock price increased by 5 percent to $185 per share.\n",
    "Analysts predict further growth in the coming quarter.\n",
    "Microsoft and Google also reported strong performance this month.\"\"\"\n",
    "\n",
    "def generate_ner_report(text):\n",
    "    \"\"\"Generate a comprehensive NER summary report.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # TODO: Initialize report dictionary\n",
    "    report = {\n",
    "        'PERSON': [],\n",
    "        'ORG': [],\n",
    "        'GPE': [],\n",
    "        'MONEY': [],\n",
    "        'DATE': [],\n",
    "        'total_tokens': ___,\n",
    "        'total_entities': ___\n",
    "    }\n",
    "    \n",
    "    # TODO: Populate report with entities\n",
    "    for ent in doc.ents:\n",
    "        if ___ in report and isinstance(report[___], list):\n",
    "            report[___].append(___)\n",
    "    \n",
    "    return report\n",
    "\n",
    "report = generate_ner_report(news_article)\n",
    "\n",
    "print(\"NER SUMMARY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total tokens: {report['total_tokens']}\")\n",
    "print(f\"Total entities: {report['total_entities']}\")\n",
    "\n",
    "print(\"\\nEntity Breakdown:\")\n",
    "for entity_type in ['PERSON', 'ORG', 'GPE', 'MONEY', 'DATE']:\n",
    "    if report[entity_type]:\n",
    "        print(f\"  {entity_type:10} ({len(report[entity_type])}): {report[entity_type]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-bonus-header",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚≠ê‚≠ê‚≠ê BONUS HARD EXERCISES (16-20)\n",
    "**Extra challenges for deeper practice!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ex16-header",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê‚≠ê HARD: Exercise 16 - Dependency Parsing & Sentence Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-ex16-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPENDENCY PARSING\n",
      "============================================================\n",
      "Token           | DEP          | Head Word       | POS     \n",
      "------------------------------------------------------------\n",
      "The             | det          | engineer        | DET     \n",
      "talented        | amod         | engineer        | ADJ     \n",
      "engineer        | nsubj        | designed        | NOUN    \n",
      "designed        | ROOT         | designed        | VERB    \n",
      "an              | det          | system          | DET     \n",
      "innovative      | amod         | system          | ADJ     \n",
      "AI              | compound     | system          | PROPN   \n",
      "system          | dobj         | designed        | NOUN    \n",
      "for             | prep         | system          | ADP     \n",
      "the             | det          | company         | DET     \n",
      "company         | pobj         | for             | NOUN    \n",
      ".               | punct        | designed        | PUNCT   \n",
      "\n",
      "Root verb: [designed]\n",
      "Direct objects: ['system']\n",
      "Subjects: ['engineer']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 16: Analyze dependency relationships between words\n",
    "text = \"The talented engineer designed an innovative AI system for the company.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# TODO: Print each token with its dependency label and head word\n",
    "print(\"DEPENDENCY PARSING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Token':<15} | {'DEP':<12} | {'Head Word':<15} | {'POS':<8}\")\n",
    "print(\"-\" * 60)\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<15} | {token.dep_:<12} | {token.head.text:<15} | {token.pos_:<8}\")\n",
    "\n",
    "# TODO: Find the ROOT verb of the sentence\n",
    "root_verb = [token for token in doc if token.dep_ == 'ROOT']\n",
    "print(f\"\\nRoot verb: {root_verb}\")\n",
    "\n",
    "# TODO: Find all direct objects (dobj)\n",
    "direct_objects = [token.text for token in doc if token.dep_ == 'dobj']\n",
    "print(f\"Direct objects: {direct_objects}\")\n",
    "\n",
    "# TODO: Find all subjects (nsubj)\n",
    "subjects = [token.text for token in doc if token.dep_ == 'nsubj']\n",
    "print(f\"Subjects: {subjects}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ex17-header",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê‚≠ê HARD: Exercise 17 - Entity Frequency & Ranking Across Multiple Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-ex17-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTITY FREQUENCY RANKING\n",
      "============================================================\n",
      "Rank   | Entity               | Type       | Count \n",
      "------------------------------------------------------------\n",
      "1      | California           | GPE        | 4     \n",
      "2      | Google               | ORG        | 3     \n",
      "3      | Apple                | ORG        | 2     \n",
      "4      | New York             | GPE        | 2     \n",
      "5      | Microsoft            | ORG        | 2     \n",
      "6      | iPhone               | ORG        | 1     \n",
      "7      | AI                   | GPE        | 1     \n",
      "8      | Tim Cook             | PERSON     | 1     \n",
      "9      | $2 billion           | MONEY      | 1     \n",
      "10     | Microsoft AI         | ORG        | 1     \n",
      "11     | Jeff Bezos           | PERSON     | 1     \n",
      "\n",
      "Most mentioned organization: California\n"
     ]
    }
   ],
   "source": [
    "# Exercise 17: Build an entity frequency ranker from multiple news headlines\n",
    "headlines = [\n",
    "    \"Apple launches new iPhone in California and New York.\",\n",
    "    \"Google and Microsoft compete in AI race.\",\n",
    "    \"Tim Cook visits Google headquarters in California.\",\n",
    "    \"Microsoft acquires startup in New York for $2 billion.\",\n",
    "    \"Apple and Google announce partnership in California.\",\n",
    "    \"Elon Musk praises Microsoft AI tools.\",\n",
    "    \"Jeff Bezos invests in California tech startup.\"\n",
    "]\n",
    "\n",
    "# TODO: Build a frequency dictionary for ALL entity types\n",
    "entity_freq = {}  # key: (entity_text, entity_label), value: count\n",
    "\n",
    "for headline in headlines:\n",
    "    doc = nlp(headline)\n",
    "    for ent in doc.ents:\n",
    "        key = (ent.text, ent.label_)\n",
    "        entity_freq[key] = entity_freq.get(key, 0) + 1\n",
    "\n",
    "# TODO: Sort by frequency (highest first)\n",
    "ranked = sorted(entity_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"ENTITY FREQUENCY RANKING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Rank':<6} | {'Entity':<20} | {'Type':<10} | {'Count':<6}\")\n",
    "print(\"-\" * 60)\n",
    "for rank, ((text, label), count) in enumerate(ranked, 1):\n",
    "    print(f\"{rank:<6} | {text:<20} | {label:<10} | {count:<6}\")\n",
    "\n",
    "# TODO: Find the most mentioned ORG\n",
    "top_org = [text for (text, label), count in ranked if label == 'ORG']\n",
    "print(f\"\\nMost mentioned organization: {top_org[0] if top_org else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ex18-header",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê‚≠ê HARD: Exercise 18 - Build a POS Pattern Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cell-ex18-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJ + NOUN Patterns Found:\n",
      "========================================\n",
      "  'The brilliant scientist made an amazing ...'\n",
      "    Matches: ['brilliant scientist', 'amazing discovery']\n",
      "  'A dangerous storm hit the coastal city y...'\n",
      "    Matches: ['dangerous storm', 'coastal city']\n",
      "  'The young developer built a powerful app...'\n",
      "    Matches: ['young developer', 'powerful application']\n",
      "\n",
      "NOUN + VERB Patterns Found:\n",
      "========================================\n",
      "  'The brilliant scientist made an amazing ...'\n",
      "    Matches: ['scientist made']\n",
      "  'A dangerous storm hit the coastal city y...'\n",
      "    Matches: ['storm hit']\n",
      "  'The young developer built a powerful app...'\n",
      "    Matches: ['developer built']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 18: Find specific POS patterns (e.g., ADJ + NOUN combinations)\n",
    "texts = [\n",
    "    \"The brilliant scientist made an amazing discovery.\",\n",
    "    \"A dangerous storm hit the coastal city yesterday.\",\n",
    "    \"The young developer built a powerful application.\"\n",
    "]\n",
    "\n",
    "def find_pos_patterns(text, pattern):\n",
    "    \"\"\"Find consecutive POS tag patterns in text.\n",
    "    pattern: list of POS tags, e.g., ['ADJ', 'NOUN']\n",
    "    Returns list of matched phrases.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    matches = []\n",
    "    tokens = list(doc)\n",
    "    \n",
    "    # TODO: Slide a window of pattern length across tokens\n",
    "    for i in range(len(tokens) - len(pattern) + 1):\n",
    "        window = tokens[i:i + len(pattern)]\n",
    "        \n",
    "        # TODO: Check if POS tags match the pattern\n",
    "        pos_tags = [token.pos_ for token in window]\n",
    "        if pos_tags == pattern:\n",
    "            phrase = \" \".join([t.text for t in window])\n",
    "            matches.append(phrase)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Find ADJ + NOUN patterns\n",
    "print(\"ADJ + NOUN Patterns Found:\")\n",
    "print(\"=\" * 40)\n",
    "for text in texts:\n",
    "    results = find_pos_patterns(text, ['ADJ', 'NOUN'])\n",
    "    print(f\"  '{text[:40]}...'\")\n",
    "    print(f\"    Matches: {results}\")\n",
    "\n",
    "# TODO: Now find NOUN + VERB patterns\n",
    "print(\"\\nNOUN + VERB Patterns Found:\")\n",
    "print(\"=\" * 40)\n",
    "for text in texts:\n",
    "    results = find_pos_patterns(text, ['NOUN', 'VERB'])\n",
    "    print(f\"  '{text[:40]}...'\")\n",
    "    print(f\"    Matches: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ex19-header",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê‚≠ê HARD: Exercise 19 - Entity Relationship Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cell-ex19-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON ‚Üî ORG RELATIONSHIPS\n",
      "==================================================\n",
      "  Tim Cook ‚Üí Apple\n",
      "  Satya Nadella manages ‚Üí Microsoft\n",
      "  Mark Zuckerberg ‚Üí Meta\n",
      "  Mark Zuckerberg ‚Üí Harvard\n",
      "\n",
      "Total relationships found: 4\n"
     ]
    }
   ],
   "source": [
    "# Exercise 19: Map relationships between PERSON and ORG entities in same sentence\n",
    "texts = [\n",
    "    \"Tim Cook is the CEO of Apple.\",\n",
    "    \"Sundar Pichai leads Google. Satya Nadella manages Microsoft.\",\n",
    "    \"Jensen Huang founded NVIDIA in California.\",\n",
    "    \"Mark Zuckerberg built Meta from his Harvard dorm room.\"\n",
    "]\n",
    "\n",
    "def extract_person_org_relations(text):\n",
    "    \"\"\"Find PERSON-ORG pairs that appear in the same sentence.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    relations = []\n",
    "    \n",
    "    # TODO: Loop through each sentence in the document\n",
    "    for sent in doc.sents:\n",
    "        # TODO: Extract PERSON and ORG entities from this sentence\n",
    "        persons = [ent.text for ent in sent.ents if ent.label_ == 'PERSON']\n",
    "        orgs = [ent.text for ent in sent.ents if ent.label_ == 'ORG']\n",
    "        \n",
    "        # TODO: Create pairs of (person, org) for entities in same sentence\n",
    "        for person in persons:\n",
    "            for org in orgs:\n",
    "                relations.append((person, org))\n",
    "    \n",
    "    return relations\n",
    "\n",
    "print(\"PERSON ‚Üî ORG RELATIONSHIPS\")\n",
    "print(\"=\" * 50)\n",
    "all_relations = []\n",
    "for text in texts:\n",
    "    rels = extract_person_org_relations(text)\n",
    "    all_relations.extend(rels)\n",
    "    for person, org in rels:\n",
    "        print(f\"  {person} ‚Üí {org}\")\n",
    "\n",
    "print(f\"\\nTotal relationships found: {len(all_relations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ex20-header",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê‚≠ê HARD: Exercise 20 - Complete NLP Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cell-ex20-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE NLP ANALYSIS REPORT\n",
      "============================================================\n",
      "\n",
      "üìä BASIC STATS:\n",
      "   Sentences: 4\n",
      "   Tokens: 60\n",
      "   Unique tokens: 45\n",
      "\n",
      "üè∑Ô∏è TOP POS TAGS:\n",
      "   PROPN: 12\n",
      "   NOUN: 8\n",
      "   VERB: 8\n",
      "\n",
      "üîç ENTITIES FOUND:\n",
      "   PERSON: ['OpenAI', 'Sam Altman', 'Bard']\n",
      "   DATE: ['November 2022', 'two months', '2030']\n",
      "   CARDINAL: ['100 million']\n",
      "   ORG: ['Microsoft']\n",
      "   MONEY: ['$10 billion']\n",
      "   GPE: ['OpenAI', 'San Francisco', 'London', 'AI']\n",
      "\n",
      "üìù KEY PHRASES (ADJ+NOUN):\n",
      "   - Artificial intelligence\n"
     ]
    }
   ],
   "source": [
    "# Exercise 20: Build a complete text analysis pipeline combining POS + NER + Stats\n",
    "article = \"\"\"Artificial intelligence company OpenAI, led by Sam Altman,\n",
    "released ChatGPT in November 2022. The product gained 100 million users\n",
    "within two months. Google responded by launching Bard, while Microsoft\n",
    "invested $10 billion in OpenAI. Experts in San Francisco and London\n",
    "predict AI will transform every industry by 2030.\"\"\"\n",
    "\n",
    "def full_nlp_pipeline(text):\n",
    "    \"\"\"Run complete NLP analysis: tokenization, POS, NER, and statistics.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # --- SECTION 1: Basic Stats ---\n",
    "    # TODO: Count sentences, tokens, and unique tokens\n",
    "    num_sentences = len(list(doc.sents))\n",
    "    num_tokens = len(doc)\n",
    "    unique_tokens = len(set([token.lower_ for token in doc if not token.is_punct]))\n",
    "    \n",
    "    # --- SECTION 2: POS Distribution ---\n",
    "    # TODO: Get top 3 most common POS tags\n",
    "    pos_counts = {}\n",
    "    for token in doc:\n",
    "        if not token.is_punct and not token.is_space:\n",
    "            pos = token.pos_\n",
    "            pos_counts[pos] = pos_counts.get(pos, 0) + 1\n",
    "    \n",
    "    top_pos = sorted(pos_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    \n",
    "    # --- SECTION 3: Entity Analysis ---\n",
    "    # TODO: Group entities by type\n",
    "    entity_groups = {}\n",
    "    for ent in doc.ents:\n",
    "        label = ent.label_\n",
    "        if label not in entity_groups:\n",
    "            entity_groups[label] = []\n",
    "        entity_groups[label].append(ent.text)\n",
    "    \n",
    "    # --- SECTION 4: Key Phrases (ADJ+NOUN) ---\n",
    "    # TODO: Extract adjective-noun phrases\n",
    "    key_phrases = []\n",
    "    tokens_list = list(doc)\n",
    "    for i in range(len(tokens_list) - 1):\n",
    "        if tokens_list[i].pos_ == 'ADJ' and tokens_list[i+1].pos_ == 'NOUN':\n",
    "            key_phrases.append(f\"{tokens_list[i].text} {tokens_list[i+1].text}\")\n",
    "    \n",
    "    return {\n",
    "        'sentences': num_sentences,\n",
    "        'tokens': num_tokens,\n",
    "        'unique_tokens': unique_tokens,\n",
    "        'top_pos': top_pos,\n",
    "        'entities': entity_groups,\n",
    "        'key_phrases': key_phrases\n",
    "    }\n",
    "\n",
    "# Run the pipeline\n",
    "results = full_nlp_pipeline(article)\n",
    "\n",
    "# Display results\n",
    "print(\"COMPLETE NLP ANALYSIS REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìä BASIC STATS:\")\n",
    "print(f\"   Sentences: {results['sentences']}\")\n",
    "print(f\"   Tokens: {results['tokens']}\")\n",
    "print(f\"   Unique tokens: {results['unique_tokens']}\")\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è TOP POS TAGS:\")\n",
    "for pos, count in results['top_pos']:\n",
    "    print(f\"   {pos}: {count}\")\n",
    "\n",
    "print(f\"\\nüîç ENTITIES FOUND:\")\n",
    "for label, entities in results['entities'].items():\n",
    "    print(f\"   {label}: {entities}\")\n",
    "\n",
    "print(f\"\\nüìù KEY PHRASES (ADJ+NOUN):\")\n",
    "for phrase in results['key_phrases']:\n",
    "    print(f\"   - {phrase}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "- **POS Tagging** identifies the grammatical role of words (NOUN, VERB, ADJ, etc.)\n",
    "- **Named Entity Recognition** identifies and classifies named entities (people, places, organizations, etc.)\n",
    "- **spaCy** is a powerful library for both tasks with pre-trained models\n",
    "- These techniques enable information extraction and deeper text understanding\n",
    "\n",
    "### Real-World Applications:\n",
    "- Resume parsing and job matching\n",
    "- News article analysis\n",
    "- Customer support automation\n",
    "- Knowledge base construction\n",
    "- Question answering systems\n",
    "\n",
    "### What's Next:\n",
    "Tomorrow we'll learn **Sentiment Analysis** - determining whether text expresses positive, negative, or neutral emotions!\n",
    "\n",
    "---\n",
    "\n",
    "*Created for Natruja's NLP study plan*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "nlp_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
