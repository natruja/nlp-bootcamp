{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Day 3: POS Tagging & Named Entity Recognition (NER)\n",
    "**The AI Engineer Course 2026 - Section 22**\n",
    "\n",
    "**Student:** Natruja\n",
    "\n",
    "**Date:** Saturday, February 14, 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand Part-of-Speech (POS) tagging\n",
    "2. Learn about Named Entity Recognition (NER)\n",
    "3. Use spaCy for advanced NLP tasks\n",
    "4. Extract meaningful information from text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup: Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "‚úì spaCy installed and English model downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install spaCy\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"spacy\", \"-q\"])\n",
    "\n",
    "# Download English model\n",
    "subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\", \"-q\"])\n",
    "\n",
    "print(\"‚úì spaCy installed and English model downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"‚úì All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Part-of-Speech (POS) Tagging: Understanding Word Roles\n",
    "\n",
    "**POS Tagging** identifies the grammatical role of each word in a sentence.\n",
    "\n",
    "### Common POS Tags:\n",
    "- **NOUN** (NN): person, place, thing (e.g., \"dog\", \"city\", \"book\")\n",
    "- **VERB** (VB): action (e.g., \"run\", \"jump\", \"eat\")\n",
    "- **ADJECTIVE** (JJ): describes nouns (e.g., \"beautiful\", \"quick\")\n",
    "- **ADVERB** (RB): describes verbs (e.g., \"quickly\", \"slowly\")\n",
    "- **PRONOUN** (PRP): replaces nouns (e.g., \"he\", \"she\", \"it\")\n",
    "- **PREPOSITION** (IN): shows relationships (e.g., \"in\", \"on\", \"at\")\n",
    "- **CONJUNCTION** (CC): connects words (e.g., \"and\", \"but\")\n",
    "- **DETERMINER** (DT): specifies nouns (e.g., \"the\", \"a\")\n",
    "\n",
    "### Why POS Tagging Matters:\n",
    "- Understand sentence structure\n",
    "- Extract specific information types\n",
    "- Improve other NLP tasks\n",
    "- Help machines understand meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## EXAMPLE: POS Tagging with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "============================================================\n",
      "Word            | POS Tag         | Explanation              \n",
      "------------------------------------------------------------\n",
      "The             | DET             | Determiner               \n",
      "quick           | ADJ             | Adjective                \n",
      "brown           | ADJ             | Adjective                \n",
      "fox             | NOUN            | Noun                     \n",
      "jumps           | VERB            | Verb                     \n",
      "over            | ADP             | Preposition              \n",
      "the             | DET             | Determiner               \n",
      "lazy            | ADJ             | Adjective                \n",
      "dog             | NOUN            | Noun                     \n",
      ".               | PUNCT           | Punctuation              \n"
     ]
    }
   ],
   "source": [
    "# Sample sentence\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Process with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"{'Word':<15} | {'POS Tag':<15} | {'Explanation':<25}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Display POS tags\n",
    "pos_explanations = {\n",
    "    'DET': 'Determiner',\n",
    "    'ADJ': 'Adjective',\n",
    "    'NOUN': 'Noun',\n",
    "    'VERB': 'Verb',\n",
    "    'ADP': 'Preposition',\n",
    "    'PUNCT': 'Punctuation'\n",
    "}\n",
    "\n",
    "for token in doc:\n",
    "    explanation = pos_explanations.get(token.pos_, token.pos_)\n",
    "    print(f\"{token.text:<15} | {token.pos_:<15} | {explanation:<25}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER): Identifying What's What\n",
    "\n",
    "**NER** identifies and classifies named entities (real-world objects like people, places, organizations).\n",
    "\n",
    "### Common Entity Types:\n",
    "- **PERSON**: Names of people (e.g., \"John Smith\", \"Elon Musk\")\n",
    "- **ORG**: Organizations (e.g., \"Google\", \"NASA\", \"Apple\")\n",
    "- **GPE**: Geographic/Political entities (e.g., \"France\", \"New York\", \"USA\")\n",
    "- **PRODUCT**: Products/Objects (e.g., \"iPhone\", \"Tesla Model 3\")\n",
    "- **MONEY**: Monetary values (e.g., \"$100\", \"‚Ç¨50\")\n",
    "- **DATE**: Dates (e.g., \"Monday\", \"February 14\")\n",
    "- **TIME**: Times (e.g., \"2:30 PM\", \"3 hours\")\n",
    "- **EVENT**: Events (e.g., \"World Cup\", \"Olympics\")\n",
    "\n",
    "### Applications:\n",
    "- Information extraction\n",
    "- Resume parsing\n",
    "- Chatbot understanding\n",
    "- Knowledge graph building\n",
    "- Recommendation systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## EXAMPLE: Named Entity Recognition with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Elon Musk founded Tesla in California. On February 14, 2024, he announced a new product worth $500 million.\n",
      "\n",
      "============================================================\n",
      "\n",
      "Named Entities Found:\n",
      "------------------------------------------------------------\n",
      "Text: Elon Musk            | Type: PERSON     | Start: 0 | End: 9\n",
      "Text: Tesla                | Type: ORG        | Start: 18 | End: 23\n",
      "Text: California           | Type: GPE        | Start: 27 | End: 37\n",
      "Text: February 14, 2024    | Type: DATE       | Start: 42 | End: 59\n",
      "Text: $500 million         | Type: MONEY      | Start: 94 | End: 106\n",
      "\n",
      "Total entities found: 5\n"
     ]
    }
   ],
   "source": [
    "# Sample text with multiple entities\n",
    "text = \"Elon Musk founded Tesla in California. On February 14, 2024, he announced a new product worth $500 million.\"\n",
    "\n",
    "# Process with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nNamed Entities Found:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Display entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"Text: {ent.text:<20} | Type: {ent.label_:<10} | Start: {ent.start_char} | End: {ent.end_char}\")\n",
    "\n",
    "print(f\"\\nTotal entities found: {len(doc.ents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90668f71",
   "metadata": {},
   "source": [
    "## EXAMPLE: Filtering Entities by Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Apple CEO Tim Cook announced a new iPhone in California. The price is $999 on September 12, 2024.\n",
      "\n",
      "============================================================\n",
      "\n",
      "Extracted Entities by Type:\n",
      "  People: ['Tim Cook']\n",
      "  Organizations: ['Apple', 'iPhone']\n",
      "  Locations: ['California']\n",
      "  Dates: ['September 12, 2024']\n",
      "  Money: ['999']\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"Apple CEO Tim Cook announced a new iPhone in California. The price is $999 on September 12, 2024.\"\n",
    "\n",
    "# Process with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Filter entities by type\n",
    "people = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "orgs = [ent.text for ent in doc.ents if ent.label_ == 'ORG']\n",
    "locations = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "dates = [ent.text for ent in doc.ents if ent.label_ == 'DATE']\n",
    "money = [ent.text for ent in doc.ents if ent.label_ == 'MONEY']\n",
    "\n",
    "print(\"\\nExtracted Entities by Type:\")\n",
    "print(f\"  People: {people}\")\n",
    "print(f\"  Organizations: {orgs}\")\n",
    "print(f\"  Locations: {locations}\")\n",
    "print(f\"  Dates: {dates}\")\n",
    "print(f\"  Money: {money}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "# EXERCISES: 15 Tasks Organized by Difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## ‚≠ê EASY: Exercise 1 - Process Text with spaCy and Print Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 1: Process text and display all tokens\ntext = \"Python is a powerful programming language.\"\n\n# TODO: Process text with spaCy\ndoc = ___\n\n# TODO: Print total tokens\nprint(f\"Text: {text}\")\nprint(f\"Total tokens: {doc}\")\n\nprint(\"\\nTokens:\")\nfor token in doc:\n    print(f\"  - {token.text}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## ‚≠ê EASY: Exercise 2 - Get POS Tag for Each Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 2: Get POS tag for each word in a sentence\ntext = \"Dogs love playing fetch.\"\n\n# TODO: Process text with spaCy\ndoc = ___\n\nprint(f\"Text: {text}\")\nprint(\"\\nWord -> POS Tag:\")\nfor token in doc:\n    print(f\"  {token.text:<10} -> {token.pos_}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## ‚≠ê EASY: Exercise 3 - Find All Entities in a Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 3: Extract all entities from text\ntext = \"Barack Obama was the 44th President of the United States.\"\n\n# TODO: Process text with spaCy\ndoc = ___\n\nprint(f\"Text: {text}\")\nprint(f\"\\nEntities found: {doc}\")\nfor ent in doc.ents:\n    print(f\"  - {ent.text}: {ent.label_}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## ‚≠ê EASY: Exercise 4 - Count Tokens in a Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 4: Count total tokens in text\ntext = \"Machine learning is transforming industries worldwide.\"\ndoc = ___\n\n# TODO: Count the total number of tokens\ntoken_count = ___\n\nprint(f\"Text: {text}\")\nprint(f\"Total tokens: {token_count}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## ‚≠ê EASY: Exercise 5 - Print Entity Text and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 5: Display entity details in formatted table\ntext = \"Google CEO Sundar Pichai lives in California.\"\ndoc = ___\n\nprint(f\"Text: {text}\")\nprint(\"\\nEntity Details:\")\nprint(f\"{'Entity Text':<20} | {'Entity Type':<10}\")\nprint(\"-\" * 32)\nfor ent in doc.ents:\n    print(f\"{ent.text:<20} | {ent.label_:<10}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê MEDIUM: Exercise 6 - Extract Only Nouns from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 6: Extract all nouns from text\ntext = \"The cat and dog played in the park.\"\ndoc = ___\n\n# TODO: Extract only NOUN tokens\nnouns = ___\n\nprint(f\"Text: {text}\")\nprint(f\"Nouns extracted: {nouns}\")\nprint(f\"Total nouns: {len(nouns)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê MEDIUM: Exercise 7 - Extract Only PERSON Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 7: Extract all PERSON entities\ntext = \"Steve Jobs founded Apple. Bill Gates created Microsoft. Both revolutionized technology.\"\ndoc = ___\n\n# TODO: Extract all entities with label_ == 'PERSON'\npeople = ___\n\nprint(f\"Text: {text}\")\nprint(f\"People mentioned: {people}\")\nprint(f\"Total people: {len(people)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê MEDIUM: Exercise 8 - Get All Verbs and Their Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 8: Extract verbs and their base forms (lemmas)\ntext = \"The students study hard. They studied chemistry yesterday.\"\ndoc = ___\n\n# TODO: Extract VERB tokens with their lemmas\nverbs = []\nfor token in doc:\n    if token.pos_ == 'VERB':\n        verbs.append((token.text, token.lemma_))\n\nprint(f\"Text: {text}\")\nprint(\"\\nVerbs and their lemmas:\")\nfor word, lemma in verbs:\n    print(f\"  {word} -> {lemma}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê MEDIUM: Exercise 9 - Compare Entities in Two Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 9: Extract and compare entities from two different texts\ntext1 = \"John works at Microsoft in Seattle.\"\ntext2 = \"Sarah works at Google in California.\"\n\ndoc1 = nlp(text1)\ndoc2 = nlp(text2)\n\n# TODO: Extract PERSON entities from both docs\npeople_text1 = ___\npeople_text2 = ___\n\nprint(\"Text 1:\", text1)\nprint(\"People:\", people_text1)\nprint()\nprint(\"Text 2:\", text2)\nprint(\"People:\", people_text2)\nprint()\nprint(\"People in both texts:\", set(people_text1) & set(people_text2))"
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê MEDIUM: Exercise 10 - Count POS Tag Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 10: Count how many tokens of each POS type exist\ntext = \"The quick brown fox jumps over the lazy dog and runs away.\"\ndoc = ___\n\n# TODO: Create a dictionary to count each POS tag\npos_counts = {}\nfor token in doc:\n    pos = ___\n    if pos in pos_counts:\n        pos_counts[___] += 1\n    else:\n        pos_counts[___] = 1\n\nprint(f\"Text: {text}\")\nprint(\"\\nPOS Tag Frequencies:\")\nfor pos, count in sorted(pos_counts.items()):\n    print(f\"  {pos}: {count}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê‚≠ê HARD: Exercise 11 - Build Entity Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 11: Create a function that returns entities organized by type\ndef extract_entities_by_type(text):\n    \"\"\"Extract entities and organize them by type in a dictionary.\"\"\"\n    doc = ___\n    \n    # TODO: Initialize a dictionary with entity types as keys\n    entities = {\n        'PERSON': [],\n        'ORG': [],\n        'GPE': []\n    }\n    \n    # TODO: Loop through entities and add them to the appropriate list\n    for ent in doc.ents:\n        if ent.label_ in entities:\n            entities[___].append___\n    \n    return entities\n\n# Test the function\ntext = \"Elon Musk works at Tesla in California. Jeff Bezos founded Amazon in Seattle.\"\nresult = ___\n\nprint(f\"Text: {text}\")\nprint(\"\\nExtracted Entities by Type:\")\nfor entity_type, entities_list in result.items():\n    print(f\"  {entity_type}: {entities_list}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê‚≠ê HARD: Exercise 12 - Analyze Paragraph: Entities, POS, Key Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 12: Comprehensive analysis of a paragraph\nparagraph = \"\"\"Artificial intelligence is transforming the world.\nCompanies like Google, Microsoft, and OpenAI are investing heavily.\nResearchers in California and New York are making breakthrough discoveries every day.\"\"\"\n\ndoc = ___\n\n# TODO: Extract unique entities\nunique_entities = ___\n\n# TODO: Count NOUN tokens\nnoun_count = ___\n\n# TODO: Extract nouns (not just count)\nkey_nouns = ___\n\nprint(\"PARAGRAPH ANALYSIS\")\nprint(\"=\" * 60)\nprint(f\"Text: {paragraph[:100]}...\\n\")\nprint(f\"Total entities: {len(doc.ents)}\")\nprint(f\"Unique entities: {unique_entities}\")\nprint(f\"\\nNoun count: {noun_count}\")\nprint(f\"Key nouns: {key_nouns}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê‚≠ê HARD: Exercise 13 - Process Multiple Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 13: Extract and find common entities across multiple documents\ndocuments = [\n    \"Apple released the iPhone in California.\",\n    \"Google headquarters is also in California.\",\n    \"Microsoft is based in Washington.\"\n]\n\n# TODO: Process each document and collect all ORG entities\nall_organizations = []\nfor doc_text in documents:\n    doc = ___\n    orgs = ___\n    all_organizations.extend(orgs)\n\n# TODO: Find organizations that appear in multiple documents\ncommon_orgs = ___\n\nprint(\"MULTI-DOCUMENT ANALYSIS\")\nprint(\"=\" * 60)\nfor i, doc_text in enumerate(documents, 1):\n    print(f\"{i}. {doc_text}\")\n\nprint(f\"\\nAll organizations found: {set(all_organizations)}\")\nprint(f\"Organizations appearing multiple times: {set(common_orgs)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê‚≠ê HARD: Exercise 14 - Build Text Highlighting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 14: Create a function that highlights entities in text\ndef highlight_entities(text, entity_type='PERSON'):\n    \"\"\"Highlight entities of a specific type in text.\"\"\"\n    doc = ___\n    \n    # TODO: Build a list of entity positions\n    entity_spans = []\n    for ent in doc.ents:\n        if ___:\n            entity_spans.append___\n    \n    # TODO: Sort by start position and create highlighted version\n    highlighted = \"\"\n    last_end = 0\n    \n    for start, end, entity_text in sorted(entity_spans):\n        highlighted += text[last_end:start]\n        highlighted += f\"[{entity_text.upper()}]\"\n        last_end = end\n    \n    highlighted += text[last_end:]\n    return highlighted\n\n# Test the function\ntext = \"John Smith and Mary Johnson work at Apple.\"\nresult = ___\n\nprint(f\"Original: {text}\")\nprint(f\"Highlighted: {result}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê‚≠ê HARD: Exercise 15 - Create NER Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 15: Generate a comprehensive NER summary report\nnews_article = \"\"\"Tech giant Apple announced record earnings on January 15, 2024.\nCEO Tim Cook, based in Cupertino, California, presented the results to investors.\nThe company stock price increased by 5 percent to $185 per share.\nAnalysts predict further growth in the coming quarter.\nMicrosoft and Google also reported strong performance this month.\"\"\"\n\ndef generate_ner_report(text):\n    \"\"\"Generate a comprehensive NER summary report.\"\"\"\n    doc = ___\n    \n    # TODO: Initialize report dictionary\n    report = {\n        'PERSON': [],\n        'ORG': [],\n        'GPE': [],\n        'MONEY': [],\n        'DATE': [],\n        'total_tokens': ___,\n        'total_entities': ___\n    }\n    \n    # TODO: Populate report with entities\n    for ent in doc.ents:\n        if ___ in report and isinstance(report[___], list):\n            report[___].append___\n    \n    return report\n\nreport = ___\n\nprint(\"NER SUMMARY REPORT\")\nprint(\"=\" * 60)\nprint(f\"Total tokens: {report['total_tokens']}\")\nprint(f\"Total entities: {report['total_entities']}\")\n\nprint(\"\\nEntity Breakdown:\")\nfor entity_type in ['PERSON', 'ORG', 'GPE', 'MONEY', 'DATE']:\n    if report[entity_type]:\n        print(f\"  {entity_type:10} ({len(report[entity_type])}): {report[entity_type]}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-bonus-header",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚≠ê‚≠ê‚≠ê BONUS HARD EXERCISES (16-20)\n",
    "**Extra challenges for deeper practice!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ex16-header",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê‚≠ê HARD: Exercise 16 - Dependency Parsing & Sentence Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ex16-code",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 16: Analyze dependency relationships between words\ntext = \"The talented engineer designed an innovative AI system for the company.\"\ndoc = ___\n\n# TODO: Print each token with its dependency label and head word\nprint(\"DEPENDENCY PARSING\")\nprint(\"=\" * 60)\nprint(f\"{'Token':<15} | {'DEP':<12} | {'Head Word':<15} | {'POS':<8}\")\nprint(\"-\" * 60)\nfor token in doc:\n    print(f\"{token.text:<15} | {token.dep_:<12} | {token.head.text:<15} | {token.pos_:<8}\")\n\n# TODO: Find the ROOT verb of the sentence\nroot_verb = ___\nprint(f\"\\nRoot verb: {root_verb}\")\n\n# TODO: Find all direct objects (dobj)\ndirect_objects = ___\nprint(f\"Direct objects: {direct_objects}\")\n\n# TODO: Find all subjects (nsubj)\nsubjects = ___\nprint(f\"Subjects: {subjects}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-ex17-header",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê‚≠ê HARD: Exercise 17 - Entity Frequency & Ranking Across Multiple Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ex17-code",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 17: Build an entity frequency ranker from multiple news headlines\nheadlines = [\n    \"Apple launches new iPhone in California and New York.\",\n    \"Google and Microsoft compete in AI race.\",\n    \"Tim Cook visits Google headquarters in California.\",\n    \"Microsoft acquires startup in New York for $2 billion.\",\n    \"Apple and Google announce partnership in California.\",\n    \"Elon Musk praises Microsoft AI tools.\",\n    \"Jeff Bezos invests in California tech startup.\"\n]\n\n# TODO: Build a frequency dictionary for ALL entity types\nentity_freq = {}  # key: (entity_text, entity_label), value: count\n\nfor headline in headlines:\n    doc = ___\n    for ent in doc.ents:\n        key = (ent.text, ent.label_)\n        entity_freq[key] = entity_freq.get(key, 0) + 1\n\n# TODO: Sort by frequency (highest first)\nranked = sorted(entity_freq.items(), key=lambda x: x[1], reverse=True)\n\nprint(\"ENTITY FREQUENCY RANKING\")\nprint(\"=\" * 60)\nprint(f\"{'Rank':<6} | {'Entity':<20} | {'Type':<10} | {'Count':<6}\")\nprint(\"-\" * 60)\nfor rank, ((text, label), count) in enumerate(ranked, 1):\n    print(f\"{rank:<6} | {text:<20} | {label:<10} | {count:<6}\")\n\n# TODO: Find the most mentioned ORG\ntop_org = ___\nprint(f\"\\nMost mentioned organization: {top_org[0] if top_org else 'None'}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-ex18-header",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê‚≠ê HARD: Exercise 18 - Build a POS Pattern Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ex18-code",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 18: Find specific POS patterns (e.g., ADJ + NOUN combinations)\ntexts = [\n    \"The brilliant scientist made an amazing discovery.\",\n    \"A dangerous storm hit the coastal city yesterday.\",\n    \"The young developer built a powerful application.\"\n]\n\ndef find_pos_patterns(text, pattern):\n    \"\"\"Find consecutive POS tag patterns in text.\n    pattern: list of POS tags, e.g., ['ADJ', 'NOUN']\n    Returns list of matched phrases.\n    \"\"\"\n    doc = ___\n    matches = []\n    tokens = list(doc)\n    \n    # TODO: Slide a window of pattern length across tokens\n    for i in range(len(tokens) - len(pattern) + 1):\n        window = tokens[i:i + len(pattern)]\n        \n        # TODO: Check if POS tags match the pattern\n        pos_tags = ___\n        if pos_tags == pattern:\n            phrase = \" \".join___\n            matches.append(phrase)\n    \n    return matches\n\n# Find ADJ + NOUN patterns\nprint(\"ADJ + NOUN Patterns Found:\")\nprint(\"=\" * 40)\nfor text in texts:\n    results = find_pos_patterns(text, ['ADJ', 'NOUN'])\n    print(f\"  '{text[:40]}...'\")\n    print(f\"    Matches: {results}\")\n\n# TODO: Now find NOUN + VERB patterns\nprint(\"\\nNOUN + VERB Patterns Found:\")\nprint(\"=\" * 40)\nfor text in texts:\n    results = find_pos_patterns(text, ['NOUN', 'VERB'])\n    print(f\"  '{text[:40]}...'\")\n    print(f\"    Matches: {results}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-ex19-header",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê‚≠ê HARD: Exercise 19 - Entity Relationship Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ex19-code",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 19: Map relationships between PERSON and ORG entities in same sentence\ntexts = [\n    \"Tim Cook is the CEO of Apple.\",\n    \"Sundar Pichai leads Google. Satya Nadella manages Microsoft.\",\n    \"Jensen Huang founded NVIDIA in California.\",\n    \"Mark Zuckerberg built Meta from his Harvard dorm room.\"\n]\n\ndef extract_person_org_relations(text):\n    \"\"\"Find PERSON-ORG pairs that appear in the same sentence.\"\"\"\n    doc = ___\n    relations = []\n    \n    # TODO: Loop through each sentence in the document\n    for sent in doc.sents:\n        # TODO: Extract PERSON and ORG entities from this sentence\n        persons = ___\n        orgs = ___\n        \n        # TODO: Create pairs of (person, org) for entities in same sentence\n        for person in persons:\n            for org in orgs:\n                relations.append((person, org))\n    \n    return relations\n\nprint(\"PERSON ‚Üî ORG RELATIONSHIPS\")\nprint(\"=\" * 50)\nall_relations = []\nfor text in texts:\n    rels = extract_person_org_relations(text)\n    all_relations.extend(rels)\n    for person, org in rels:\n        print(f\"  {person} ‚Üí {org}\")\n\nprint(f\"\\nTotal relationships found: {len(all_relations)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-ex20-header",
   "metadata": {},
   "source": [
    "## ‚≠ê‚≠ê‚≠ê HARD: Exercise 20 - Complete NLP Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ex20-code",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 20: Build a complete text analysis pipeline combining POS + NER + Stats\narticle = \"\"\"Artificial intelligence company OpenAI, led by Sam Altman,\nreleased ChatGPT in November 2022. The product gained 100 million users\nwithin two months. Google responded by launching Bard, while Microsoft\ninvested $10 billion in OpenAI. Experts in San Francisco and London\npredict AI will transform every industry by 2030.\"\"\"\n\ndef full_nlp_pipeline(text):\n    \"\"\"Run complete NLP analysis: tokenization, POS, NER, and statistics.\"\"\"\n    doc = ___\n    \n    # --- SECTION 1: Basic Stats ---\n    # TODO: Count sentences, tokens, and unique tokens\n    num_sentences = len(list(doc.sents))\n    num_tokens = len(doc)\n    unique_tokens = len(set___)\n    \n    # --- SECTION 2: POS Distribution ---\n    # TODO: Get top 3 most common POS tags\n    pos_counts = {}\n    for token in doc:\n        if not token.is_punct and not token.is_space:\n            pos = token.pos_\n            pos_counts[pos] = pos_counts.get(pos, 0) + 1\n    \n    top_pos = sorted(pos_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n    \n    # --- SECTION 3: Entity Analysis ---\n    # TODO: Group entities by type\n    entity_groups = {}\n    for ent in doc.ents:\n        label = ent.label_\n        if label not in entity_groups:\n            entity_groups[label] = []\n        entity_groups[label].append(ent.text)\n    \n    # --- SECTION 4: Key Phrases (ADJ+NOUN) ---\n    # TODO: Extract adjective-noun phrases\n    key_phrases = []\n    tokens_list = list(doc)\n    for i in range(len(tokens_list) - 1):\n        if tokens_list[i].pos_ == 'ADJ' and tokens_list[i+1].pos_ == 'NOUN':\n            key_phrases.append(f\"{tokens_list[i].text} {tokens_list[i+1].text}\")\n    \n    return {\n        'sentences': num_sentences,\n        'tokens': num_tokens,\n        'unique_tokens': unique_tokens,\n        'top_pos': top_pos,\n        'entities': entity_groups,\n        'key_phrases': key_phrases\n    }\n\n# Run the pipeline\nresults = ___\n\n# Display results\nprint(\"COMPLETE NLP ANALYSIS REPORT\")\nprint(\"=\" * 60)\nprint(f\"\\nüìä BASIC STATS:\")\nprint(f\"   Sentences: {results['sentences']}\")\nprint(f\"   Tokens: {results['tokens']}\")\nprint(f\"   Unique tokens: {results['unique_tokens']}\")\n\nprint(f\"\\nüè∑Ô∏è TOP POS TAGS:\")\nfor pos, count in results['top_pos']:\n    print(f\"   {pos}: {count}\")\n\nprint(f\"\\nüîç ENTITIES FOUND:\")\nfor label, entities in results['entities'].items():\n    print(f\"   {label}: {entities}\")\n\nprint(f\"\\nüìù KEY PHRASES (ADJ+NOUN):\")\nfor phrase in results['key_phrases']:\n    print(f\"   - {phrase}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "- **POS Tagging** identifies the grammatical role of words (NOUN, VERB, ADJ, etc.)\n",
    "- **Named Entity Recognition** identifies and classifies named entities (people, places, organizations, etc.)\n",
    "- **spaCy** is a powerful library for both tasks with pre-trained models\n",
    "- These techniques enable information extraction and deeper text understanding\n",
    "\n",
    "### Real-World Applications:\n",
    "- Resume parsing and job matching\n",
    "- News article analysis\n",
    "- Customer support automation\n",
    "- Knowledge base construction\n",
    "- Question answering systems\n",
    "\n",
    "### What's Next:\n",
    "Tomorrow we'll learn **Sentiment Analysis** - determining whether text expresses positive, negative, or neutral emotions!\n",
    "\n",
    "---\n",
    "\n",
    "*Created for Natruja's NLP study plan*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "nlp_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}