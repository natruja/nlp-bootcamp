{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 5: Text Vectorization & Topic Modeling\n",
    "**The AI Engineer Course 2026 - Sections 24 & 25**\n",
    "\n",
    "**Student:** Natruja\n",
    "\n",
    "**Date:** Monday, February 16, 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand text vectorization (converting words to numbers)\n",
    "2. Learn Bag of Words (BoW) and TF-IDF\n",
    "3. Understand topic modeling concepts\n",
    "4. Use Latent Dirichlet Allocation (LDA)\n",
    "5. Apply Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install scikit-learn\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn\", \"nltk\", \"-q\"])\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"✓ Libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Convert Text to Numbers?\n",
    "\n",
    "Machine learning algorithms work with numbers, not text. We need to convert words into numerical representations.\n",
    "\n",
    "### The Problem:\n",
    "- Algorithms need vectors (lists of numbers)\n",
    "- Can't process raw text directly\n",
    "- Need to preserve word meaning and relationships\n",
    "\n",
    "### The Solution:\n",
    "- **Vectorization**: Convert documents into vectors\n",
    "- Each number represents a word's importance\n",
    "- Algorithms can now compare documents numerically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Bag of Words (BoW) - CountVectorizer\n",
    "\n",
    "**Bag of Words** counts how many times each word appears in a document.\n",
    "\n",
    "### How It Works:\n",
    "1. Create vocabulary of all unique words\n",
    "2. For each document, count word occurrences\n",
    "3. Create vector with word counts\n",
    "\n",
    "### Example:\n",
    "- Doc1: \"I love machine learning\" → [0, 1, 1, 1, 1]\n",
    "- Doc2: \"I love Python\" → [1, 1, 0, 0, 0]\n",
    "\n",
    "### Pros & Cons:\n",
    "- Pros: Simple, fast, works well\n",
    "- Cons: Ignores word order, treats \"the\" same as \"important\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"machine learning is fun\",\n",
    "    \"machine learning is powerful\",\n",
    "    \"deep learning is amazing\",\n",
    "    \"python is great for machine learning\"\n",
    "]\n",
    "\n",
    "# Create CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Bag of Words Vectorization:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Vocabulary: {feature_names}\")\n",
    "print(f\"\\nWord count vectors (shape: {bow_matrix.shape}):\")\n",
    "print(bow_matrix.toarray())\n",
    "\n",
    "# Display in readable format\n",
    "print(\"\\nDetailed view:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\nDoc {i+1}: \\\"{doc}\\\"\")\n",
    "    print(f\"  Vector: {bow_matrix[i].toarray()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "**TF-IDF** gives more weight to important, unique words and less to common words.\n",
    "\n",
    "### How It Works:\n",
    "- **Term Frequency (TF)**: How often a word appears in a document\n",
    "- **Inverse Document Frequency (IDF)**: How unique is the word across all documents\n",
    "- **TF-IDF**: TF × IDF (words that appear often but are rare across docs get high scores)\n",
    "\n",
    "### Example:\n",
    "- \"the\" appears 100 times in every document → Low IDF, low TF-IDF\n",
    "- \"quantum\" appears 5 times in one document → High IDF, high TF-IDF\n",
    "\n",
    "### Why Better Than BoW:\n",
    "- Reduces importance of common words (the, a, is)\n",
    "- Highlights distinctive words\n",
    "- Better for topic detection and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same documents\n",
    "documents = [\n",
    "    \"machine learning is fun\",\n",
    "    \"machine learning is powerful\",\n",
    "    \"deep learning is amazing\",\n",
    "    \"python is great for machine learning\"\n",
    "]\n",
    "\n",
    "# Create TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"TF-IDF Vectorization:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTF-IDF scores (shape: {tfidf_matrix.shape}):\")\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# Show top words per document\n",
    "print(\"\\nTop 3 important words per document:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\nDoc {i+1}: \\\"{doc}\\\"\")\n",
    "    # Get indices of top 3 values\n",
    "    top_indices = tfidf_matrix[i].toarray()[0].argsort()[-3:]\n",
    "    for idx in sorted(top_indices, reverse=True):\n",
    "        if tfidf_matrix[i].toarray()[0][idx] > 0:\n",
    "            print(f\"  {feature_names[idx]}: {tfidf_matrix[i].toarray()[0][idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling: Finding Hidden Themes\n",
    "\n",
    "**Topic Modeling** discovers abstract topics in a collection of documents.\n",
    "\n",
    "### What is a Topic?\n",
    "- A collection of words that frequently appear together\n",
    "- Example: {\"machine\", \"learning\", \"algorithm\", \"data\"}\n",
    "- Documents can have multiple topics\n",
    "\n",
    "### Real-World Applications:\n",
    "- News categorization\n",
    "- Document organization\n",
    "- Discovering trends\n",
    "- Understanding document collections\n",
    "\n",
    "### Two Popular Algorithms:\n",
    "1. **LDA (Latent Dirichlet Allocation)**: Probabilistic approach\n",
    "2. **NMF (Non-Negative Matrix Factorization)**: Matrix decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"machine learning is a subset of artificial intelligence\",\n",
    "    \"deep learning uses neural networks for data analysis\",\n",
    "    \"python is popular for machine learning and data science\",\n",
    "    \"neural networks are inspired by biological neurons\",\n",
    "    \"data science involves statistics and programming\",\n",
    "    \"natural language processing helps machines understand text\"\n",
    "]\n",
    "\n",
    "# Vectorize using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_df=0.8, min_df=1, stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Fit LDA model (2 topics)\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42, max_iter=10)\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"LDA Topic Modeling:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFound 2 topics from {len(documents)} documents\")\n",
    "print(f\"\\nTop words per topic:\")\n",
    "\n",
    "# Display topics\n",
    "n_top_words = 5\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words_idx = topic.argsort()[-n_top_words:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"\\nTopic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "\n",
    "# Show topic distribution per document\n",
    "print(f\"\\n\\nTopic distribution per document:\")\n",
    "doc_topic = lda.transform(doc_term_matrix)\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\nDoc {i+1}: \\\"{doc[:50]}...\\\"\")\n",
    "    print(f\"  Topic 1: {doc_topic[i][0]:.2f}\")\n",
    "    print(f\"  Topic 2: {doc_topic[i][1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISES: Organized by Difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 1 - Create CountVectorizer on Simple Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a simple CountVectorizer and fit on documents\n\ndocs = [\n    \"I like cats\",\n    \"I like dogs\",\n    \"cats and dogs\"\n]\n\n# TODO: Create a CountVectorizer object\nvec = ___\n\n# TODO: Fit and transform the documents\nmatrix = ___\n\nprint(f\"Success! Matrix shape: {matrix.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 2 - Print Vocabulary (Feature Names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Print the vocabulary (all unique words) from a vectorizer\n\ndocs = [\n    \"hello world\",\n    \"world of python\",\n    \"python hello\"\n]\n\nvec = CountVectorizer()\nmatrix = vec.fit_transform(docs)\n\n# TODO: Get the feature names (vocabulary) using get_feature_names_out()\nwords = ___\n\n# TODO: Print the vocabulary\nprint(\"Vocabulary:\")\nprint(___)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 3 - Check Matrix Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Understand the shape of the vectorized matrix\n\ndocs = [\n    \"apple orange\",\n    \"banana apple\",\n    \"orange banana apple\",\n    \"apple apple banana\"\n]\n\nvec = CountVectorizer()\nmatrix = vec.fit_transform(docs)\n\n# TODO: Get the shape of the matrix\nshape = ___\n\nprint(f\"Matrix shape: {shape}\")\nprint(f\"Number of documents: {shape[0]}\")\nprint(f\"Number of unique words: {shape[1]}\")\n\n# TODO: Print what the shape means\nprint(f\"Interpretation: We have {shape[0]} documents and {shape[1]} unique words\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 4 - Create TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a TfidfVectorizer and fit on documents\n\ndocs = [\n    \"the quick brown fox\",\n    \"the lazy dog\",\n    \"quick brown dog\"\n]\n\n# TODO: Create a TfidfVectorizer object\ntfidf_vec = ___\n\n# TODO: Fit and transform the documents\ntfidf_matrix = ___\n\nprint(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\nprint(f\"\\nTF-IDF matrix (converted to array):\")\nprint(tfidf_matrix.toarray())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 5 - Stop Words Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare vectorization with and without stop words\n\ndocs = [\n    \"the machine learning is important\",\n    \"machine learning is powerful\",\n    \"important learning algorithms\"\n]\n\n# TODO: Create CountVectorizer WITHOUT stop words\nvec_no_stop = ___\nmatrix_no_stop = vec_no_stop.fit_transform(docs)\n\n# TODO: Create CountVectorizer WITH English stop words\nvec_with_stop = ___\nmatrix_with_stop = vec_with_stop.fit_transform(docs)\n\nprint(f\"Without stop words: {len(vec_no_stop.get_feature_names_out())} words\")\nprint(f\"Vocabulary: {list(vec_no_stop.get_feature_names_out())}\")\nprint(f\"\\nWith stop words: {len(vec_with_stop.get_feature_names_out())} words\")\nprint(f\"Vocabulary: {list(vec_with_stop.get_feature_names_out())}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 6 - Compare BoW vs TF-IDF on Same Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare Bag of Words and TF-IDF on the same documents\n\ndocs = [\n    \"data science data analysis\",\n    \"machine learning models\",\n    \"data science and machine learning\"\n]\n\n# TODO: Create and fit CountVectorizer\nbow_vec = ___\nbow_matrix = ___\n\n# TODO: Create and fit TfidfVectorizer\ntfidf_vec = ___\ntfidf_matrix = ___\n\nprint(\"Bag of Words (word counts):\")\nprint(bow_matrix.toarray())\n\nprint(\"\\nTF-IDF (weighted importance):\")\nprint(tfidf_matrix.toarray())\n\nprint(\"\\nNote: TF-IDF reduces weight on 'data' because it appears in multiple docs\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 7 - Use Bigrams with ngram_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create vectors with bigrams (2-word phrases)\n\ndocs = [\n    \"machine learning is fun\",\n    \"deep learning networks\",\n    \"machine learning algorithms\"\n]\n\n# TODO: Create CountVectorizer with ngram_range for bigrams (1,2)\n# This should capture both individual words and 2-word phrases\nvec = ___\nmatrix = vec.fit_transform(docs)\n\n# TODO: Get feature names to see the bigrams\nfeatures = ___\n\nprint(f\"Features (words + bigrams): {features}\")\nprint(f\"\\nTotal features: {len(features)}\")\nprint(f\"\\nMatrix shape: {matrix.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 8 - Limit Vocabulary with max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Limit vocabulary size using max_features parameter\n\ndocs = [\n    \"python java javascript ruby programming languages\",\n    \"data science machine learning analytics\",\n    \"web development frontend backend\",\n    \"artificial intelligence neural networks deep learning\"\n]\n\n# TODO: Create CountVectorizer with max_features=5 (keep only top 5 words)\nvec = ___\nmatrix = vec.fit_transform(docs)\n\n# TODO: Get the feature names\nwords = ___\n\nprint(f\"Vocabulary limited to {len(words)} words:\")\nprint(words)\nprint(f\"\\nMatrix shape: {matrix.shape}\")\nprint(\"Note: Only the most frequent words are kept\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 9 - Convert Sparse Matrix to Array and Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert sparse matrix to dense array and inspect individual values\n\ndocs = [\n    \"cat dog pet\",\n    \"dog animal\",\n    \"cat pet animal\"\n]\n\nvec = CountVectorizer()\nsparse_matrix = vec.fit_transform(docs)\n\n# TODO: Convert sparse matrix to dense array using .toarray()\ndense_array = ___\n\n# TODO: Print the dense array\nprint(\"Dense array:\")\nprint(___)\n\n# TODO: Get and print a single document's vector (first document)\nfirst_doc_vector = ___\nprint(f\"\\nFirst document vector: {first_doc_vector}\")\n\n# TODO: Get value at position [1, 2] (second doc, third word)\nvalue = ___\nprint(f\"\\nValue at position [1, 2]: {value}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 10 - Apply LDA with n_components=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply LDA topic modeling on simple documents\n\ndocs = [\n    \"machine learning algorithm data\",\n    \"neural network deep learning\",\n    \"machine learning model training\",\n    \"deep neural network training\"\n]\n\n# TODO: Create CountVectorizer and fit_transform\nvec = ___\nmatrix = ___\n\n# TODO: Create LatentDirichletAllocation with n_components=2\nlda = ___\n\n# TODO: Fit the LDA model\n___\n\n# TODO: Get feature names\nwords = ___\n\nprint(\"Topics found:\")\nfor topic_idx, topic in enumerate(lda.components_):\n    # Get top 3 word indices\n    top_indices = topic.argsort()[-3:]\n    top_words = ___\n    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 11 - Complete Vectorization Pipeline with Stop Words + Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build a complete vectorization pipeline with multiple parameters\n\ndocs = [\n    \"the quick brown fox jumps over the lazy dog\",\n    \"the lazy dog sleeps all day\",\n    \"quick brown dogs love to run\",\n    \"the fox is quick and clever\"\n]\n\n# TODO: Create TfidfVectorizer with:\n# - stop_words='english'\n# - ngram_range=(1, 2)\n# - max_features=10\nvec = ___\n\n# TODO: Fit and transform the documents\nmatrix = ___\n\n# TODO: Get feature names\nfeatures = ___\n\nprint(f\"Features (unigrams + bigrams, no stop words, max 10):\")\nprint(features)\nprint(f\"\\nMatrix shape: {matrix.shape}\")\nprint(f\"\\nTF-IDF values:\")\nprint(matrix.toarray())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 12 - Apply LDA and Print Top Words Per Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply LDA and display top N words for each topic\n\ndocs = [\n    \"python machine learning data science algorithms\",\n    \"deep neural networks artificial intelligence\",\n    \"machine learning models training data\",\n    \"neural networks deep learning frameworks\",\n    \"data science analytics python programming\",\n    \"artificial intelligence machine learning\"\n]\n\n# TODO: Vectorize with CountVectorizer (stop_words='english')\nvec = ___\nmatrix = ___\n\n# TODO: Create and fit LDA with n_components=3 (3 topics)\nlda = ___\n___\n\n# TODO: Get feature names\nwords = ___\n\n# TODO: Print top 5 words for each topic\nprint(\"Topic Modeling Results (LDA):\")\nfor topic_idx, topic in enumerate(lda.components_):\n    # Get top 5 word indices\n    top_indices = ___\n    top_words = ___\n    print(f\"\\nTopic {topic_idx + 1}: {', '.join(top_words)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 13 - Apply NMF and Compare with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Apply NMF topic modeling and compare with LDA results\n\ndocs = [\n    \"car vehicle automobile transportation\",\n    \"airplane flight aircraft travel\",\n    \"bicycle bike pedal cycle\",\n    \"train railway locomotive transport\",\n    \"ship vessel boat navigation\",\n    \"helicopter aircraft sky transport\"\n]\n\n# TODO: Vectorize with TfidfVectorizer (stop_words='english')\nvec = ___\ntfidf_matrix = ___\n\n# TODO: Create and fit NMF with n_components=2\nnmf = ___\n___\n\n# TODO: Get feature names\nwords = ___\n\n# TODO: Print top 4 words for each NMF topic\nprint(\"NMF Topics:\")\nfor topic_idx, topic in enumerate(nmf.components_):\n    top_indices = ___\n    top_words = ___\n    print(f\"\\nTopic {topic_idx + 1}: {', '.join(top_words)}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 14 - Process Document Collection and Find Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Process a complete document collection with vectorization and topic modeling\n\ndocs = [\n    \"artificial intelligence machine learning neural networks\",\n    \"python programming language data science\",\n    \"deep learning convolutional neural networks\",\n    \"machine learning algorithms classification regression\",\n    \"python libraries numpy pandas scikit learn\",\n    \"natural language processing text mining\",\n    \"data analysis statistics visualization\",\n    \"computer vision image recognition\"\n]\n\n# TODO: Vectorize with CountVectorizer\n# - stop_words='english'\n# - max_df=0.7 (ignore words in more than 70% of docs)\n# - min_df=1 (keep words in at least 1 doc)\nvec = ___\nmatrix = ___\n\n# TODO: Create and fit LDA with n_components=3\nlda = ___\n___\n\n# TODO: Get document-topic distribution\ndoc_topics = ___\n\n# TODO: Get feature names\nwords = ___\n\nprint(\"=\" * 60)\nprint(\"DOCUMENT COLLECTION ANALYSIS\")\nprint(\"=\" * 60)\n\n# Print topics\nprint(\"\\nTopics discovered:\")\nfor topic_idx, topic in enumerate(lda.components_):\n    top_indices = topic.argsort()[-5:]\n    top_words = ___\n    print(f\"\\nTopic {topic_idx + 1}: {', '.join(top_words)}\")\n\n# Print document topic assignments\nprint(f\"\\n\\nDocument-Topic Distribution:\")\nfor i, doc in enumerate(docs):\n    dominant_topic = doc_topics[i].argmax()\n    print(f\"\\nDoc {i+1}: \\\"{doc[:40]}...\\\"\")\n    print(f\"  Dominant Topic: {dominant_topic + 1}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 15 - Build Topic Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build a reusable function that performs complete topic analysis\n\n# TODO: Create a function that takes documents, n_topics, and returns summary\ndef analyze_topics(documents, n_topics=2, n_top_words=5):\n    \"\"\"\n    Analyze topics in a document collection.\n    \n    Args:\n        documents: list of text documents\n        n_topics: number of topics to extract\n        n_top_words: number of top words to show per topic\n    \n    Returns:\n        dict with 'topics', 'doc_topics', and 'vocabulary'\n    \"\"\"\n    # TODO: Vectorize documents with CountVectorizer\n    # Use: stop_words='english', max_df=0.8, min_df=1\n    vec = ___\n    matrix = ___\n    \n    # TODO: Create and fit LDA model\n    lda = ___\n    ___\n    \n    # TODO: Extract topics\n    words = ___\n    topics = []\n    \n    for topic_idx, topic in enumerate(lda.components_):\n        # Get top word indices\n        top_indices = ___\n        # Convert to words\n        top_words = ___\n        topics.append(top_words)\n    \n    # TODO: Get document-topic distribution\n    doc_topics = ___\n    \n    return {\n        'topics': topics,\n        'doc_topics': doc_topics,\n        'vocabulary': words\n    }\n\n# Test the function\ntest_docs = [\n    \"apple orange banana fruit\",\n    \"cat dog animal pet\",\n    \"apple fruit food healthy\",\n    \"dog pet companion animal\"\n]\n\nresult = ___\n\nprint(\"TOPIC ANALYSIS RESULTS\")\nprint(\"=\" * 60)\nprint(\"\\nTopics:\")\nfor i, topic_words in enumerate(result['topics']):\n    print(f\"Topic {i+1}: {', '.join(topic_words)}\")\n\nprint(f\"\\nVocabulary size: {len(result['vocabulary'])} words\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Vectorization** converts text to numbers for algorithms\n",
    "- **Bag of Words** counts word occurrences\n",
    "- **TF-IDF** weights words by importance\n",
    "- **Topic Modeling** discovers hidden themes\n",
    "- **LDA** and **NMF** are popular topic modeling algorithms\n",
    "\n",
    "### When to Use:\n",
    "- **BoW**: Fast, simple classification\n",
    "- **TF-IDF**: Better for text similarity and classification\n",
    "- **LDA**: Probabilistic topics, interpretability\n",
    "- **NMF**: Matrix decomposition, semantic topics\n",
    "\n",
    "### What's Next:\n",
    "Tomorrow we'll build a **Text Classifier** using these techniques to categorize documents!\n",
    "\n",
    "---\n",
    "\n",
    "*Created for Natruja's NLP study plan*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}