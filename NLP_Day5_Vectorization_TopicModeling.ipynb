{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 5: Text Vectorization & Topic Modeling\n",
    "**The AI Engineer Course 2026 - Sections 24 & 25**\n",
    "\n",
    "**Student:** Natruja\n",
    "\n",
    "**Date:** Monday, February 16, 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand text vectorization (converting words to numbers)\n",
    "2. Learn Bag of Words (BoW) and TF-IDF\n",
    "3. Understand topic modeling concepts\n",
    "4. Use Latent Dirichlet Allocation (LDA)\n",
    "5. Apply Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install scikit-learn\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn\", \"nltk\", \"-q\"])\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"\u2713 Libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "print(\"\u2713 All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Convert Text to Numbers?\n",
    "\n",
    "Machine learning algorithms work with numbers, not text. We need to convert words into numerical representations.\n",
    "\n",
    "### The Problem:\n",
    "- Algorithms need vectors (lists of numbers)\n",
    "- Can't process raw text directly\n",
    "- Need to preserve word meaning and relationships\n",
    "\n",
    "### The Solution:\n",
    "- **Vectorization**: Convert documents into vectors\n",
    "- Each number represents a word's importance\n",
    "- Algorithms can now compare documents numerically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Bag of Words (BoW) - CountVectorizer\n",
    "\n",
    "**Bag of Words** counts how many times each word appears in a document.\n",
    "\n",
    "### How It Works:\n",
    "1. Create vocabulary of all unique words\n",
    "2. For each document, count word occurrences\n",
    "3. Create vector with word counts\n",
    "\n",
    "### Example:\n",
    "- Doc1: \"I love machine learning\" \u2192 [0, 1, 1, 1, 1]\n",
    "- Doc2: \"I love Python\" \u2192 [1, 1, 0, 0, 0]\n",
    "\n",
    "### Pros & Cons:\n",
    "- Pros: Simple, fast, works well\n",
    "- Cons: Ignores word order, treats \"the\" same as \"important\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"machine learning is fun\",\n",
    "    \"machine learning is powerful\",\n",
    "    \"deep learning is amazing\",\n",
    "    \"python is great for machine learning\"\n",
    "]\n",
    "\n",
    "# Create CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Bag of Words Vectorization:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Vocabulary: {feature_names}\")\n",
    "print(f\"\\nWord count vectors (shape: {bow_matrix.shape}):\")\n",
    "print(bow_matrix.toarray())\n",
    "\n",
    "# Display in readable format\n",
    "print(\"\\nDetailed view:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\nDoc {i+1}: \\\"{doc}\\\"\")\n",
    "    print(f\"  Vector: {bow_matrix[i].toarray()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "**TF-IDF** gives more weight to important, unique words and less to common words.\n",
    "\n",
    "### How It Works:\n",
    "- **Term Frequency (TF)**: How often a word appears in a document\n",
    "- **Inverse Document Frequency (IDF)**: How unique is the word across all documents\n",
    "- **TF-IDF**: TF \u00d7 IDF (words that appear often but are rare across docs get high scores)\n",
    "\n",
    "### Example:\n",
    "- \"the\" appears 100 times in every document \u2192 Low IDF, low TF-IDF\n",
    "- \"quantum\" appears 5 times in one document \u2192 High IDF, high TF-IDF\n",
    "\n",
    "### Why Better Than BoW:\n",
    "- Reduces importance of common words (the, a, is)\n",
    "- Highlights distinctive words\n",
    "- Better for topic detection and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same documents\n",
    "documents = [\n",
    "    \"machine learning is fun\",\n",
    "    \"machine learning is powerful\",\n",
    "    \"deep learning is amazing\",\n",
    "    \"python is great for machine learning\"\n",
    "]\n",
    "\n",
    "# Create TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"TF-IDF Vectorization:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTF-IDF scores (shape: {tfidf_matrix.shape}):\")\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# Show top words per document\n",
    "print(\"\\nTop 3 important words per document:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\nDoc {i+1}: \\\"{doc}\\\"\")\n",
    "    # Get indices of top 3 values\n",
    "    top_indices = tfidf_matrix[i].toarray()[0].argsort()[-3:]\n",
    "    for idx in sorted(top_indices, reverse=True):\n",
    "        if tfidf_matrix[i].toarray()[0][idx] > 0:\n",
    "            print(f\"  {feature_names[idx]}: {tfidf_matrix[i].toarray()[0][idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling: Finding Hidden Themes\n",
    "\n",
    "**Topic Modeling** discovers abstract topics in a collection of documents.\n",
    "\n",
    "### What is a Topic?\n",
    "- A collection of words that frequently appear together\n",
    "- Example: {\"machine\", \"learning\", \"algorithm\", \"data\"}\n",
    "- Documents can have multiple topics\n",
    "\n",
    "### Real-World Applications:\n",
    "- News categorization\n",
    "- Document organization\n",
    "- Discovering trends\n",
    "- Understanding document collections\n",
    "\n",
    "### Two Popular Algorithms:\n",
    "1. **LDA (Latent Dirichlet Allocation)**: Probabilistic approach\n",
    "2. **NMF (Non-Negative Matrix Factorization)**: Matrix decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"machine learning is a subset of artificial intelligence\",\n",
    "    \"deep learning uses neural networks for data analysis\",\n",
    "    \"python is popular for machine learning and data science\",\n",
    "    \"neural networks are inspired by biological neurons\",\n",
    "    \"data science involves statistics and programming\",\n",
    "    \"natural language processing helps machines understand text\"\n",
    "]\n",
    "\n",
    "# Vectorize using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_df=0.8, min_df=1, stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Fit LDA model (2 topics)\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42, max_iter=10)\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"LDA Topic Modeling:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFound 2 topics from {len(documents)} documents\")\n",
    "print(f\"\\nTop words per topic:\")\n",
    "\n",
    "# Display topics\n",
    "n_top_words = 5\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words_idx = topic.argsort()[-n_top_words:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"\\nTopic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "\n",
    "# Show topic distribution per document\n",
    "print(f\"\\n\\nTopic distribution per document:\")\n",
    "doc_topic = lda.transform(doc_term_matrix)\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\nDoc {i+1}: \\\"{doc[:50]}...\\\"\")\n",
    "    print(f\"  Topic 1: {doc_topic[i][0]:.2f}\")\n",
    "    print(f\"  Topic 2: {doc_topic[i][1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISES: Organized by Difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2b50 EASY: Exercise 1 - Create CountVectorizer on Simple Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple CountVectorizer and fit on documents\n",
    "\n",
    "docs = [\n",
    "    \"I like cats\",\n",
    "    \"I like dogs\",\n",
    "    \"cats and dogs\"\n",
    "]\n",
    "\n",
    "# TODO: Create a CountVectorizer object\n",
    "vec = ___\n",
    "\n",
    "# TODO: Fit and transform the documents\n",
    "matrix = ___\n",
    "\n",
    "print(f\"Success! Matrix shape: {matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2b50 EASY: Exercise 2 - Print Vocabulary (Feature Names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the vocabulary (all unique words) from a vectorizer\n",
    "\n",
    "docs = [\n",
    "    \"hello world\",\n",
    "    \"world of python\",\n",
    "    \"python hello\"\n",
    "]\n",
    "\n",
    "vec = CountVectorizer()\n",
    "matrix = vec.fit_transform(docs)\n",
    "\n",
    "# TODO: Get the feature names (vocabulary) using get_feature_names_out()\n",
    "words = ___\n",
    "\n",
    "# TODO: Print the vocabulary\n",
    "print(\"Vocabulary:\")\n",
    "print(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2b50 EASY: Exercise 3 - Check Matrix Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the shape of the vectorized matrix\n",
    "\n",
    "docs = [\n",
    "    \"apple orange\",\n",
    "    \"banana apple\",\n",
    "    \"orange banana apple\",\n",
    "    \"apple apple banana\"\n",
    "]\n",
    "\n",
    "vec = CountVectorizer()\n",
    "matrix = vec.fit_transform(docs)\n",
    "\n",
    "# TODO: Get the shape of the matrix\n",
    "shape = ___\n",
    "\n",
    "print(f\"Matrix shape: {shape}\")\nprint(f\"Number of documents: {shape[0]}\")\n",
    "print(f\"Number of unique words: {shape[1]}\")\n",
    "\n",
    "# TODO: Print what the shape means\n",
    "print(f\"Interpretation: We have {shape[0]} documents and {shape[1]} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2b50 EASY: Exercise 4 - Create TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TfidfVectorizer and fit on documents\n",
    "\n",
    "docs = [\n",
    "    \"the quick brown fox\",\n",
    "    \"the lazy dog\",\n",
    "    \"quick brown dog\"\n",
    "]\n",
    "\n",
    "# TODO: Create a TfidfVectorizer object\n",
    "tfidf_vec = ___\n",
    "\n",
    "# TODO: Fit and transform the documents\n",
    "tfidf_matrix = ___\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"\\nTF-IDF matrix (converted to array):\")\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2b50 EASY: Exercise 5 - Stop Words Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare vectorization with and without stop words\n",
    "\n",
    "docs = [\n",
    "    \"the machine learning is important\",\n",
    "    \"machine learning is powerful\",\n",
    "    \"important learning algorithms\"\n",
    "]\n",
    "\n",
    "# TODO: Create CountVectorizer WITHOUT stop words\n",
    "vec_no_stop = ___\n",
    "matrix_no_stop = vec_no_stop.fit_transform(docs)\n",
    "\n",
    "# TODO: Create CountVectorizer WITH English stop words\n",
    "vec_with_stop = ___\n",
    "matrix_with_stop = vec_with_stop.fit_transform(docs)\n",
    "\n",
    "print(f\"Without stop words: {len(vec_no_stop.get_feature_names_out())} words\")\n",
    "print(f\"Vocabulary: {list(vec_no_stop.get_feature_names_out())}\")\nprint(f\"\\nWith stop words: {len(vec_with_stop.get_feature_names_out())} words\")\n",
    "print(f\"Vocabulary: {list(vec_with_stop.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2b50\u2b50 MEDIUM: Exercise 6 - Compare BoW vs TF-IDF on Same Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Bag of Words and TF-IDF on the same documents\n",
    "\n",
    "docs = [\n",
    "    \"data science data analysis\",\n",
    "    \"machine learning models\",\n",
    "    \"data science and machine learning\"\n",
    "]\n",
    "\n",
    "# TODO: Create and fit CountVectorizer\n",
    "bow_vec = ___\n",
    "bow_matrix = ___\n",
    "\n",
    "# TODO: Create and fit TfidfVectorizer\n",
    "tfidf_vec = ___\n",
    "tfidf_matrix = ___\n",
    "\n",
    "print(\"Bag of Words (word counts):\")\n",
    "print(bow_matrix.toarray())\n",
    "\n",
    "print(\"\\nTF-IDF (weighted importance):\")\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "print(\"\\nNote: TF-IDF reduces weight on 'data' because it appears in multiple docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2b50\u2b50 MEDIUM: Exercise 7 - Use Bigrams with ngram_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectors with bigrams (2-word phrases)\n",
    "\n",
    "docs = [\n",
    "    \"machine learning is fun\",\n",
    "    \"deep learning networks\",\n",
    "    \"machine learning algorithms\"\n",
    "]\n",
    "\n",
    "# TODO: Create CountVectorizer with ngram_range for bigrams (1,2)\n",
    "# This should capture both individual words and 2-word phrases\n",
    "vec = ___\n",
    "matrix = vec.fit_transform(docs)\n",
    "\n",
    "# TODO: Get feature names to see the bigrams\n",
    "features = ___\n",
    "\n",
    "print(f\"Features (words + bigrams): {features}\")\n",
    "print(f\"\\nTotal features: {len(features)}\")\n",
    "print(f\"\\nMatrix shape: {matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2b50\u2b50 MEDIUM: Exercise 8 - Limit Vocabulary with max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit vocabulary size using max_features parameter\n",
    "\n",
    "docs = [\n",
    "    \"python java javascript ruby programming languages\",\n",
    "    \"data science machine learning analytics\",\n",
    "    \"web development frontend backend\",\n",
    "    \"artificial intelligence neural networks deep learning\"\n",
    "]\n",
    "\n",
    "# TODO: Create CountVectorizer with max_features=5 (keep only top 5 words)\n",
    "vec = ___\n",
    "matrix = vec.fit_transform(docs)\n",
    "\n",
    "# TODO: Get the feature names\n",
    "words = ___\n",
    "\n",
    "print(f\"Vocabulary limited to {len(words)} words:\")\n",
    "print(words)\n",
    "print(f\"\\nMatrix shape: {matrix.shape}\")\n",
    "print(\"Note: Only the most frequent words are kept\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2b50\u2b50 MEDIUM: Exercise 9 - Convert Sparse Matrix to Array and Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix to dense array and inspect individual values\n",
    "\n",
    "docs = [\n",
    "    \"cat dog pet\",\n",
    "    \"dog animal\",\n",
    "    \"cat pet animal\"\n",
    "]\n",
    "\n",
    "vec = CountVectorizer()\n",
    "sparse_matrix = vec.fit_transform(docs)\n",
    "\n",
    "# TODO: Convert sparse matrix to dense array using .toarray()\n",
    "dense_array = ___\n",
    "\n",
    "# TODO: Print the dense array\n",
    "print(\"Dense array:\")\n",
    "print(___)\n",
    "\n",
    "# TODO: Get and print a single document's vector (first document)\n",
    "first_doc_vector = ___\n",
    "print(f\"\\nFirst document vector: {first_doc_vector}\")\n",
    "\n",
    "# TODO: Get value at position [1, 2] (second doc, third word)\n",
    "value = ___\n",
    "print(f\"\\nValue at position [1, 2]: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2b50\u2b50 MEDIUM: Exercise 10 - Apply LDA with n_components=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LDA topic modeling on simple documents\n",
    "\n",
    "docs = [\n",
    "    \"machine learning algorithm data\",\n",
    "    \"neural network deep learning\",\n",
    "    \"machine learning model training\",\n",
    "    \"deep neural network training\"\n",
    "]\n",
    "\n",
    "# TODO: Create CountVectorizer and fit_transform\n",
    "vec = ___\n",
    "matrix = ___\n",
    "\n",
    "# TODO: Create LatentDirichletAllocation with n_components=2\n",
    "lda = ___\n",
    "\n",
    "# TODO: Fit the LDA model\n",
    "___\n",
    "\n",
    "# TODO: Get feature names\n",
    "words = ___\n",
    "\n",
    "print(\"Topics found:\")\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    # Get top 3 word indices\n",
    "    top_indices = topic.argsort()[-3:]\n",
    "    top_words = [words[i] for i in top_indices]\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2b50\u2b50\u2b50 HARD: Exercise 11 - Complete Vectorization Pipeline with Stop Words + Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a complete vectorization pipeline with multiple parameters\n",
    "\n",
    "docs = [\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"the lazy dog sleeps all day\",\n",
    "    \"quick brown dogs love to run\",\n",
    "    \"the fox is quick and clever\"\n",
    "]\n",
    "\n",
    "# TODO: Create TfidfVectorizer with:\n",
    "# - stop_words='english'\n",
    "# - ngram_range=(1, 2)\n",
    "# - max_features=10\n",
    "vec = ___\n",
    "\n",
    "# TODO: Fit and transform the documents\n",
    "matrix = ___\n",
    "\n",
    "# TODO: Get feature names\n",
    "features = ___\n",
    "\n",
    "print(f\"Features (unigrams + bigrams, no stop words, max 10):\")\n",
    "print(features)\n",
    "print(f\"\\nMatrix shape: {matrix.shape}\")\n",
    "print(f\"\\nTF-IDF values:\")\n",
    "print(matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2b50\u2b50\u2b50 HARD: Exercise 12 - Apply LDA and Print Top Words Per Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LDA and display top N words for each topic\n",
    "\n",
    "docs = [\n",
    "    \"python machine learning data science algorithms\",\n",
    "    \"deep neural networks artificial intelligence\",\n",
    "    \"machine learning models training data\",\n",
    "    \"neural networks deep learning frameworks\",\n",
    "    \"data science analytics python programming\",\n",
    "    \"artificial intelligence machine learning\"\n",
    "]\n",
    "\n",
    "# TODO: Vectorize with CountVectorizer (stop_words='english')\n",
    "vec = ___\n",
    "matrix = ___\n",
    "\n",
    "# TODO: Create and fit LDA with n_components=3 (3 topics)\n",
    "lda = ___\n",
    "___\n",
    "\n",
    "# TODO: Get feature names\n",
    "words = ___\n",
    "\n",
    "# TODO: Print top 5 words for each topic\n",
    "print(\"Topic Modeling Results (LDA):\")\nfor topic_idx, topic in enumerate(lda.components_):\n",
    "    # Get top 5 word indices\n",
    "    top_indices = ___\n",
    "    top_words = [words[i] for i in top_indices]\n",
    "    print(f\"\\nTopic {topic_idx + 1}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2b50\u2b50\u2b50 HARD: Exercise 13 - Apply NMF and Compare with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Apply NMF topic modeling and compare with LDA results\n",
    "\n",
    "docs = [\n",
    "    \"car vehicle automobile transportation\",\n",
    "    \"airplane flight aircraft travel\",\n",
    "    \"bicycle bike pedal cycle\",\n",
    "    \"train railway locomotive transport\",\n",
    "    \"ship vessel boat navigation\",\n",
    "    \"helicopter aircraft sky transport\"\n",
    "]\n",
    "\n",
    "# TODO: Vectorize with TfidfVectorizer (stop_words='english')\n",
    "vec = ___\n",
    "tfidf_matrix = ___\n",
    "\n",
    "# TODO: Create and fit NMF with n_components=2\n",
    "nmf = ___\n",
    "___\n",
    "\n",
    "# TODO: Get feature names\n",
    "words = ___\n",
    "\n",
    "# TODO: Print top 4 words for each NMF topic\n",
    "print(\"NMF Topics:\")\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    top_indices = ___\n",
    "    top_words = [words[i] for i in top_indices]\n",
    "    print(f\"\\nTopic {topic_idx + 1}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2b50\u2b50\u2b50 HARD: Exercise 14 - Process Document Collection and Find Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a complete document collection with vectorization and topic modeling\n",
    "\n",
    "docs = [\n",
    "    \"artificial intelligence machine learning neural networks\",\n",
    "    \"python programming language data science\",\n",
    "    \"deep learning convolutional neural networks\",\n",
    "    \"machine learning algorithms classification regression\",\n",
    "    \"python libraries numpy pandas scikit learn\",\n",
    "    \"natural language processing text mining\",\n",
    "    \"data analysis statistics visualization\",\n",
    "    \"computer vision image recognition\"\n",
    "]\n",
    "\n",
    "# TODO: Vectorize with CountVectorizer\n",
    "# - stop_words='english'\n",
    "# - max_df=0.7 (ignore words in more than 70% of docs)\n",
    "# - min_df=1 (keep words in at least 1 doc)\n",
    "vec = ___\n",
    "matrix = ___\n",
    "\n",
    "# TODO: Create and fit LDA with n_components=3\n",
    "lda = ___\n",
    "___\n",
    "\n",
    "# TODO: Get document-topic distribution\n",
    "doc_topics = ___\n",
    "\n",
    "# TODO: Get feature names\n",
    "words = ___\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DOCUMENT COLLECTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Print topics\n",
    "print(\"\\nTopics discovered:\")\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_indices = topic.argsort()[-5:]\n",
    "    top_words = [words[i] for i in top_indices]\n",
    "    print(f\"\\nTopic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "\n",
    "# Print document topic assignments\n",
    "print(f\"\\n\\nDocument-Topic Distribution:\")\n",
    "for i, doc in enumerate(docs):\n",
    "    dominant_topic = doc_topics[i].argmax()\n",
    "    print(f\"\\nDoc {i+1}: \\\"{doc[:40]}...\\\"\")\n",
    "    print(f\"  Dominant Topic: {dominant_topic + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2b50\u2b50\u2b50 HARD: Exercise 15 - Build Topic Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a reusable function that performs complete topic analysis\n",
    "\n",
    "# TODO: Create a function that takes documents, n_topics, and returns summary\n",
    "def analyze_topics(documents, n_topics=2, n_top_words=5):\n",
    "    \"\"\"\n",
    "    Analyze topics in a document collection.\n",
    "    \n",
    "    Args:\n",
    "        documents: list of text documents\n",
    "        n_topics: number of topics to extract\n",
    "        n_top_words: number of top words to show per topic\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'topics', 'doc_topics', and 'vocabulary'\n",
    "    \"\"\"\n",
    "    # TODO: Vectorize documents with CountVectorizer\n",
    "    # Use: stop_words='english', max_df=0.8, min_df=1\n",
    "    vec = ___\n",
    "    matrix = ___\n",
    "    \n",
    "    # TODO: Create and fit LDA model\n",
    "    lda = ___\n",
    "    ___\n",
    "    \n",
    "    # TODO: Extract topics\n",
    "    words = ___\n",
    "    topics = []\n",
    "    \n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        # Get top word indices\n",
    "        top_indices = ___\n",
    "        # Convert to words\n",
    "        top_words = ___\n",
    "        topics.append(top_words)\n",
    "    \n",
    "    # TODO: Get document-topic distribution\n",
    "    doc_topics = ___\n",
    "    \n",
    "    return {\n",
    "        'topics': topics,\n",
    "        'doc_topics': doc_topics,\n",
    "        'vocabulary': words\n",
    "    }\n",
    "\n",
    "# Test the function\n",
    "test_docs = [\n",
    "    \"apple orange banana fruit\",\n",
    "    \"cat dog animal pet\",\n",
    "    \"apple fruit food healthy\",\n",
    "    \"dog pet companion animal\"\n",
    "]\n",
    "\n",
    "result = analyze_topics(test_docs, n_topics=2)\n",
    "\n",
    "print(\"TOPIC ANALYSIS RESULTS\")\n",
    "print(\"=\" * 60)\nprint(\"\\nTopics:\")\n",
    "for i, topic_words in enumerate(result['topics']):\n",
    "    print(f\"Topic {i+1}: {', '.join(topic_words)}\")\n",
    "\n",
    "print(f\"\\nVocabulary size: {len(result['vocabulary'])} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Vectorization** converts text to numbers for algorithms\n",
    "- **Bag of Words** counts word occurrences\n",
    "- **TF-IDF** weights words by importance\n",
    "- **Topic Modeling** discovers hidden themes\n",
    "- **LDA** and **NMF** are popular topic modeling algorithms\n",
    "\n",
    "### When to Use:\n",
    "- **BoW**: Fast, simple classification\n",
    "- **TF-IDF**: Better for text similarity and classification\n",
    "- **LDA**: Probabilistic topics, interpretability\n",
    "- **NMF**: Matrix decomposition, semantic topics\n",
    "\n",
    "### What's Next:\n",
    "Tomorrow we'll build a **Text Classifier** using these techniques to categorize documents!\n",
    "\n",
    "---\n",
    "\n",
    "*Created for Natruja's NLP study plan*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}