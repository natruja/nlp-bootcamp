{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Stemming, Lemmatization & Regex\n",
    "**The AI Engineer Course 2026 - Section 21b**\n",
    "\n",
    "**Student:** Natruja\n",
    "\n",
    "**Date:** Friday, February 13, 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand stemming and its purpose\n",
    "2. Learn lemmatization and how it differs from stemming\n",
    "3. Use regular expressions (Regex) for text processing\n",
    "4. Compare these text normalization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ NLTK installed and data downloaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
      "[nltk_data]     failed: unable to get local issuer certificate\n",
      "[nltk_data]     (_ssl.c:1028)>\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install NLTK\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\", \"-q\"])\n",
    "\n",
    "# Download required NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "print(\"✓ NLTK installed and data downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization: Why Do We Need It?\n",
    "\n",
    "In NLP, words can appear in different forms:\n",
    "- \"running\", \"runs\", \"run\" - all refer to the same action\n",
    "- \"better\", \"best\" - all related to \"good\"\n",
    "- \"flying\", \"flies\" - same root concept\n",
    "\n",
    "**Text Normalization** converts these variations into a standard form, helping NLP models recognize that they're related.\n",
    "\n",
    "### Two Main Approaches:\n",
    "1. **Stemming**: Remove suffixes to get the root (fast, rough)\n",
    "2. **Lemmatization**: Convert to dictionary form using linguistic rules (slower, more accurate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming: Quick Root Extraction\n",
    "\n",
    "**Stemming** uses algorithms to remove common suffixes and prefixes.\n",
    "\n",
    "### Porter Stemmer Algorithm:\n",
    "- Most popular stemming algorithm\n",
    "- Uses simple rule-based approach\n",
    "- Very fast\n",
    "- May not always produce real words\n",
    "\n",
    "### Examples:\n",
    "- \"running\" → \"run\"\n",
    "- \"jumps\" → \"jump\"\n",
    "- \"relational\" → \"relat\"\n",
    "- \"universities\" → \"univers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Stemming with Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemming Examples:\n",
      "========================================\n",
      "Original Word   | Stemmed        \n",
      "----------------------------------------\n",
      "running         | run            \n",
      "runs            | run            \n",
      "ran             | ran            \n",
      "runner          | runner         \n",
      "jumped          | jump           \n",
      "jumps           | jump           \n",
      "jumping         | jump           \n",
      "relational      | relat          \n",
      "universities    | univers        \n"
     ]
    }
   ],
   "source": [
    "# Create a Porter Stemmer object\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Words to stem\n",
    "words = ['running', 'runs', 'ran', 'runner', 'jumped', 'jumps', 'jumping', 'relational', 'universities']\n",
    "\n",
    "print(\"Porter Stemming Examples:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"{'Original Word':<15} | {'Stemmed':<15}\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "for word in words:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    print(f\"{word:<15} | {stemmed:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization: Dictionary-Based Normalization\n",
    "\n",
    "**Lemmatization** converts words to their base form (lemma) using vocabulary and morphological analysis.\n",
    "\n",
    "### WordNetLemmatizer:\n",
    "- Uses WordNet dictionary\n",
    "- More accurate than stemming\n",
    "- Slower but produces real words\n",
    "- Requires POS (Part-of-Speech) tags for best results\n",
    "\n",
    "### Examples:\n",
    "- \"running\" → \"run\"\n",
    "- \"better\" → \"good\"\n",
    "- \"universities\" → \"university\"\n",
    "- \"organized\" → \"organize\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Lemmatization with WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet Lemmatization Examples:\n",
      "========================================\n",
      "Original Word   | Lemma          \n",
      "----------------------------------------\n",
      "running         | running        \n",
      "runs            | run            \n",
      "ran             | ran            \n",
      "better          | better         \n",
      "best            | best           \n",
      "organized       | organized      \n",
      "universities    | university     \n"
     ]
    }
   ],
   "source": [
    "# Create a Lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Words to lemmatize\n",
    "words = ['running', 'runs', 'ran', 'better', 'best', 'organized', 'universities']\n",
    "\n",
    "print(\"WordNet Lemmatization Examples:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"{'Original Word':<15} | {'Lemma':<15}\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "for word in words:\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    print(f\"{word:<15} | {lemma:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Stemming vs Lemmatization Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming vs Lemmatization:\n",
      "=======================================================\n",
      "Word            | Stemmed         | Lemmatized     \n",
      "-------------------------------------------------------\n",
      "running         | run             | running        \n",
      "better          | better          | better         \n",
      "organized       | organ           | organized      \n",
      "universities    | univers         | university     \n",
      "jumped          | jump            | jumped         \n",
      "happening       | happen          | happening      \n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = ['running', 'better', 'organized', 'universities', 'jumped', 'happening']\n",
    "\n",
    "print(\"Stemming vs Lemmatization:\")\n",
    "print(\"=\"*55)\n",
    "print(f\"{'Word':<15} | {'Stemmed':<15} | {'Lemmatized':<15}\")\n",
    "print(\"-\"*55)\n",
    "\n",
    "for word in words:\n",
    "    stem = stemmer.stem(word)\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    print(f\"{word:<15} | {stem:<15} | {lemma:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions (Regex): Pattern Matching\n",
    "\n",
    "**Regular Expressions** are patterns for matching and manipulating text.\n",
    "\n",
    "### Common Patterns:\n",
    "- `\\d` : Any digit (0-9)\n",
    "- `\\w` : Any word character (letters, digits, underscore)\n",
    "- `\\s` : Any whitespace\n",
    "- `.` : Any character\n",
    "- `*` : 0 or more repetitions\n",
    "- `+` : 1 or more repetitions\n",
    "- `[abc]` : Any character in brackets\n",
    "- `[^abc]` : Any character NOT in brackets\n",
    "\n",
    "### Common Use Cases:\n",
    "- Email validation\n",
    "- Phone number extraction\n",
    "- Removing special characters\n",
    "- Finding patterns in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Regex for Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Hello! I have 123 apples. My email is john@example.com. Call me at 555-1234!\n",
      "\n",
      "============================================================\n",
      "\n",
      "Remove special characters:\n",
      "Hello I have 123 apples My email is johnexamplecom Call me at 5551234\n",
      "\n",
      "Remove numbers:\n",
      "Hello! I have  apples. My email is john@example.com. Call me at -!\n",
      "\n",
      "Clean extra whitespace:\n",
      "Hello! I have 123 apples. My email is john@example.com. Call me at 555-1234!\n"
     ]
    }
   ],
   "source": [
    "# Sample text with special characters and numbers\n",
    "text = \"Hello! I have 123 apples. My email is john@example.com. Call me at 555-1234!\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Remove special characters (keep only letters, numbers, and spaces)\n",
    "cleaned = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "print(\"\\nRemove special characters:\")\n",
    "print(cleaned)\n",
    "\n",
    "# Remove numbers\n",
    "no_numbers = re.sub(r'\\d+', '', text)\n",
    "print(\"\\nRemove numbers:\")\n",
    "print(no_numbers)\n",
    "\n",
    "# Remove extra whitespace\n",
    "no_extra_spaces = re.sub(r'\\s+', ' ', text).strip()\n",
    "print(\"\\nClean extra whitespace:\")\n",
    "print(no_extra_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Regex for Pattern Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "Contact info: alice@example.com or bob@company.org. Phone: 555-1234 or 555-5678\n",
      "\n",
      "============================================================\n",
      "\n",
      "Emails found:\n",
      "['alice@example.com', 'bob@company.org']\n",
      "\n",
      "Phone numbers found:\n",
      "['555-1234', '555-5678']\n",
      "\n",
      "All numbers found:\n",
      "['555', '1234', '555', '5678']\n"
     ]
    }
   ],
   "source": [
    "# Text with email addresses and phone numbers\n",
    "text = \"Contact info: alice@example.com or bob@company.org. Phone: 555-1234 or 555-5678\"\n",
    "\n",
    "print(\"Text:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Extract email addresses\n",
    "emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "print(\"\\nEmails found:\")\n",
    "print(emails)\n",
    "\n",
    "# Extract phone numbers\n",
    "phones = re.findall(r'\\d{3}-\\d{4}', text)\n",
    "print(\"\\nPhone numbers found:\")\n",
    "print(phones)\n",
    "\n",
    "# Extract all numbers\n",
    "numbers = re.findall(r'\\d+', text)\n",
    "print(\"\\nAll numbers found:\")\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EXERCISE SECTION: 15 Exercises Organized by Difficulty\n",
    "\n",
    "Work through these exercises to master stemming, lemmatization, and regex!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 1 - Stem a Single Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: running\n",
      "Stemmed word: run\n",
      "Success! 'running' was stemmed to 'run'\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a stemmer and stem the word 'running'\n",
    "# Store the result in a variable called 'result'\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "word = 'running'\n",
    "result = stemmer.stem(word)\n",
    "\n",
    "print(f\"Original word: {word}\")\n",
    "print(f\"Stemmed word: {result}\")\n",
    "print(f\"Success! '{word}' was stemmed to '{result}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 2 - Stem a List of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['happiness', 'walking', 'studies', 'flying', 'connection']\n",
      "Stemmed:  ['happi', 'walk', 'studi', 'fli', 'connect']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Stem all words in the list using list comprehension or a loop\n",
    "# Create a list called 'stemmed_words' with the stemmed versions\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = ['happiness', 'walking', 'studies', 'flying', 'connection']\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(f\"Original: {words}\")\n",
    "print(f\"Stemmed:  {stemmed_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 3 - Lemmatize a Single Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: better\n",
      "Lemmatized word: better\n",
      "Success! 'better' was lemmatized to 'better'\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a lemmatizer and lemmatize the word 'better'\n",
    "# Store the result in a variable called 'result'\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word = 'better'\n",
    "result = lemmatizer.lemmatize(word)\n",
    "\n",
    "print(f\"Original word: {word}\")\n",
    "print(f\"Lemmatized word: {result}\")\n",
    "print(f\"Success! '{word}' was lemmatized to '{result}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 4 - Use re.findall() to Find Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I have 3 cats, 5 dogs, and 12 birds. There are 100 total animals.\n",
      "Numbers found: ['3', '5', '12', '100']\n",
      "Total of 4 numbers extracted!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Use re.findall() with the pattern \\d+ to extract all numbers from the text\n",
    "# Store the result in a variable called 'numbers'\n",
    "\n",
    "text = \"I have 3 cats, 5 dogs, and 12 birds. There are 100 total animals.\"\n",
    "\n",
    "numbers = re.findall(r'\\d+', text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Numbers found: {numbers}\")\n",
    "print(f\"Total of {len(numbers)} numbers extracted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ EASY: Exercise 5 - Use re.sub() to Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello! How are you? I'm great, thanks!!!!\n",
      "Cleaned:  Hello How are you Im great thanks\n"
     ]
    }
   ],
   "source": [
    "# TODO: Use re.sub() to remove all punctuation from the text\n",
    "# Pattern [^a-zA-Z\\s] means: any character that is NOT a letter or whitespace\n",
    "# Replace it with empty string ''\n",
    "\n",
    "text = \"Hello! How are you? I'm great, thanks!!!!\"\n",
    "\n",
    "cleaned = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Cleaned:  {cleaned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 6 - Compare Stemming vs Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\t| Stemmed\t| Lemmatized\n",
      "--------------------------------------------------\n",
      "running         | run             | running        \n",
      "better          | better          | better         \n",
      "organized       | organ           | organized      \n",
      "universities    | univers         | university     \n"
     ]
    }
   ],
   "source": [
    "# TODO: For each word in the list, create a dictionary with the original word,\n",
    "# its stemmed version, and its lemmatized version.\n",
    "# You can use a list comprehension or loop to create a list of dictionaries.\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = ['running', 'better', 'organized', 'universities']\n",
    "\n",
    "results = [\n",
    "    {\n",
    "        'original' : word,\n",
    "        'stemmed' : stemmer.stem(word),\n",
    "        'lemmatized' : lemmatizer.lemmatize(word)\n",
    "    }\n",
    "\n",
    "    for word in words\n",
    "    \n",
    "]\n",
    "\n",
    "# Print results in a readable format\n",
    "print(\"Word\\t\\t| Stemmed\\t| Lemmatized\")\n",
    "print(\"-\" * 50)\n",
    "for result in results:\n",
    "    print(f\"{result['original']:<15} | {result['stemmed']:<15} | {result['lemmatized']:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 7 - Lemmatize with Different POS Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: 'saw'\n",
      "As noun (pos='n'): saw\n",
      "As verb (pos='v'): saw\n",
      "\n",
      "Notice how POS tag changes the lemmatization result!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n"
     ]
    }
   ],
   "source": [
    "# TODO: Lemmatize the word 'saw' as:\n",
    "# 1. A noun (pos='n') - should stay as 'saw'\n",
    "# 2. A verb (pos='v') - should become 'see'\n",
    "# The pos parameter changes how the word is interpreted!\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word = 'saw'\n",
    "\n",
    "lemma_noun = lemmatizer.lemmatize(word, pos='n')  # pos='n'\n",
    "lemma_verb = lemmatizer.lemmatize(word, pos='v')  # pos='v'\n",
    "\n",
    "print(f\"Original word: '{word}'\")\n",
    "print(f\"As noun (pos='n'): {lemma_noun}\")\n",
    "print(f\"As verb (pos='v'): {lemma_verb}\")\n",
    "print(f\"\\nNotice how POS tag changes the lemmatization result!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 8 - Extract Emails and URLs with Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Contact us at support@company.com or sales@business.org. \n",
      "Visit our website at https://www.example.com or www.another-site.net. \n",
      "Email me at john.doe@mail.co.uk anytime!\n",
      "\n",
      "Emails found: ['support@company.com', 'sales@business.org', 'john.doe@mail.co.uk']\n",
      "URLs found: ['https://www.example.com', 'www.another-site.net.']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Use re.findall() with appropriate regex patterns to extract:\n",
    "# 1. All email addresses from the text\n",
    "# 2. All URLs from the text\n",
    "# Hints:\n",
    "# - Email pattern: r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "# - URL pattern: r'http\\S+|www\\S+'\n",
    "\n",
    "text = \"\"\"Contact us at support@company.com or sales@business.org. \n",
    "Visit our website at https://www.example.com or www.another-site.net. \n",
    "Email me at john.doe@mail.co.uk anytime!\"\"\"\n",
    "\n",
    "emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "urls = re.findall(r'http\\S+|www\\S+', text)\n",
    "\n",
    "print(f\"Text: {text}\\n\")\n",
    "print(f\"Emails found: {emails}\")\n",
    "print(f\"URLs found: {urls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 9 - Clean Text with Regex Then Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The quick BROWN fox!!! It's jumping over the lazy 123 dogs... Wow!!!\n",
      "Cleaned: the quick brown fox its jumping over the lazy  dogs wow\n",
      "Tokens: ['the', 'quick', 'brown', 'fox', 'its', 'jumping', 'over', 'the', 'lazy', 'dogs', 'wow']\n"
     ]
    }
   ],
   "source": [
    "# TODO: \n",
    "# 1. Use re.sub() to remove special characters and numbers from the text\n",
    "# 2. Convert to lowercase\n",
    "# 3. Use word_tokenize() to split into words\n",
    "# 4. Filter out empty strings\n",
    "\n",
    "text = \"The quick BROWN fox!!! It's jumping over the lazy 123 dogs... Wow!!!\"\n",
    "\n",
    "# Step 1: Clean with regex (remove punctuation, numbers, special chars)\n",
    "cleaned = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "# Step 2: Convert to lowercase\n",
    "cleaned = cleaned.lower()\n",
    "\n",
    "# # Step 3: Tokenize\n",
    "tokens = word_tokenize(cleaned)\n",
    "\n",
    "# # Step 4: Filter out empty strings\n",
    "tokens = [w for w in tokens if w]\n",
    "\n",
    "\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Cleaned: {cleaned.strip()}\")\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐ MEDIUM: Exercise 10 - Stem All Words in a Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The runners were running quickly through the running competition.\n",
      "Stemmed:  the runner were run quickli through the run competit .\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "# 1. Tokenize the sentence into words\n",
    "# 2. Stem each word using PorterStemmer\n",
    "# 3. Create a list of stemmed words\n",
    "# 4. Join them back into a sentence\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "sentence = \"The runners were running quickly through the running competition.\"\n",
    "\n",
    "# Step 1: Tokenize\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Step 2 & 3: Stem each word (use list comprehension)\n",
    "stemmed_tokens = [stemmer.stem(w) for w in tokens]\n",
    "\n",
    "# Step 4: Join back into a sentence\n",
    "stemmed_sentence = ' '.join(stemmed_tokens)\n",
    "\n",
    "print(f\"Original: {sentence}\")\n",
    "print(f\"Stemmed:  {stemmed_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 11 - Full Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The quick BROWN fox!!! It's jumping 123 times over the lazy dogs... Amazing!!!\n",
      "After removing special chars: The quick BROWN fox Its jumping  times over the lazy dogs Amazing\n",
      "After tokenization: ['the', 'quick', 'brown', 'fox', 'its', 'jumping', 'times', 'over', 'the', 'lazy', 'dogs', 'amazing']\n",
      "After removing stopwords: ['quick', 'brown', 'fox', 'jumping', 'times', 'lazy', 'dogs', 'amazing']\n",
      "After lemmatization: ['quick', 'brown', 'fox', 'jumping', 'time', 'lazy', 'dog', 'amazing']\n",
      "\n",
      "Final token count: 8\n"
     ]
    }
   ],
   "source": [
    "# TODO: Build a complete text preprocessing pipeline that:\n",
    "# 1. Removes special characters and numbers (keep only letters and spaces)\n",
    "# 2. Converts to lowercase\n",
    "# 3. Tokenizes the text\n",
    "# 4. Removes stopwords (use nltk.corpus.stopwords)\n",
    "# 5. Lemmatizes all remaining words\n",
    "\n",
    "# First, download stopwords if needed\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "raw_text = \"The quick BROWN fox!!! It's jumping 123 times over the lazy dogs... Amazing!!!\"\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Step 1: Remove special characters and numbers\n",
    "step1 = re.sub(r'[^a-zA-Z\\s]', '', raw_text)\n",
    "\n",
    "# Step 2: Convert to lowercase\n",
    "step2 = step1.lower()\n",
    "\n",
    "# Step 3: Tokenize\n",
    "step3 = word_tokenize(step2)\n",
    "\n",
    "# Step 4: Remove stopwords (filter out words in stop_words set)\n",
    "step4 = [w for w in step3 if w not in stop_words]\n",
    "\n",
    "# Step 5: Lemmatize\n",
    "step5 = [lemmatizer.lemmatize(w) for w in step4]\n",
    "\n",
    "print(f\"Original: {raw_text}\")\n",
    "print(f\"After removing special chars: {step1}\")\n",
    "print(f\"After tokenization: {step3}\")\n",
    "print(f\"After removing stopwords: {step4}\")\n",
    "print(f\"After lemmatization: {step5}\")\n",
    "print(f\"\\nFinal token count: {len(step5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 12 - Smart POS-Based Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\t| Smart Lemma\n",
      "------------------------------\n",
      "running         | run            \n",
      "better          | good           \n",
      "saw             | saw            \n",
      "organized       | organize       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a function that lemmatizes words and tries different POS tags\n",
    "# to find the shortest lemmatized form.\n",
    "# \n",
    "# The function should:\n",
    "# 1. Try lemmatizing with pos='n' (noun)\n",
    "# 2. Try lemmatizing with pos='v' (verb)\n",
    "# 3. Try lemmatizing with pos='a' (adjective)\n",
    "# 4. Return the shortest result\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def smart_lemmatize(word, lemmatizer):\n",
    "    \"\"\"Try different POS tags and return shortest lemmatized form.\"\"\"\n",
    "    \n",
    "    # Create a list of lemmatized results using different pos tags\n",
    "    results = [\n",
    "        lemmatizer.lemmatize(word, pos='n'),  # as noun\n",
    "        lemmatizer.lemmatize(word, pos='v'),  # as verb\n",
    "        lemmatizer.lemmatize(word, pos='a')   # as adjective\n",
    "    ]\n",
    "    \n",
    "    # Return the shortest result\n",
    "    return min(results, key=len)  # Use min() with key=len\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "test_words = ['running', 'better', 'saw', 'organized']\n",
    "\n",
    "print(\"Word\\t\\t| Smart Lemma\")\n",
    "print(\"-\" * 30)\n",
    "for word in test_words:\n",
    "    result = smart_lemmatize(word, lemmatizer)\n",
    "    print(f\"{word:<15} | {result:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 13 - Extract and Clean Phone Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Contact us:\n",
      "Main line: (555) 123-4567\n",
      "Sales: 555.987.6543\n",
      "Support: 555-246-8135\n",
      "Fax: (555) 111-2222\n",
      "Invalid: 555-12-34\n",
      "\n",
      "\n",
      "Extracted phone numbers: ['555.987.6543', '555-246-8135']\n",
      "Cleaned phone numbers: ['5559876543', '5552468135']\n"
     ]
    }
   ],
   "source": [
    "# TODO: \n",
    "# 1. Extract phone numbers from the text using regex\n",
    "# 2. Clean them to remove formatting (dashes, spaces, parentheses)\n",
    "# 3. Return only valid phone numbers (exactly 10 digits)\n",
    "#\n",
    "# Hints:\n",
    "# - Pattern to find phone numbers: r'\\(?\\d{3}\\)?[-.]?\\d{3}[-.]?\\d{4}'\n",
    "# - Use re.sub() to remove non-digit characters: r'\\D' matches non-digits\n",
    "# - Check length to ensure exactly 10 digits\n",
    "\n",
    "text = \"\"\"Contact us:\n",
    "Main line: (555) 123-4567\n",
    "Sales: 555.987.6543\n",
    "Support: 555-246-8135\n",
    "Fax: (555) 111-2222\n",
    "Invalid: 555-12-34\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Extract potential phone numbers\n",
    "phone_pattern = r'\\(?\\d{3}\\)?[-.]?\\d{3}[-.]?\\d{4}'\n",
    "extracted = re.findall(phone_pattern, text)\n",
    "\n",
    "\n",
    "# Step 2 & 3: Clean them and keep only valid ones (10 digits)\n",
    "cleaned_phones = []\n",
    "for phone in extracted:\n",
    "    # Remove non-digit characters\n",
    "    cleaned = re.sub(r'\\D', '', phone)\n",
    "    # Keep only if exactly 10 digits\n",
    "    if len(cleaned) == 10:\n",
    "        cleaned_phones.append(cleaned)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"\\nExtracted phone numbers: {extracted}\")\n",
    "print(f\"Cleaned phone numbers: {cleaned_phones}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 14 - Process Multiple Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1:\n",
      "  Original: The quick brown fox jumps over the lazy dog!!!\n",
      "  Tokens: 9 → 7\n",
      "  Processed: ['The', 'quick', 'brown', 'fox', 'jump', 'lazy', 'dog']\n",
      "\n",
      "Document 2:\n",
      "  Original: Natural Language Processing is amazing!!! I'm learning NLP with 100% enthusiasm.\n",
      "  Tokens: 10 → 8\n",
      "  Processed: ['Natural', 'Language', 'Processing', 'amazing', 'Im', 'learning', 'NLP', 'enthusiasm']\n",
      "\n",
      "Document 3:\n",
      "  Original: Python, Java, and C++ are popular programming languages. They're great!!!\n",
      "  Tokens: 10 → 8\n",
      "  Processed: ['Python', 'Java', 'C', 'popular', 'programming', 'language', 'Theyre', 'great']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a function that processes multiple documents through:\n",
    "# 1. Regex cleaning (remove special characters, numbers)\n",
    "# 2. Tokenization\n",
    "# 3. Stopword removal\n",
    "# 4. Lemmatization\n",
    "# \n",
    "# The function should return stats about each document:\n",
    "# - Original token count\n",
    "# - Final token count\n",
    "# - List of final lemmatized tokens\n",
    "\n",
    "def process_document(text, lemmatizer, stop_words):\n",
    "    \"\"\"Process a single document through the full pipeline.\"\"\"\n",
    "    # Step 1: Clean (remove special chars, numbers)\n",
    "    cleaned =  re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Step 2: Tokenize\n",
    "    tokens = word_tokenize(cleaned)\n",
    "    original_count = len(tokens)\n",
    "    \n",
    "    # Step 3: Remove stopwords\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    \n",
    "    # Step 4: Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    \n",
    "    return {\n",
    "        'original_count': original_count,\n",
    "        'final_count': len(tokens),\n",
    "        'tokens': tokens\n",
    "    }\n",
    "\n",
    "# Documents to process\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog!!!\",\n",
    "    \"Natural Language Processing is amazing!!! I'm learning NLP with 100% enthusiasm.\",\n",
    "    \"Python, Java, and C++ are popular programming languages. They're great!!!\"\n",
    "]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "results = []\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    result = process_document(doc, lemmatizer, stop_words)\n",
    "    results.append(result)\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"  Original: {doc}\")\n",
    "    print(f\"  Tokens: {result['original_count']} → {result['final_count']}\")\n",
    "    print(f\"  Processed: {result['tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐⭐⭐ HARD: Exercise 15 - Compare Stemming vs Lemmatization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: The runners were running quickly. They ran and jumped repeatedly, running faster!\n",
      "\n",
      "Stemming pipeline:\n",
      "  Result: ['runner', 'run', 'quickli', 'ran', 'jump', 'repeatedli', 'run', 'faster']\n",
      "  Unique tokens: 7\n",
      "\n",
      "Lemmatization pipeline:\n",
      "  Result: ['runner', 'running', 'quickly', 'ran', 'jumped', 'repeatedly', 'running', 'faster']\n",
      "  Unique tokens: 7\n",
      "\n",
      "Comparison:\n",
      "  Stemming produced 8 tokens with 7 unique\n",
      "  Lemmatization produced 8 tokens with 7 unique\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create two preprocessing pipelines - one using stemming, one using lemmatization\n",
    "# Then compare their outputs.\n",
    "# \n",
    "# Stemming pipeline:\n",
    "# - Clean text with regex\n",
    "# - Tokenize\n",
    "# - Remove stopwords\n",
    "# - Stem words\n",
    "#\n",
    "# Lemmatization pipeline:\n",
    "# - Clean text with regex\n",
    "# - Tokenize\n",
    "# - Remove stopwords\n",
    "# - Lemmatize words\n",
    "#\n",
    "# Compare results and count unique tokens in each pipeline.\n",
    "\n",
    "def stemming_pipeline(text, stemmer, stop_words):\n",
    "    \"\"\"Process text using stemming.\"\"\"\n",
    "    cleaned = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    tokens = word_tokenize(cleaned)\n",
    "    tokens = [t for t in tokens if t not in stop_words and t.isalpha()]\n",
    "    stemmed = [stemmer.stem(w) for w in tokens]  # Apply stemmer to each token\n",
    "    return stemmed\n",
    "\n",
    "def lemmatization_pipeline(text, lemmatizer, stop_words):\n",
    "    \"\"\"Process text using lemmatization.\"\"\"\n",
    "    cleaned = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    tokens = word_tokenize(cleaned)\n",
    "    tokens = [t for t in tokens if t not in stop_words and t.isalpha()]\n",
    "    lemmatized = [lemmatizer.lemmatize(w) for w in tokens]  # Apply lemmatizer to each token\n",
    "    return lemmatized\n",
    "\n",
    "# Test text\n",
    "text = \"The runners were running quickly. They ran and jumped repeatedly, running faster!\"\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Process with both pipelines\n",
    "stemmed_result = stemming_pipeline(text, stemmer, stop_words)\n",
    "lemmatized_result = lemmatization_pipeline(text, lemmatizer, stop_words)\n",
    "\n",
    "print(f\"Original text: {text}\\n\")\n",
    "print(f\"Stemming pipeline:\")\n",
    "print(f\"  Result: {stemmed_result}\")\n",
    "print(f\"  Unique tokens: {len(set(stemmed_result))}\")\n",
    "print(f\"\\nLemmatization pipeline:\")\n",
    "print(f\"  Result: {lemmatized_result}\")\n",
    "print(f\"  Unique tokens: {len(set(lemmatized_result))}\")\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Stemming produced {len(stemmed_result)} tokens with {len(set(stemmed_result))} unique\")\n",
    "print(f\"  Lemmatization produced {len(lemmatized_result)} tokens with {len(set(lemmatized_result))} unique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Stemming** removes suffixes to get roots (fast, approximate)\n",
    "- **Lemmatization** converts to dictionary form (slower, more accurate)\n",
    "- **Regex** is powerful for finding and cleaning text patterns\n",
    "- Each technique has different use cases and trade-offs\n",
    "\n",
    "### When to Use Each:\n",
    "- **Stemming**: Quick processing, search engines, text analysis\n",
    "- **Lemmatization**: NLP tasks, sentiment analysis, when accuracy matters\n",
    "- **Regex**: Data cleaning, pattern extraction, preprocessing\n",
    "\n",
    "### What's Next:\n",
    "Tomorrow we'll explore **POS Tagging and Named Entity Recognition (NER)** - identifying what role each word plays in a sentence!\n",
    "\n",
    "---\n",
    "\n",
    "*Created for Natruja's NLP study plan*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "nlp_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
