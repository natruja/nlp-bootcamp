{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1: NLP Introduction + Tokenization & Stop Words\n",
    "**The AI Engineer Course 2026 - Sections 20 & 21a**\n",
    "\n",
    "**Student:** Natruja\n",
    "\n",
    "**Date:** Thursday, February 12, 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand what Natural Language Processing (NLP) is\n",
    "2. Learn about tokenization (word and sentence level)\n",
    "3. Understand stop words and why they matter\n",
    "4. Apply these concepts to real text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ NLTK installed and data downloaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n"
     ]
    }
   ],
   "source": [
    "# Install NLTK (Natural Language Toolkit)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install NLTK\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\", \"-q\"])\n",
    "\n",
    "# Download required NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "print(\"✓ NLTK installed and data downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Natural Language Processing (NLP)?\n",
    "\n",
    "Natural Language Processing is a branch of artificial intelligence that focuses on the interaction between computers and human language. It enables machines to:\n",
    "- Understand text and spoken words\n",
    "- Extract meaning from language\n",
    "- Generate human-like responses\n",
    "\n",
    "### Real-World Applications:\n",
    "- Sentiment Analysis (Twitter mood tracking)\n",
    "- Machine Translation (Google Translate)\n",
    "- Chatbots (Customer service)\n",
    "- Text Classification (Spam detection)\n",
    "- Named Entity Recognition (Finding people, places, companies in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization: Breaking Text into Pieces\n",
    "\n",
    "**Tokenization** is the process of breaking text into smaller units called **tokens**.\n",
    "\n",
    "### Types of Tokenization:\n",
    "1. **Word Tokenization**: Split text into individual words\n",
    "2. **Sentence Tokenization**: Split text into sentences\n",
    "3. **Character Tokenization**: Split text into characters (less common)\n",
    "\n",
    "### Why is Tokenization Important?\n",
    "- Machines can't process raw text; they need structured data\n",
    "- Tokenization is the first step in almost all NLP tasks\n",
    "- Different tokenization methods suit different problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Hello! How are you doing today? Natural Language Processing is fascinating!\n",
      "\n",
      "============================================================\n",
      "\n",
      "Tokenized words:\n",
      "['Hello', '!', 'How', 'are', 'you', 'doing', 'today', '?', 'Natural', 'Language', 'Processing', 'is', 'fascinating', '!']\n",
      "\n",
      "Total number of tokens: 14\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"Hello! How are you doing today? Natural Language Processing is fascinating!\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Word tokenization\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"\\nTokenized words:\")\n",
    "print(tokens)\n",
    "print(f\"\\nTotal number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text with multiple sentences\n",
    "text = \"Hello! How are you? I'm learning NLP. It's amazing. What do you think?\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"\\nTokenized sentences:\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sentence}\")\n",
    "\n",
    "print(f\"\\nTotal number of sentences: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words: Common Words to Ignore\n",
    "\n",
    "**Stop words** are common words that appear frequently in text but often don't carry much meaning.\n",
    "\n",
    "### Examples of Stop Words:\n",
    "- Articles: \"a\", \"an\", \"the\"\n",
    "- Pronouns: \"he\", \"she\", \"it\", \"I\", \"you\"\n",
    "- Prepositions: \"in\", \"at\", \"on\", \"by\"\n",
    "- Conjunctions: \"and\", \"but\", \"or\"\n",
    "- Auxiliary verbs: \"is\", \"am\", \"are\"\n",
    "\n",
    "### Why Remove Stop Words?\n",
    "- They reduce noise in text analysis\n",
    "- They decrease computation time\n",
    "- They help focus on meaningful content\n",
    "- Some ML models work better without them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE: Identifying and Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of English stop words: 198\n",
      "\n",
      "First 20 stop words: ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n",
      "\n",
      "============================================================\n",
      "Original tokens: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "Total tokens: 9\n",
      "\n",
      "Filtered tokens (stop words removed): ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n",
      "Total tokens after filtering: 6\n"
     ]
    }
   ],
   "source": [
    "# Get English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(f\"Number of English stop words: {len(stop_words)}\")\n",
    "print(f\"\\nFirst 20 stop words: {sorted(list(stop_words))[:20]}\")\n",
    "\n",
    "# Sample text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"Original tokens: {tokens}\")\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "\n",
    "# Remove stop words\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "print(f\"\\nFiltered tokens (stop words removed): {filtered_tokens}\")\n",
    "print(f\"Total tokens after filtering: {len(filtered_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR TURN: Exercise 1 - Word Tokenization\n",
    "\n",
    "Tokenize the following text into words. Count how many tokens you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['python', 'is', 'great', 'for', 'data', 'science', '.', 'machine', 'learning', 'and', 'ai', 'are', 'the', 'future', '!']\n",
      "Number of tokens: 15\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: Tokenize this text\n",
    "exercise_text = \"Python is great for data science. Machine learning and AI are the future!\"\n",
    "\n",
    "# TODO: Use word_tokenize() to tokenize exercise_text\n",
    "tokens = word_tokenize(exercise_text.lower())\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR TURN: Exercise 2 - Sentence Tokenization\n",
    "\n",
    "Break the following paragraph into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 4\n",
      "\n",
      "Each sentence:\n",
      "1. NLP is fascinating.\n",
      "2. It helps machines understand human language.\n",
      "3. You should learn it!\n",
      "4. Don't you agree?\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: Tokenize into sentences\n",
    "paragraph = \"NLP is fascinating. It helps machines understand human language. You should learn it! Don't you agree?\"\n",
    "\n",
    "# TODO: Use sent_tokenize() to split into sentences\n",
    "sentences = sent_tokenize(paragraph)\n",
    "\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "print(\"\\nEach sentence:\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR TURN: Exercise 3 - Stop Words Removal\n",
    "\n",
    "Remove stop words from the tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'on', 'a', 'sunny', 'day']\n",
      "Original count: 13\n",
      "\n",
      "Filtered tokens: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', 'sunny', 'day']\n",
      "Filtered count: 8\n",
      "\n",
      "Stop words removed: 5\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: Remove stop words\n",
    "text = \"The quick brown fox jumps over the lazy dog on a sunny day\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(f\"Original tokens: {tokens}\")\n",
    "print(f\"Original count: {len(tokens)}\")\n",
    "\n",
    "# TODO: Filter out stop words using list comprehension\n",
    "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "print(f\"\\nFiltered tokens: {filtered_tokens}\")\n",
    "print(f\"Filtered count: {len(filtered_tokens)}\")\n",
    "print(f\"\\nStop words removed: {len(tokens) - len(filtered_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR TURN: Exercise 4 (Harder) - Complete Pipeline\n",
    "\n",
    "Create a complete text processing pipeline: tokenize words, convert to lowercase, and remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Artificial Intelligence and Machine Learning are transforming industries. These technologies are shaping the future!\n",
      "\n",
      "Cleaned tokens: ['artificial', 'intelligence', 'machine', 'learning', 'transforming', 'industries', '.', 'technologies', 'shaping', 'future', '!']\n",
      "Reduction: 16 → 11 tokens\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: Complete preprocessing pipeline\n",
    "raw_text = \"Artificial Intelligence and Machine Learning are transforming industries. These technologies are shaping the future!\"\n",
    "\n",
    "# Step 1: Convert to lowercase\n",
    "# Step 2: Tokenize\n",
    "# Step 3: Remove stop words\n",
    "# Step 4: Print results\n",
    "\n",
    "# TODO: Fill in the steps\n",
    "text_lower = raw_text.lower()\n",
    "tokens = word_tokenize(text_lower)\n",
    "stop_words = stopwords.words('english')\n",
    "cleaned_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "print(f\"Original: {raw_text}\")\n",
    "print(f\"\\nCleaned tokens: {cleaned_tokens}\")\n",
    "print(f\"Reduction: {len(tokens)} → {len(cleaned_tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHALLENGE PROJECT: Text Analysis Tool\n",
    "\n",
    "Create a simple text analysis tool that:\n",
    "1. Takes any input text\n",
    "2. Tokenizes it into words and sentences\n",
    "3. Shows word statistics (with and without stop words)\n",
    "4. Displays the most important words (non-stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEXT ANALYSIS REPORT\n",
      "============================================================\n",
      "\n",
      "Original Text: Machine learning is a subset of artificial intelligence. It enables computers to learn from data and improve over time.\n",
      "\n",
      "Statistics:\n",
      "  • Sentences: 2\n",
      "  • Total words: 21\n",
      "  • Meaningful words (non-stop): 11\n",
      "  • Stop words removed: 10\n",
      "\n",
      "Meaningful words: ['machine', 'learning', 'subset', 'artificial', 'intelligence', 'enables', 'computers', 'learn', 'data', 'improve', 'time']\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def analyze_text(text):\n",
    "    \"\"\"Analyze text and show tokenization statistics.\"\"\"\n",
    "    \n",
    "    # Tokenize into sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    meaningful_words = [w for w in words if w.isalnum() and w not in stop_words]\n",
    "    \n",
    "    # Print analysis\n",
    "    print(\"=\"*60)\n",
    "    print(\"TEXT ANALYSIS REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nOriginal Text: {text}\")\n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  • Sentences: {len(sentences)}\")\n",
    "    print(f\"  • Total words: {len(words)}\")\n",
    "    print(f\"  • Meaningful words (non-stop): {len(meaningful_words)}\")\n",
    "    print(f\"  • Stop words removed: {len(words) - len(meaningful_words)}\")\n",
    "    print(f\"\\nMeaningful words: {meaningful_words}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Test with sample text\n",
    "sample = \"Machine learning is a subset of artificial intelligence. It enables computers to learn from data and improve over time.\"\n",
    "analyze_text(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Tokenization** is breaking text into smaller units (words or sentences)\n",
    "- **Word tokenization** helps analyze text at the word level\n",
    "- **Sentence tokenization** helps understand document structure\n",
    "- **Stop words** are common words that don't add much meaning\n",
    "- Removing stop words can improve NLP model performance\n",
    "\n",
    "### What's Next:\n",
    "Tomorrow we'll learn about **Stemming and Lemmatization** - techniques to normalize words to their root forms!\n",
    "\n",
    "---\n",
    "\n",
    "*Created for Natruja's NLP study plan*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "nlp_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
